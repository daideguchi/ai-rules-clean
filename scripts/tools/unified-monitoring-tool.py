#!/usr/bin/env python3
"""
Unified Monitoring Tool - Phase 4 çµ±åˆãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ 
Consolidates: ai-api-check.sh + simple-log-analyzer.py + smart-log-manager.py + status-updater-daemon.sh

o3æ¨å¥¨ã‚»ãƒ¼ãƒ•ãƒ†ã‚£æ©Ÿèƒ½:
- ãƒ—ãƒ­ã‚»ã‚¹åˆ†é›¢ã«ã‚ˆã‚‹ã‚µãƒ–ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œ
- æ®µéšçš„ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯å¯¾å¿œ
- æ¨©é™åˆ†é›¢ã¨ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¼·åŒ–
- æ§‹é€ åŒ–ãƒ­ã‚°ã¨ç›£è¦–é€£æº
"""

import argparse
import gzip
import json
import logging
import os
import shutil
import signal
import subprocess
import sys
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

# ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±
TOOL_VERSION = "1.0.0"
CONSOLIDATED_SCRIPTS = [
    "ai-api-check.sh",
    "simple-log-analyzer.py",
    "smart-log-manager.py",
    "status-updater-daemon.sh",
]


class UnifiedMonitoringTool:
    """çµ±åˆãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ãƒ„ãƒ¼ãƒ« - o3æ¨å¥¨ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£"""

    def __init__(
        self, log_root: str = "runtime/logs", config_file: Optional[str] = None
    ):
        self.log_root = Path(log_root)
        self.log_root.mkdir(parents=True, exist_ok=True)

        # è¨­å®šèª­ã¿è¾¼ã¿
        self.config = self._load_config(config_file)

        # o3æ¨å¥¨: æ§‹é€ åŒ–ãƒ­ã‚°è¨­å®š
        logging.basicConfig(
            level=getattr(logging, self.config.get("log_level", "INFO")),
            format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
            handlers=[
                logging.FileHandler(self.log_root / "unified-monitoring.log"),
                logging.StreamHandler(),
            ],
        )
        self.logger = logging.getLogger("unified-monitoring")

        # ãƒ—ãƒ­ã‚»ã‚¹ç®¡ç†
        self.daemon_pid_file = self.log_root / "monitoring-daemon.pid"
        self.daemon_lock_file = self.log_root / "monitoring-daemon.lock"

        self.logger.info(f"UnifiedMonitoringTool v{TOOL_VERSION} åˆæœŸåŒ–å®Œäº†")

    def _load_config(self, config_file: Optional[str]) -> Dict[str, Any]:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿"""
        default_config = {
            "log_level": "INFO",
            "api_models": ["gemini-1.5-pro", "gemini-1.5-flash", "o3-mini"],
            "log_rotation": {"max_size_mb": 100, "max_age_hours": 24},
            "cleanup": {"retention_days": 30, "compression_days": 7},
            "monitoring": {
                "update_interval": 300,  # 5åˆ†
                "health_check_interval": 3600,  # 1æ™‚é–“
            },
        }

        if config_file and Path(config_file).exists():
            try:
                with open(config_file) as f:
                    user_config = json.load(f)
                default_config.update(user_config)
            except Exception as e:
                self.logger.warning(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {e}")

        return default_config

    # ========== API ãƒã‚§ãƒƒã‚¯æ©Ÿèƒ½ (ai-api-check.sh çµ±åˆ) ==========

    def check_api_status(
        self, model: Optional[str] = None, interactive: bool = False
    ) -> Dict[str, Any]:
        """APIå®Ÿè¡Œå‰ãƒã‚§ãƒƒã‚¯ - ai-api-check.shçµ±åˆæ©Ÿèƒ½"""
        self.logger.info("APIå®Ÿè¡Œå‰ãƒã‚§ãƒƒã‚¯é–‹å§‹")

        if interactive:
            return self._interactive_api_check()

        # éå¯¾è©±å¼ãƒã‚§ãƒƒã‚¯
        valid_models = self.config["api_models"]
        if model and model not in valid_models:
            return {
                "status": "error",
                "message": f"ç„¡åŠ¹ãªãƒ¢ãƒ‡ãƒ«å: {model}",
                "valid_models": valid_models,
                "timestamp": datetime.now().isoformat(),
            }

        # APIç–é€šç¢ºèªï¼ˆãƒ¢ãƒƒã‚¯ï¼‰
        result = {
            "status": "success",
            "model": model or "auto-detect",
            "api_available": True,
            "recommendations": [
                "ã‚¨ãƒ©ãƒ¼æ™‚ã¯o3ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯",
                "ã‚¯ã‚ªãƒ¼ã‚¿åˆ¶é™æ™‚ã¯æ™‚é–“ã‚’ç½®ã",
                "APIå…¨ä½“åœæ­¢æ™‚ã¯ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œ",
            ],
            "timestamp": datetime.now().isoformat(),
        }

        # ãƒ­ã‚°è¨˜éŒ²
        self._log_api_check(result)

        self.logger.info("APIå®Ÿè¡Œå‰ãƒã‚§ãƒƒã‚¯å®Œäº†", extra={"model": model})
        return result

    def _interactive_api_check(self) -> Dict[str, Any]:
        """å¯¾è©±å¼APIãƒã‚§ãƒƒã‚¯"""
        print("ğŸ” AI API å®Ÿè¡Œå‰ãƒã‚§ãƒƒã‚¯")
        print("==========================")

        # ãƒ¢ãƒ‡ãƒ«é¸æŠ
        print("1. ä½¿ç”¨äºˆå®šãƒ¢ãƒ‡ãƒ«åã‚’é¸æŠ:")
        for i, model in enumerate(self.config["api_models"], 1):
            print(f"   {i}. {model}")

        try:
            choice = input("é¸æŠ (1-3): ").strip()
            model_idx = int(choice) - 1
            if 0 <= model_idx < len(self.config["api_models"]):
                model = self.config["api_models"][model_idx]
                print(f"âœ… é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«: {model}")
            else:
                return {"status": "error", "message": "ç„¡åŠ¹ãªé¸æŠ"}
        except (ValueError, KeyboardInterrupt):
            return {"status": "cancelled", "message": "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚‹ã‚­ãƒ£ãƒ³ã‚»ãƒ«"}

        # ã‚³ãƒãƒ³ãƒ‰æ§‹æ–‡ç¢ºèª
        print("\n2. ã‚³ãƒãƒ³ãƒ‰æ§‹æ–‡ç¢ºèª:")
        if "gemini" in model:
            print(
                f'æ­£ã—ã„å½¢å¼: echo "prompt" | npx https://github.com/google-gemini/gemini-cli -m "{model}"'
            )
        elif "o3" in model:
            print("æ­£ã—ã„å½¢å¼: mcp__o3__o3-search with input parameter")

        syntax_ok = input("ã‚³ãƒãƒ³ãƒ‰æ§‹æ–‡ç¢ºèªæ¸ˆã¿ï¼Ÿ [y/n]: ").strip().lower()
        if syntax_ok != "y":
            return {"status": "error", "message": "ã‚³ãƒãƒ³ãƒ‰æ§‹æ–‡ã‚’ç¢ºèªã—ã¦ãã ã•ã„"}

        # ä»£æ›¿æ‰‹æ®µç¢ºèª
        print("\n3. ã‚¨ãƒ©ãƒ¼æ™‚ã®ä»£æ›¿æ‰‹æ®µæº–å‚™:")
        print("   - Geminiå¤±æ•— â†’ O3ä½¿ç”¨")
        print("   - ã‚¯ã‚ªãƒ¼ã‚¿åˆ¶é™ â†’ æ™‚é–“ã‚’ç½®ã")
        print("   - APIå…¨ä½“åœæ­¢ â†’ ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œ")

        backup_ok = input("ä»£æ›¿æ‰‹æ®µæº–å‚™æ¸ˆã¿ï¼Ÿ [y/n]: ").strip().lower()
        if backup_ok != "y":
            return {"status": "error", "message": "ä»£æ›¿æ‰‹æ®µã‚’æº–å‚™ã—ã¦ãã ã•ã„"}

        result = {
            "status": "success",
            "model": model,
            "interactive_check": True,
            "timestamp": datetime.now().isoformat(),
        }

        print("\nâœ… äº‹å‰ãƒã‚§ãƒƒã‚¯å®Œäº†")
        print("å®‰å…¨ã«APIå®Ÿè¡Œã—ã¦ãã ã•ã„")

        return result

    def _log_api_check(self, result: Dict[str, Any]):
        """API ãƒã‚§ãƒƒã‚¯çµæœãƒ­ã‚°"""
        log_file = self.log_root / f"api_checks_{datetime.now().strftime('%Y%m%d')}.log"
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "check_result": result,
            "tool_version": TOOL_VERSION,
        }

        with open(log_file, "a") as f:
            f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")

    # ========== ãƒ­ã‚°åˆ†ææ©Ÿèƒ½ (simple-log-analyzer.py çµ±åˆ) ==========

    def analyze_logs(self, scope: str = "all") -> Dict[str, Any]:
        """ãƒ­ã‚°åˆ†æå®Ÿè¡Œ - simple-log-analyzer.pyçµ±åˆæ©Ÿèƒ½"""
        self.logger.info("ãƒ­ã‚°åˆ†æé–‹å§‹", extra={"scope": scope})

        stats = {
            "total_files": 0,
            "total_size_mb": 0,
            "file_types": {},
            "large_files": [],
            "old_files": [],
            "errors_found": 0,
            "warnings_found": 0,
        }

        search_patterns = ["*.log", "*.txt"] if scope == "logs" else ["*"]

        for pattern in search_patterns:
            for log_file in self.log_root.rglob(pattern):
                if not log_file.is_file():
                    continue

                stats["total_files"] += 1
                file_size = log_file.stat().st_size
                stats["total_size_mb"] += file_size / (1024 * 1024)

                # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—çµ±è¨ˆ
                suffix = log_file.suffix.lower()
                stats["file_types"][suffix] = stats["file_types"].get(suffix, 0) + 1

                # å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«æ¤œå‡º
                if file_size > 10 * 1024 * 1024:  # 10MBä»¥ä¸Š
                    stats["large_files"].append(
                        {
                            "file": str(log_file.relative_to(self.log_root)),
                            "size_mb": round(file_size / (1024 * 1024), 2),
                        }
                    )

                # å¤ã„ãƒ•ã‚¡ã‚¤ãƒ«æ¤œå‡º
                mtime = datetime.fromtimestamp(log_file.stat().st_mtime)
                if datetime.now() - mtime > timedelta(days=30):
                    stats["old_files"].append(
                        {
                            "file": str(log_file.relative_to(self.log_root)),
                            "age_days": (datetime.now() - mtime).days,
                        }
                    )

                # ãƒ­ã‚°å†…å®¹åˆ†æ
                if suffix in [".log", ".txt"]:
                    error_count, warning_count = self._analyze_log_content(log_file)
                    stats["errors_found"] += error_count
                    stats["warnings_found"] += warning_count

        result = {
            "analysis_time": datetime.now().isoformat(),
            "scope": scope,
            "statistics": stats,
            "recommendations": self._generate_analysis_recommendations(stats),
            "tool_version": TOOL_VERSION,
        }

        self.logger.info(
            "ãƒ­ã‚°åˆ†æå®Œäº†",
            extra={
                "files": stats["total_files"],
                "size_mb": stats["total_size_mb"],
                "errors": stats["errors_found"],
            },
        )

        return result

    def _analyze_log_content(self, log_file: Path) -> Tuple[int, int]:
        """ãƒ­ã‚°å†…å®¹ã®åˆ†æ"""
        error_count = 0
        warning_count = 0

        try:
            with open(log_file, encoding="utf-8", errors="ignore") as f:
                for line in f:
                    line_lower = line.lower()
                    if any(
                        keyword in line_lower
                        for keyword in ["error", "fatal", "critical"]
                    ):
                        error_count += 1
                    elif any(
                        keyword in line_lower
                        for keyword in ["warning", "warn", "caution"]
                    ):
                        warning_count += 1
        except Exception:
            pass  # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–

        return error_count, warning_count

    def _generate_analysis_recommendations(self, stats: Dict[str, Any]) -> List[str]:
        """åˆ†æçµæœã‹ã‚‰ã®æ¨å¥¨äº‹é …ç”Ÿæˆ"""
        recommendations = []

        if len(stats["large_files"]) > 0:
            recommendations.append(
                f"ğŸ—‚ï¸ å¤§å®¹é‡ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ« {len(stats['large_files'])}å€‹ - ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æ¨å¥¨"
            )

        if len(stats["old_files"]) > 5:
            recommendations.append(
                f"ğŸ—‘ï¸ å¤ã„ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ« {len(stats['old_files'])}å€‹ - ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—æ¨å¥¨"
            )

        if stats["errors_found"] > 100:
            recommendations.append(
                f"ğŸš¨ å¤šæ•°ã®ã‚¨ãƒ©ãƒ¼æ¤œå‡º ({stats['errors_found']}ä»¶) - èª¿æŸ»ãŒå¿…è¦"
            )

        if stats["warnings_found"] > 500:
            recommendations.append(
                f"âš ï¸ å¤šæ•°ã®è­¦å‘Šæ¤œå‡º ({stats['warnings_found']}ä»¶) - ç¢ºèªæ¨å¥¨"
            )

        if stats["total_size_mb"] > 100:
            recommendations.append(
                f"ğŸ’¾ ãƒ­ã‚°ã‚µã‚¤ã‚ºå¤§ ({stats['total_size_mb']:.1f}MB) - ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–æ¨å¥¨"
            )

        if not recommendations:
            recommendations.append("âœ… ãƒ­ã‚°çŠ¶æ…‹ã¯è‰¯å¥½ã§ã™")

        return recommendations

    # ========== ãƒ­ã‚°ç®¡ç†æ©Ÿèƒ½ (smart-log-manager.py çµ±åˆ) ==========

    def rotate_logs(self) -> Dict[str, Any]:
        """ãƒ­ã‚°ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ - smart-log-manager.pyçµ±åˆæ©Ÿèƒ½"""
        self.logger.info("ãƒ­ã‚°ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹")

        config = self.config["log_rotation"]
        max_size = config["max_size_mb"] * 1024 * 1024
        max_age = timedelta(hours=config["max_age_hours"])

        rotated_files = []
        total_size_before = 0
        total_size_after = 0

        for log_file in self.log_root.rglob("*.log"):
            if not log_file.is_file():
                continue

            file_size = log_file.stat().st_size
            total_size_before += file_size

            # ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³åˆ¤å®š
            should_rotate = False
            if file_size > max_size:
                should_rotate = True
                reason = "size"
            else:
                mtime = datetime.fromtimestamp(log_file.stat().st_mtime)
                if datetime.now() - mtime > max_age:
                    should_rotate = True
                    reason = "age"

            if should_rotate:
                rotated_path = self._rotate_single_file(log_file)
                if rotated_path:
                    rotated_files.append(
                        {
                            "original": str(log_file.relative_to(self.log_root)),
                            "rotated": str(rotated_path.relative_to(self.log_root)),
                            "size_mb": file_size / (1024 * 1024),
                            "reason": reason,
                        }
                    )

        # ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å¾Œã®ã‚µã‚¤ã‚ºè¨ˆç®—
        for file_path in self.log_root.rglob("*"):
            if file_path.is_file():
                total_size_after += file_path.stat().st_size

        result = {
            "rotated_files": len(rotated_files),
            "files_details": rotated_files,
            "size_reduction_mb": (total_size_before - total_size_after) / (1024 * 1024),
            "timestamp": datetime.now().isoformat(),
            "tool_version": TOOL_VERSION,
        }

        self.logger.info(
            "ãƒ­ã‚°ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å®Œäº†",
            extra={
                "rotated_count": len(rotated_files),
                "size_reduction_mb": result["size_reduction_mb"],
            },
        )

        return result

    def _rotate_single_file(self, file_path: Path) -> Optional[Path]:
        """å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
            rotated_name = f"{file_path.stem}-{timestamp}{file_path.suffix}.gz"
            rotated_path = file_path.parent / rotated_name

            # gzipåœ§ç¸®ã—ã¦ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
            with open(file_path, "rb") as f_in:
                with gzip.open(rotated_path, "wb") as f_out:
                    shutil.copyfileobj(f_in, f_out)

            # å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªã‚¢ï¼ˆå®Œå…¨å‰Šé™¤ã§ã¯ãªãç©ºã«ã™ã‚‹ï¼‰
            file_path.write_text("")

            return rotated_path

        except Exception as e:
            self.logger.error(
                "ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å¤±æ•—", extra={"file": str(file_path), "error": str(e)}
            )
            return None

    def cleanup_old_logs(self, days: int = None) -> Dict[str, Any]:
        """å¤ã„ãƒ­ã‚°ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        if days is None:
            days = self.config["cleanup"]["retention_days"]

        self.logger.info("å¤ã„ãƒ­ã‚°ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—é–‹å§‹", extra={"retention_days": days})

        cutoff_date = datetime.now() - timedelta(days=days)
        deleted_files = []
        freed_space_mb = 0

        for log_file in self.log_root.rglob("*.log.*"):  # ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«
            if not log_file.is_file():
                continue

            mtime = datetime.fromtimestamp(log_file.stat().st_mtime)
            if mtime < cutoff_date:
                file_size = log_file.stat().st_size
                freed_space_mb += file_size / (1024 * 1024)
                deleted_files.append(str(log_file.relative_to(self.log_root)))

                try:
                    log_file.unlink()
                except Exception as e:
                    self.logger.error(
                        "å‰Šé™¤å¤±æ•—", extra={"file": str(log_file), "error": str(e)}
                    )

        result = {
            "cleaned_files": len(deleted_files),
            "files": deleted_files,
            "freed_space_mb": round(freed_space_mb, 2),
            "cutoff_days": days,
            "timestamp": datetime.now().isoformat(),
        }

        self.logger.info(
            "å¤ã„ãƒ­ã‚°ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†",
            extra={"deleted_count": len(deleted_files), "freed_mb": freed_space_mb},
        )

        return result

    # ========== ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°ãƒ‡ãƒ¼ãƒ¢ãƒ³ (status-updater-daemon.sh çµ±åˆ) ==========

    def start_status_daemon(self) -> Dict[str, Any]:
        """ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°ãƒ‡ãƒ¼ãƒ¢ãƒ³é–‹å§‹ - status-updater-daemon.shçµ±åˆæ©Ÿèƒ½"""

        # æ—¢å­˜ãƒ‡ãƒ¼ãƒ¢ãƒ³ãƒã‚§ãƒƒã‚¯
        if self._is_daemon_running():
            return {
                "status": "already_running",
                "message": f"ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°ãƒ‡ãƒ¼ãƒ¢ãƒ³ã¯æ—¢ã«å®Ÿè¡Œä¸­ã§ã™ (PID: {self._get_daemon_pid()})",
                "timestamp": datetime.now().isoformat(),
            }

        # ãƒ•ã‚©ãƒ¼ã‚¯ã—ã¦ãƒ‡ãƒ¼ãƒ¢ãƒ³åŒ–
        try:
            pid = os.fork()
            if pid > 0:
                # è¦ªãƒ—ãƒ­ã‚»ã‚¹ - PIDä¿å­˜ã—ã¦çµ‚äº†
                with open(self.daemon_pid_file, "w") as f:
                    f.write(str(pid))

                return {
                    "status": "started",
                    "pid": pid,
                    "message": "ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°ãƒ‡ãƒ¼ãƒ¢ãƒ³ã‚’é–‹å§‹ã—ã¾ã—ãŸ",
                    "timestamp": datetime.now().isoformat(),
                }
            else:
                # å­ãƒ—ãƒ­ã‚»ã‚¹ - ãƒ‡ãƒ¼ãƒ¢ãƒ³å®Ÿè¡Œ
                self._run_status_daemon()

        except OSError as e:
            return {
                "status": "error",
                "message": f"ãƒ‡ãƒ¼ãƒ¢ãƒ³é–‹å§‹å¤±æ•—: {e}",
                "timestamp": datetime.now().isoformat(),
            }

    def stop_status_daemon(self) -> Dict[str, Any]:
        """ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°ãƒ‡ãƒ¼ãƒ¢ãƒ³åœæ­¢"""
        if not self._is_daemon_running():
            return {
                "status": "not_running",
                "message": "ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°ãƒ‡ãƒ¼ãƒ¢ãƒ³ã¯å®Ÿè¡Œã•ã‚Œã¦ã„ã¾ã›ã‚“",
                "timestamp": datetime.now().isoformat(),
            }

        try:
            pid = self._get_daemon_pid()
            os.kill(pid, signal.SIGTERM)

            # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            self.daemon_pid_file.unlink(missing_ok=True)
            self.daemon_lock_file.unlink(missing_ok=True)

            return {
                "status": "stopped",
                "pid": pid,
                "message": "ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°ãƒ‡ãƒ¼ãƒ¢ãƒ³ã‚’åœæ­¢ã—ã¾ã—ãŸ",
                "timestamp": datetime.now().isoformat(),
            }

        except (ProcessLookupError, OSError) as e:
            return {
                "status": "error",
                "message": f"ãƒ‡ãƒ¼ãƒ¢ãƒ³åœæ­¢å¤±æ•—: {e}",
                "timestamp": datetime.now().isoformat(),
            }

    def _is_daemon_running(self) -> bool:
        """ãƒ‡ãƒ¼ãƒ¢ãƒ³å®Ÿè¡ŒçŠ¶æ…‹ç¢ºèª"""
        if not self.daemon_pid_file.exists():
            return False

        try:
            pid = self._get_daemon_pid()
            os.kill(pid, 0)  # ã‚·ã‚°ãƒŠãƒ«0ã§ãƒ—ãƒ­ã‚»ã‚¹å­˜åœ¨ç¢ºèª
            return True
        except (ProcessLookupError, OSError):
            # staleãªPIDãƒ•ã‚¡ã‚¤ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            self.daemon_pid_file.unlink(missing_ok=True)
            return False

    def _get_daemon_pid(self) -> int:
        """ãƒ‡ãƒ¼ãƒ¢ãƒ³PIDå–å¾—"""
        if self.daemon_pid_file.exists():
            return int(self.daemon_pid_file.read_text().strip())
        return 0

    def _run_status_daemon(self):
        """ãƒ‡ãƒ¼ãƒ¢ãƒ³ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—"""
        # ãƒ‡ãƒ¼ãƒ¢ãƒ³åŒ–å‡¦ç†
        os.setsid()
        os.chdir("/")
        os.umask(0)

        # æ¨™æº–å…¥å‡ºåŠ›ã‚’ã‚¯ãƒ­ãƒ¼ã‚º
        with open("/dev/null") as dev_null:
            os.dup2(dev_null.fileno(), sys.stdin.fileno())
        with open("/dev/null", "w") as dev_null:
            os.dup2(dev_null.fileno(), sys.stdout.fileno())
            os.dup2(dev_null.fileno(), sys.stderr.fileno())

        # ãƒ­ãƒƒã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
        self.daemon_lock_file.touch()

        # çµ‚äº†ã‚·ã‚°ãƒŠãƒ«ãƒãƒ³ãƒ‰ãƒ©
        def signal_handler(signum, frame):
            self.daemon_lock_file.unlink(missing_ok=True)
            sys.exit(0)

        signal.signal(signal.SIGTERM, signal_handler)
        signal.signal(signal.SIGINT, signal_handler)

        # ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—
        interval = self.config["monitoring"]["update_interval"]
        while True:
            try:
                # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°å®Ÿè¡Œ
                self._update_status()
                time.sleep(interval)

            except Exception as e:
                # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°è¨˜éŒ²
                error_log = self.log_root / "daemon_errors.log"
                with open(error_log, "a") as f:
                    f.write(f"{datetime.now().isoformat()}: {e}\n")
                time.sleep(60)  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯1åˆ†å¾…æ©Ÿ

    def _update_status(self):
        """ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°å‡¦ç†"""
        project_root = Path(__file__).parent.parent.parent

        # ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´ç¢ºèª
        check_paths = [
            project_root / "runtime",
            project_root / "scripts",
            project_root / "docs",
            project_root / "src",
        ]

        for path in check_paths:
            if not path.exists():
                continue

            # æœ€è¿‘å¤‰æ›´ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚§ãƒƒã‚¯
            for file_path in path.rglob("*"):
                if not file_path.is_file():
                    continue

                if file_path.suffix in [".json", ".py", ".md", ".sh"]:
                    mtime = datetime.fromtimestamp(file_path.stat().st_mtime)
                    if datetime.now() - mtime < timedelta(minutes=5):
                        # è‡ªå‹•ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¡¨ç¤ºå®Ÿè¡Œ
                        status_script = (
                            project_root / "scripts" / "auto-status-display.py"
                        )
                        if status_script.exists():
                            try:
                                subprocess.run(
                                    ["python3", str(status_script)],
                                    capture_output=True,
                                    timeout=30,
                                )
                            except (
                                subprocess.TimeoutExpired,
                                subprocess.CalledProcessError,
                            ):
                                pass
                        return

    # ========== çµ±åˆãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ ==========

    def health_check(self) -> Dict[str, Any]:
        """çµ±åˆãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ"""
        self.logger.info("çµ±åˆãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯é–‹å§‹")

        health_results = {
            "api_status": self.check_api_status(interactive=False),
            "log_analysis": self.analyze_logs(),
            "log_rotation": self.rotate_logs(),
            "cleanup_result": self.cleanup_old_logs(),
            "daemon_status": {
                "running": self._is_daemon_running(),
                "pid": self._get_daemon_pid() if self._is_daemon_running() else None,
            },
        }

        # ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—
        health_score = self._calculate_health_score(health_results)

        result = {
            "health_score": health_score,
            "timestamp": datetime.now().isoformat(),
            "tool_version": TOOL_VERSION,
            "consolidated_scripts": CONSOLIDATED_SCRIPTS,
            "results": health_results,
            "recommendations": self._generate_health_recommendations(
                health_results, health_score
            ),
        }

        # ãƒ˜ãƒ«ã‚¹ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        report_file = (
            self.log_root
            / f"health-report-{datetime.now().strftime('%Y%m%d-%H%M%S')}.json"
        )
        with open(report_file, "w", encoding="utf-8") as f:
            json.dump(result, f, indent=2, ensure_ascii=False)

        self.logger.info(
            "çµ±åˆãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯å®Œäº†",
            extra={"health_score": health_score, "report_file": str(report_file)},
        )

        return result

    def _calculate_health_score(self, health_results: Dict[str, Any]) -> float:
        """ãƒ˜ãƒ«ã‚¹ã‚¹ã‚³ã‚¢è¨ˆç®— (0-100)"""
        score = 100.0

        # APIçŠ¶æ…‹
        if health_results["api_status"]["status"] != "success":
            score -= 20

        # ãƒ­ã‚°ã‚¨ãƒ©ãƒ¼ç‡
        log_stats = health_results["log_analysis"]["statistics"]
        if log_stats["errors_found"] > 50:
            score -= min(log_stats["errors_found"] / 10, 30)

        # ãƒ­ã‚°ã‚µã‚¤ã‚º
        if log_stats["total_size_mb"] > 500:
            score -= 15

        # ãƒ‡ãƒ¼ãƒ¢ãƒ³çŠ¶æ…‹
        if not health_results["daemon_status"]["running"]:
            score -= 10

        return max(score, 0.0)

    def _generate_health_recommendations(
        self, health_results: Dict[str, Any], health_score: float
    ) -> List[str]:
        """ãƒ˜ãƒ«ã‚¹æ”¹å–„æ¨å¥¨äº‹é …"""
        recommendations = []

        if health_score < 70:
            recommendations.append("ğŸš¨ ã‚·ã‚¹ãƒ†ãƒ å¥å…¨æ€§ãŒä½ä¸‹ - ç·Šæ€¥å¯¾å¿œå¿…è¦")

        if health_results["api_status"]["status"] != "success":
            recommendations.append("ğŸ”§ APIæ¥ç¶šã«å•é¡Œ - è¨­å®šç¢ºèªãŒå¿…è¦")

        log_stats = health_results["log_analysis"]["statistics"]
        if log_stats["errors_found"] > 100:
            recommendations.append(
                f"ğŸ”¥ å¤šæ•°ã‚¨ãƒ©ãƒ¼æ¤œå‡º ({log_stats['errors_found']}ä»¶) - èª¿æŸ»å¿…è¦"
            )

        if not health_results["daemon_status"]["running"]:
            recommendations.append("âš™ï¸ ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°ãƒ‡ãƒ¼ãƒ¢ãƒ³åœæ­¢ - å†èµ·å‹•æ¨å¥¨")

        if len(health_results["log_rotation"]["files_details"]) > 10:
            recommendations.append("ğŸ“ å¤§é‡ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ - è¨­å®šè¦‹ç›´ã—æ¨å¥¨")

        if not recommendations:
            recommendations.append("âœ… å…¨ã‚·ã‚¹ãƒ†ãƒ æ­£å¸¸å‹•ä½œä¸­")

        return recommendations


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    parser = argparse.ArgumentParser(
        description=f"Unified Monitoring Tool v{TOOL_VERSION} - çµ±åˆãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=f"""
çµ±åˆæ¸ˆã¿ã‚¹ã‚¯ãƒªãƒ—ãƒˆ:
  {", ".join(CONSOLIDATED_SCRIPTS)}

ä½¿ç”¨ä¾‹:
  %(prog)s api-check --interactive          # å¯¾è©±å¼APIãƒã‚§ãƒƒã‚¯
  %(prog)s analyze --scope logs             # ãƒ­ã‚°åˆ†æå®Ÿè¡Œ
  %(prog)s rotate                           # ãƒ­ã‚°ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
  %(prog)s daemon start                     # ãƒ‡ãƒ¼ãƒ¢ãƒ³é–‹å§‹
  %(prog)s health                           # çµ±åˆãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
        """,
    )

    parser.add_argument(
        "--version", action="version", version=f"%(prog)s {TOOL_VERSION}"
    )
    parser.add_argument("--config", help="è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹")
    parser.add_argument(
        "--log-root", default="runtime/logs", help="ãƒ­ã‚°ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª"
    )

    subparsers = parser.add_subparsers(dest="command", help="å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰")

    # API ãƒã‚§ãƒƒã‚¯
    api_parser = subparsers.add_parser("api-check", help="APIå®Ÿè¡Œå‰ãƒã‚§ãƒƒã‚¯")
    api_parser.add_argument("--model", help="ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«å")
    api_parser.add_argument("--interactive", action="store_true", help="å¯¾è©±å¼ãƒã‚§ãƒƒã‚¯")

    # ãƒ­ã‚°åˆ†æ
    analyze_parser = subparsers.add_parser("analyze", help="ãƒ­ã‚°åˆ†æå®Ÿè¡Œ")
    analyze_parser.add_argument(
        "--scope", choices=["all", "logs"], default="all", help="åˆ†æã‚¹ã‚³ãƒ¼ãƒ—"
    )

    # ãƒ­ã‚°ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
    subparsers.add_parser("rotate", help="ãƒ­ã‚°ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ")

    # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    cleanup_parser = subparsers.add_parser("cleanup", help="å¤ã„ãƒ­ã‚°ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—")
    cleanup_parser.add_argument("--days", type=int, help="ä¿æŒæœŸé–“ï¼ˆæ—¥ï¼‰")

    # ãƒ‡ãƒ¼ãƒ¢ãƒ³ç®¡ç†
    daemon_parser = subparsers.add_parser("daemon", help="ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°ãƒ‡ãƒ¼ãƒ¢ãƒ³ç®¡ç†")
    daemon_parser.add_argument(
        "action", choices=["start", "stop", "status"], help="ãƒ‡ãƒ¼ãƒ¢ãƒ³ã‚¢ã‚¯ã‚·ãƒ§ãƒ³"
    )

    # ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
    subparsers.add_parser("health", help="çµ±åˆãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ")

    args = parser.parse_args()

    if not args.command:
        parser.print_help()
        return

    # ãƒ„ãƒ¼ãƒ«åˆæœŸåŒ–
    tool = UnifiedMonitoringTool(args.log_root, args.config)

    try:
        # ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œ
        if args.command == "api-check":
            result = tool.check_api_status(
                getattr(args, "model", None), getattr(args, "interactive", False)
            )

        elif args.command == "analyze":
            result = tool.analyze_logs(getattr(args, "scope", "all"))

        elif args.command == "rotate":
            result = tool.rotate_logs()

        elif args.command == "cleanup":
            result = tool.cleanup_old_logs(getattr(args, "days", None))

        elif args.command == "daemon":
            action = getattr(args, "action", "status")
            if action == "start":
                result = tool.start_status_daemon()
            elif action == "stop":
                result = tool.stop_status_daemon()
            elif action == "status":
                result = {
                    "running": tool._is_daemon_running(),
                    "pid": tool._get_daemon_pid()
                    if tool._is_daemon_running()
                    else None,
                    "timestamp": datetime.now().isoformat(),
                }

        elif args.command == "health":
            result = tool.health_check()
            print("\n" + "=" * 60)
            print("ğŸ¥ UNIFIED MONITORING TOOL - ãƒ˜ãƒ«ã‚¹ãƒ¬ãƒãƒ¼ãƒˆ")
            print("=" * 60)
            print(f"ğŸ“Š å¥å…¨æ€§ã‚¹ã‚³ã‚¢: {result['health_score']:.1f}/100")
            print(f"ğŸ› ï¸ ãƒ„ãƒ¼ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {result['tool_version']}")
            print(f"ğŸ“¦ çµ±åˆã‚¹ã‚¯ãƒªãƒ—ãƒˆæ•°: {len(result['consolidated_scripts'])}")
            print("\nğŸ¯ æ¨å¥¨äº‹é …:")
            for rec in result["recommendations"]:
                print(f"  {rec}")
            print("=" * 60)
            return

        # çµæœå‡ºåŠ›
        if not getattr(args, "interactive", False) or args.command != "api-check":
            print(json.dumps(result, indent=2, ensure_ascii=False))

    except KeyboardInterrupt:
        print("\næ“ä½œãŒã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ")
        sys.exit(1)
    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼: {e}", file=sys.stderr)
        sys.exit(1)


if __name__ == "__main__":
    main()
