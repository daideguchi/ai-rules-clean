#!/usr/bin/env python3
"""
[LEGACY WRAPPER] Smart Log Manager

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ unified-monitoring-tool.py ã«çµ±åˆã•ã‚Œã¾ã—ãŸã€‚
Phase 4 çµ±åˆå®Œäº† - ãƒ¬ã‚¬ã‚·ãƒ¼äº’æ›æ€§ã®ãŸã‚ã®wrapperã‚¹ã‚¯ãƒªãƒ—ãƒˆ

æ–°ã—ã„ä½¿ç”¨æ–¹æ³•:
  scripts/tools/unified-monitoring-tool.py rotate     # ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
  scripts/tools/unified-monitoring-tool.py cleanup   # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
  scripts/tools/unified-monitoring-tool.py health    # ãƒ˜ãƒ«ã‚¹ãƒ¬ãƒãƒ¼ãƒˆ
"""

import gzip
import json
import logging
import os
import shutil
import subprocess
import sys
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, List, Optional

print("âš ï¸  [LEGACY] smart-log-manager.py ã¯çµ±åˆã•ã‚Œã¾ã—ãŸ")
print("ğŸ“¦ unified-monitoring-tool.py ã«ç§»è¡Œã—ã¦ãã ã•ã„")
print("")
print("ğŸ”„ è‡ªå‹•è»¢é€ä¸­...")

# çµ±åˆãƒ„ãƒ¼ãƒ«ã®å®Ÿè¡Œ
script_dir = Path(__file__).parent
unified_tool = script_dir.parent / "unified-monitoring-tool.py"

# å¼•æ•°å¤‰æ› (action based)
args = sys.argv[1:] if len(sys.argv) > 1 else ["health"]
action_mapping = {
    "rotate": "rotate",
    "compress": "rotate",
    "cleanup": "cleanup",
    "analyze": "analyze",
    "health": "health",
}

action = args[0] if args else "health"
new_command = action_mapping.get(action, "health")
new_args = [new_command] + args[1:]

os.execv(sys.executable, [sys.executable, str(unified_tool)] + new_args)


class SmartLogManager:
    """o3+Geminiçµ±åˆã‚¹ãƒãƒ¼ãƒˆãƒ­ã‚°ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, log_root: str = "runtime/logs"):
        self.log_root = Path(log_root)
        self.log_root.mkdir(parents=True, exist_ok=True)

        # o3æˆ¦ç•¥: ãƒ­ã‚°è¨­å®šï¼ˆæ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç‰ˆï¼‰
        logging.basicConfig(
            level=getattr(logging, os.getenv("LOG_LEVEL", "INFO")),
            format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        )
        self.logger = logging.getLogger("smart-log-manager")

        # è¨­å®š
        self.max_file_size = 100 * 1024 * 1024  # 100MB (o3æ¨å¥¨)
        # DISABLED: Memory inheritance system never expires data
        self.retention_days = {
            "hot": 7,  # ãƒ­ãƒ¼ã‚«ãƒ«SSD
            "warm": 30,  # æ¨™æº–ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸
            "cold": 90,  # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸
        }

        self.logger.info(
            f"SmartLogManageråˆæœŸåŒ–å®Œäº† - log_root: {self.log_root}, max_size_mb: {self.max_file_size // (1024 * 1024)}"
        )

    def rotate_logs(self) -> Dict[str, Any]:
        """o3æˆ¦ç•¥: ãƒ­ã‚°ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ"""
        self.logger.info("ãƒ­ã‚°ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹")

        rotated_files = []
        total_size_before = 0
        total_size_after = 0

        # å…¨ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒ£ãƒ³
        for log_file in self.log_root.rglob("*.log"):
            if not log_file.is_file():
                continue

            file_size = log_file.stat().st_size
            total_size_before += file_size

            # ã‚µã‚¤ã‚ºã¾ãŸã¯æ—¥ä»˜ã§ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³åˆ¤å®š
            if self._should_rotate(log_file, file_size):
                rotated_path = self._rotate_single_file(log_file)
                if rotated_path:
                    rotated_files.append(
                        {
                            "original": str(log_file),
                            "rotated": str(rotated_path),
                            "size_mb": file_size / (1024 * 1024),
                        }
                    )

        # JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚‚åŒæ§˜ã«ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
        for json_file in self.log_root.rglob("*.json"):
            if not json_file.is_file() or "current-session" in json_file.name:
                continue

            file_size = json_file.stat().st_size
            total_size_before += file_size

            if self._should_rotate(json_file, file_size):
                rotated_path = self._rotate_single_file(json_file)
                if rotated_path:
                    rotated_files.append(
                        {
                            "original": str(json_file),
                            "rotated": str(rotated_path),
                            "size_mb": file_size / (1024 * 1024),
                        }
                    )

        # ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å¾Œã®ã‚µã‚¤ã‚ºè¨ˆç®—
        for file_path in self.log_root.rglob("*"):
            if file_path.is_file():
                total_size_after += file_path.stat().st_size

        result = {
            "rotated_files": len(rotated_files),
            "files_details": rotated_files,
            "size_reduction_mb": (total_size_before - total_size_after) / (1024 * 1024),
            "timestamp": datetime.now().isoformat(),
        }

        self.logger.info("ãƒ­ã‚°ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å®Œäº†", **result)
        return result

    def _should_rotate(self, file_path: Path, file_size: int) -> bool:
        """ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³åˆ¤å®š"""
        # ã‚µã‚¤ã‚ºãƒ™ãƒ¼ã‚¹
        if file_size > self.max_file_size:
            return True

        # æ™‚é–“ãƒ™ãƒ¼ã‚¹ï¼ˆ24æ™‚é–“ä»¥ä¸ŠçµŒéï¼‰
        try:
            mtime = datetime.fromtimestamp(file_path.stat().st_mtime)
            if datetime.now() - mtime > timedelta(hours=24):
                return True
        except OSError:
            pass

        return False

    def _rotate_single_file(self, file_path: Path) -> Optional[Path]:
        """å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
            rotated_name = f"{file_path.stem}-{timestamp}{file_path.suffix}.gz"
            rotated_path = file_path.parent / rotated_name

            # gzipåœ§ç¸®ã—ã¦ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
            with open(file_path, "rb") as f_in:
                with gzip.open(rotated_path, "wb") as f_out:
                    shutil.copyfileobj(f_in, f_out)

            # å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªã‚¢ï¼ˆå®Œå…¨å‰Šé™¤ã§ã¯ãªãç©ºã«ã™ã‚‹ï¼‰
            file_path.write_text("")

            self.logger.debug(
                "ãƒ•ã‚¡ã‚¤ãƒ«ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å®Œäº†",
                original=str(file_path),
                rotated=str(rotated_path),
            )
            return rotated_path

        except Exception as e:
            self.logger.error("ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å¤±æ•—", file=str(file_path), error=str(e))
            return None

    def compress_old_logs(self) -> Dict[str, Any]:
        """o3æˆ¦ç•¥: å¤ã„ãƒ­ã‚°ã®åœ§ç¸®"""
        self.logger.info("å¤ã„ãƒ­ã‚°åœ§ç¸®é–‹å§‹")

        compressed_files = []
        total_saved_bytes = 0

        # 7æ—¥ä»¥ä¸ŠçµŒéã—ãŸæœªåœ§ç¸®ãƒ­ã‚°ã‚’åœ§ç¸®
        cutoff_date = datetime.now() - timedelta(days=7)

        for log_file in self.log_root.rglob("*.log"):
            if not log_file.is_file():
                continue

            try:
                mtime = datetime.fromtimestamp(log_file.stat().st_mtime)
                if mtime < cutoff_date:
                    original_size = log_file.stat().st_size
                    compressed_path = self._compress_file(log_file)

                    if compressed_path:
                        compressed_size = compressed_path.stat().st_size
                        saved_bytes = original_size - compressed_size
                        total_saved_bytes += saved_bytes

                        compressed_files.append(
                            {
                                "file": str(log_file),
                                "compressed": str(compressed_path),
                                "original_size_mb": original_size / (1024 * 1024),
                                "compressed_size_mb": compressed_size / (1024 * 1024),
                                "compression_ratio": compressed_size / original_size
                                if original_size > 0
                                else 0,
                            }
                        )

                        # å…ƒãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
                        log_file.unlink()

            except OSError as e:
                self.logger.error("åœ§ç¸®å‡¦ç†å¤±æ•—", file=str(log_file), error=str(e))

        result = {
            "compressed_files": len(compressed_files),
            "files_details": compressed_files,
            "total_saved_mb": total_saved_bytes / (1024 * 1024),
            "timestamp": datetime.now().isoformat(),
        }

        self.logger.info("å¤ã„ãƒ­ã‚°åœ§ç¸®å®Œäº†", **result)
        return result

    def _compress_file(self, file_path: Path) -> Optional[Path]:
        """ãƒ•ã‚¡ã‚¤ãƒ«åœ§ç¸®ï¼ˆzstdä½¿ç”¨ã€fallbackã§gzipï¼‰"""
        compressed_path = file_path.with_suffix(file_path.suffix + ".zst")

        # zstdåœ§ç¸®ã‚’è©¦è¡Œ
        try:
            subprocess.run(
                ["zstd", "-19", str(file_path), "-o", str(compressed_path)],
                capture_output=True,
                check=True,
            )

            if compressed_path.exists():
                return compressed_path

        except (subprocess.CalledProcessError, FileNotFoundError):
            # zstdãŒå¤±æ•—ã—ãŸå ´åˆã¯gzipã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            self.logger.warning(
                "zstdåœ§ç¸®å¤±æ•—ã€gzipã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯", file=str(file_path)
            )

        # gzipåœ§ç¸®
        try:
            compressed_path = file_path.with_suffix(file_path.suffix + ".gz")
            with open(file_path, "rb") as f_in:
                with gzip.open(compressed_path, "wb") as f_out:
                    shutil.copyfileobj(f_in, f_out)
            return compressed_path

        except Exception as e:
            self.logger.error("gzipåœ§ç¸®ã‚‚å¤±æ•—", file=str(file_path), error=str(e))
            return None

    def cleanup_old_archives(self) -> Dict[str, Any]:
        """DISABLED: Memory inheritance system never deletes archives"""
        return {
            "status": "disabled",
            "message": "Memory inheritance system preserves all archives",
            "deleted_files": 0,
            "files_details": [],
            "bytes_freed": 0,
        }
        self.logger.info("å¤ã„ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—é–‹å§‹")

        deleted_files = []
        total_freed_bytes = 0

        # è¨­å®šã•ã‚ŒãŸä¿æŒæœŸé–“ã‚’è¶…ãˆãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
        cutoff_date = datetime.now() - timedelta(days=self.retention_days["cold"])

        for archive_file in self.log_root.rglob("*.gz"):
            if not archive_file.is_file():
                continue

            try:
                mtime = datetime.fromtimestamp(archive_file.stat().st_mtime)
                if mtime < cutoff_date:
                    file_size = archive_file.stat().st_size
                    total_freed_bytes += file_size

                    deleted_files.append(
                        {
                            "file": str(archive_file),
                            "size_mb": file_size / (1024 * 1024),
                            "age_days": (datetime.now() - mtime).days,
                        }
                    )

                    archive_file.unlink()

            except OSError as e:
                self.logger.error("å‰Šé™¤å¤±æ•—", file=str(archive_file), error=str(e))

        # .zstãƒ•ã‚¡ã‚¤ãƒ«ã‚‚åŒæ§˜ã«å‡¦ç†
        for archive_file in self.log_root.rglob("*.zst"):
            if not archive_file.is_file():
                continue

            try:
                mtime = datetime.fromtimestamp(archive_file.stat().st_mtime)
                if mtime < cutoff_date:
                    file_size = archive_file.stat().st_size
                    total_freed_bytes += file_size

                    deleted_files.append(
                        {
                            "file": str(archive_file),
                            "size_mb": file_size / (1024 * 1024),
                            "age_days": (datetime.now() - mtime).days,
                        }
                    )

                    archive_file.unlink()

            except OSError as e:
                self.logger.error("å‰Šé™¤å¤±æ•—", file=str(archive_file), error=str(e))

        result = {
            "deleted_files": len(deleted_files),
            "files_details": deleted_files,
            "freed_space_mb": total_freed_bytes / (1024 * 1024),
            "retention_days": self.retention_days["cold"],
            "timestamp": datetime.now().isoformat(),
        }

        self.logger.info("å¤ã„ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†", **result)
        return result

    def analyze_log_patterns(self) -> Dict[str, Any]:
        """Geminiæˆ¦ç•¥: ãƒ­ã‚°ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
        self.logger.info("ãƒ­ã‚°ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æé–‹å§‹")

        patterns = {
            "error_frequency": {},
            "warning_patterns": {},
            "performance_issues": [],
            "security_events": [],
            "size_distribution": {},
        }

        total_lines = 0
        total_files = 0

        # å…¨ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†æ
        for log_file in self.log_root.rglob("*.log"):
            if not log_file.is_file():
                continue

            total_files += 1
            file_lines = 0

            try:
                with open(log_file, encoding="utf-8", errors="ignore") as f:
                    for line in f:
                        total_lines += 1
                        file_lines += 1

                        # ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º
                        if "ERROR" in line or "FATAL" in line:
                            error_type = self._extract_error_type(line)
                            patterns["error_frequency"][error_type] = (
                                patterns["error_frequency"].get(error_type, 0) + 1
                            )

                        # è­¦å‘Šãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º
                        elif "WARNING" in line or "WARN" in line:
                            warning_type = self._extract_warning_type(line)
                            patterns["warning_patterns"][warning_type] = (
                                patterns["warning_patterns"].get(warning_type, 0) + 1
                            )

                        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å•é¡Œæ¤œå‡º
                        elif "timeout" in line.lower() or "slow" in line.lower():
                            patterns["performance_issues"].append(
                                {
                                    "file": str(log_file),
                                    "line": line.strip(),
                                    "timestamp": self._extract_timestamp(line),
                                }
                            )

                        # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¤ãƒ™ãƒ³ãƒˆæ¤œå‡º
                        elif any(
                            keyword in line.lower()
                            for keyword in ["auth", "login", "permission", "security"]
                        ):
                            patterns["security_events"].append(
                                {
                                    "file": str(log_file),
                                    "line": line.strip(),
                                    "timestamp": self._extract_timestamp(line),
                                }
                            )

                # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºåˆ†å¸ƒ
                file_size_mb = log_file.stat().st_size / (1024 * 1024)
                size_category = self._categorize_file_size(file_size_mb)
                patterns["size_distribution"][size_category] = (
                    patterns["size_distribution"].get(size_category, 0) + 1
                )

            except Exception as e:
                self.logger.error("ãƒ­ã‚°åˆ†æå¤±æ•—", file=str(log_file), error=str(e))

        # åˆ†æçµæœã®è¦ç´„
        result = {
            "total_files": total_files,
            "total_lines": total_lines,
            "analysis_patterns": patterns,
            "insights": self._generate_insights(patterns),
            "timestamp": datetime.now().isoformat(),
        }

        self.logger.info(
            "ãƒ­ã‚°ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æå®Œäº†",
            files=total_files,
            lines=total_lines,
            errors=len(patterns["error_frequency"]),
            warnings=len(patterns["warning_patterns"]),
        )

        return result

    def _extract_error_type(self, line: str) -> str:
        """ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—æŠ½å‡º"""
        common_errors = [
            "connection",
            "timeout",
            "permission",
            "not found",
            "invalid",
            "failed",
        ]
        for error_type in common_errors:
            if error_type in line.lower():
                return error_type
        return "other"

    def _extract_warning_type(self, line: str) -> str:
        """è­¦å‘Šã‚¿ã‚¤ãƒ—æŠ½å‡º"""
        common_warnings = [
            "deprecated",
            "limit",
            "capacity",
            "memory",
            "disk",
            "performance",
        ]
        for warning_type in common_warnings:
            if warning_type in line.lower():
                return warning_type
        return "other"

    def _extract_timestamp(self, line: str) -> Optional[str]:
        """ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æŠ½å‡ºï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        import re

        # ISOå½¢å¼ã®æ—¥æ™‚ã‚’æ¤œç´¢
        iso_pattern = r"\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}"
        match = re.search(iso_pattern, line)
        return match.group(0) if match else None

    def _categorize_file_size(self, size_mb: float) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚«ãƒ†ã‚´ãƒªåˆ†é¡"""
        if size_mb < 1:
            return "small (<1MB)"
        elif size_mb < 10:
            return "medium (1-10MB)"
        elif size_mb < 100:
            return "large (10-100MB)"
        else:
            return "very_large (>100MB)"

    def _generate_insights(self, patterns: Dict[str, Any]) -> List[str]:
        """Geminiæˆ¦ç•¥: ã‚¤ãƒ³ã‚µã‚¤ãƒˆç”Ÿæˆ"""
        insights = []

        # ã‚¨ãƒ©ãƒ¼é »åº¦åˆ†æ
        if patterns["error_frequency"]:
            most_common_error = max(
                patterns["error_frequency"].items(), key=lambda x: x[1]
            )
            insights.append(
                f"æœ€é »å‡ºã‚¨ãƒ©ãƒ¼: {most_common_error[0]} ({most_common_error[1]}å›)"
            )

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å•é¡Œ
        if len(patterns["performance_issues"]) > 10:
            insights.append(
                f"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å•é¡Œæ¤œå‡º: {len(patterns['performance_issues'])}ä»¶ - èª¿æŸ»æ¨å¥¨"
            )

        # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¤ãƒ™ãƒ³ãƒˆ
        if len(patterns["security_events"]) > 5:
            insights.append(
                f"ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£é–¢é€£ã‚¤ãƒ™ãƒ³ãƒˆ: {len(patterns['security_events'])}ä»¶ - è¦ç¢ºèª"
            )

        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºåˆ†å¸ƒ
        large_files = patterns["size_distribution"].get("very_large (>100MB)", 0)
        if large_files > 0:
            insights.append(f"å¤§å®¹é‡ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ« {large_files}å€‹ - ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æ¨å¥¨")

        return insights

    def generate_health_report(self) -> Dict[str, Any]:
        """ç·åˆãƒ˜ãƒ«ã‚¹ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        self.logger.info("ãƒ­ã‚°ãƒ˜ãƒ«ã‚¹ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆé–‹å§‹")

        # å„ç¨®åˆ†æå®Ÿè¡Œ
        rotation_result = self.rotate_logs()
        compression_result = self.compress_old_logs()
        cleanup_result = self.cleanup_old_archives()
        pattern_analysis = self.analyze_log_patterns()

        # ç·åˆè©•ä¾¡
        health_score = self._calculate_health_score(pattern_analysis)

        report = {
            "report_timestamp": datetime.now().isoformat(),
            "log_root": str(self.log_root),
            "health_score": health_score,
            "operations": {
                "rotation": rotation_result,
                "compression": compression_result,
                "cleanup": cleanup_result,
            },
            "analysis": pattern_analysis,
            "recommendations": self._generate_recommendations(
                pattern_analysis, health_score
            ),
            "next_maintenance": (datetime.now() + timedelta(days=1)).isoformat(),
        }

        # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        report_file = (
            self.log_root
            / f"health-report-{datetime.now().strftime('%Y%m%d-%H%M%S')}.json"
        )
        with open(report_file, "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        self.logger.info(
            "ãƒ­ã‚°ãƒ˜ãƒ«ã‚¹ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†",
            report_file=str(report_file),
            health_score=health_score,
        )

        return report

    def _calculate_health_score(self, analysis: Dict[str, Any]) -> float:
        """ãƒ˜ãƒ«ã‚¹ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆ0-100ï¼‰"""
        score = 100.0

        # ã‚¨ãƒ©ãƒ¼ç‡ã§ã‚¹ã‚³ã‚¢æ¸›ç®—
        total_lines = analysis.get("total_lines", 1)
        error_count = sum(analysis["analysis_patterns"]["error_frequency"].values())
        error_rate = error_count / total_lines if total_lines > 0 else 0
        score -= min(error_rate * 1000, 30)  # æœ€å¤§30ç‚¹æ¸›ç‚¹

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å•é¡Œã§ã‚¹ã‚³ã‚¢æ¸›ç®—
        perf_issues = len(analysis["analysis_patterns"]["performance_issues"])
        score -= min(perf_issues * 2, 20)  # æœ€å¤§20ç‚¹æ¸›ç‚¹

        # å¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«ã§ã‚¹ã‚³ã‚¢æ¸›ç®—
        large_files = analysis["analysis_patterns"]["size_distribution"].get(
            "very_large (>100MB)", 0
        )
        score -= min(large_files * 5, 15)  # æœ€å¤§15ç‚¹æ¸›ç‚¹

        return max(score, 0.0)

    def _generate_recommendations(
        self, analysis: Dict[str, Any], health_score: float
    ) -> List[str]:
        """æ”¹å–„æ¨å¥¨äº‹é …ç”Ÿæˆ"""
        recommendations = []

        if health_score < 70:
            recommendations.append(
                "ğŸš¨ ãƒ­ã‚°å¥å…¨æ€§ãŒä½ä¸‹ã—ã¦ã„ã¾ã™ã€‚ç·Šæ€¥å¯¾å¿œãŒå¿…è¦ã§ã™ã€‚"
            )

        error_count = sum(analysis["analysis_patterns"]["error_frequency"].values())
        if error_count > 100:
            recommendations.append(
                f"ğŸ”¥ ã‚¨ãƒ©ãƒ¼æ•°ãŒå¤šã™ãã¾ã™ ({error_count}ä»¶)ã€‚æ ¹æœ¬åŸå› ã®èª¿æŸ»ã‚’æ¨å¥¨ã—ã¾ã™ã€‚"
            )

        perf_issues = len(analysis["analysis_patterns"]["performance_issues"])
        if perf_issues > 10:
            recommendations.append(
                f"âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å•é¡ŒãŒå¤šç™ºã—ã¦ã„ã¾ã™ ({perf_issues}ä»¶)ã€‚æœ€é©åŒ–ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚"
            )

        large_files = analysis["analysis_patterns"]["size_distribution"].get(
            "very_large (>100MB)", 0
        )
        if large_files > 0:
            recommendations.append(
                f"ğŸ“ å¤§å®¹é‡ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãŒ {large_files}å€‹ã‚ã‚Šã¾ã™ã€‚ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®šã‚’è¦‹ç›´ã—ã¦ãã ã•ã„ã€‚"
            )

        if not recommendations:
            recommendations.append("âœ… ãƒ­ã‚°ã‚·ã‚¹ãƒ†ãƒ ã¯å¥å…¨ã«å‹•ä½œã—ã¦ã„ã¾ã™ã€‚")

        return recommendations


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    import argparse

    parser = argparse.ArgumentParser(
        description="Smart Log Manager - o3+Geminiçµ±åˆãƒ­ã‚°ç®¡ç†"
    )
    parser.add_argument(
        "action",
        choices=["rotate", "compress", "cleanup", "analyze", "health"],
        help="å®Ÿè¡Œã™ã‚‹ã‚¢ã‚¯ã‚·ãƒ§ãƒ³",
    )
    parser.add_argument(
        "--log-root", default="runtime/logs", help="ãƒ­ã‚°ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª"
    )
    parser.add_argument("--log-level", default="INFO", help="ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«")

    args = parser.parse_args()

    # ç’°å¢ƒå¤‰æ•°è¨­å®š
    os.environ["LOG_LEVEL"] = args.log_level

    manager = SmartLogManager(args.log_root)

    if args.action == "rotate":
        result = manager.rotate_logs()
        print(json.dumps(result, indent=2, ensure_ascii=False))

    elif args.action == "compress":
        result = manager.compress_old_logs()
        print(json.dumps(result, indent=2, ensure_ascii=False))

    elif args.action == "cleanup":
        result = manager.cleanup_old_archives()
        print(json.dumps(result, indent=2, ensure_ascii=False))

    elif args.action == "analyze":
        result = manager.analyze_log_patterns()
        print(json.dumps(result, indent=2, ensure_ascii=False))

    elif args.action == "health":
        result = manager.generate_health_report()
        print("\n" + "=" * 60)
        print("ğŸ¥ SMART LOG MANAGER - ãƒ˜ãƒ«ã‚¹ãƒ¬ãƒãƒ¼ãƒˆ")
        print("=" * 60)
        print(f"ğŸ“Š å¥å…¨æ€§ã‚¹ã‚³ã‚¢: {result['health_score']:.1f}/100")
        print(f"ğŸ“ ãƒ­ã‚°ãƒ«ãƒ¼ãƒˆ: {result['log_root']}")
        print(f"ğŸ“ˆ åˆ†æãƒ•ã‚¡ã‚¤ãƒ«æ•°: {result['analysis']['total_files']}")
        print(f"ğŸ“ ç·ãƒ­ã‚°è¡Œæ•°: {result['analysis']['total_lines']:,}")
        print("\nğŸ¯ æ¨å¥¨äº‹é …:")
        for rec in result["recommendations"]:
            print(f"  {rec}")
        print("=" * 60)


if __name__ == "__main__":
    main()
