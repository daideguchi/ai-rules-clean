#!/usr/bin/env python3
"""
ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±åˆãƒ»æ•´ç†ã‚·ã‚¹ãƒ†ãƒ 
o3ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã«åŸºã¥ã3DBçµ±ä¸€ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
"""

import json
import shutil
import sqlite3
from datetime import datetime
from pathlib import Path


class DatabaseConsolidation:
    def __init__(self):
        self.project_root = Path("/Users/dd/Desktop/1_dev/coding-rule2")
        self.backup_dir = (
            self.project_root
            / "runtime"
            / "db_backups"
            / datetime.now().strftime("%Y%m%d_%H%M%S")
        )
        self.backup_dir.mkdir(parents=True, exist_ok=True)

        # æ–°ã—ã„çµ±åˆDBè¨­è¨ˆ
        self.target_dbs = {
            "core.db": {
                "description": "ã‚³ã‚¢ã‚·ã‚¹ãƒ†ãƒ  - ãƒ¡ãƒ¢ãƒªã€ã‚»ãƒƒã‚·ãƒ§ãƒ³ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼è¨˜éŒ²",
                "source_dbs": [
                    "runtime/memory/forever_ledger.db",
                    "runtime/memory/user_prompts.db",
                    "runtime/memory/autonomous_growth.db",
                    "runtime/memory/ai_growth.db",
                ],
            },
            "ai_organization.db": {
                "description": "AIçµ„ç¹”ã‚·ã‚¹ãƒ†ãƒ  - tmuxå”èª¿ã€å½¹å‰²ç®¡ç†",
                "source_dbs": [
                    "runtime/databases/ai_organization_bridge.db",
                    "runtime/ai_organization_bridge.db",  # é‡è¤‡ç¢ºèª
                ],
            },
            "enforcement.db": {
                "description": "ã‚¬ãƒãƒŠãƒ³ã‚¹ãƒ»ã‚¨ãƒ³ãƒ•ã‚©ãƒ¼ã‚¹ãƒ¡ãƒ³ãƒˆ - ãƒãƒªã‚·ãƒ¼ã€é•åè¨˜éŒ²",
                "source_dbs": [
                    "runtime/enforcement/policy_decisions.db",
                    "runtime/databases/ultra_correction.db",
                ],
            },
        }

    def backup_existing_dbs(self):
        """æ—¢å­˜DBå…¨ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—"""
        print("ğŸ“ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—é–‹å§‹...")

        backed_up = set()

        for db_pattern in ["**/*.db"]:
            for db_file in self.project_root.glob(db_pattern):
                if db_file.exists() and db_file.name not in backed_up:
                    backup_path = self.backup_dir / db_file.name
                    shutil.copy2(db_file, backup_path)
                    backed_up.add(db_file.name)
                    print(f"  âœ… {db_file.name} â†’ {backup_path}")

        print(f"ğŸ“¦ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Œäº†: {self.backup_dir} ({len(backed_up)}ãƒ•ã‚¡ã‚¤ãƒ«)")

    def analyze_table_schemas(self, db_path):
        """ãƒ†ãƒ¼ãƒ–ãƒ«ã‚¹ã‚­ãƒ¼ãƒåˆ†æ"""
        tables = {}

        try:
            with sqlite3.connect(db_path) as conn:
                cursor = conn.execute(
                    "SELECT name FROM sqlite_master WHERE type='table'"
                )
                table_names = [row[0] for row in cursor.fetchall()]

                for table_name in table_names:
                    cursor = conn.execute(f"PRAGMA table_info({table_name})")
                    columns = cursor.fetchall()

                    cursor = conn.execute(f"SELECT COUNT(*) FROM {table_name}")
                    row_count = cursor.fetchone()[0]

                    tables[table_name] = {
                        "columns": len(columns),
                        "rows": row_count,
                        "schema": columns,
                    }
        except Exception as e:
            print(f"âŒ {db_path} åˆ†æã‚¨ãƒ©ãƒ¼: {e}")

        return tables

    def create_consolidated_db(self, target_name, config):
        """çµ±åˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä½œæˆ"""
        target_path = self.project_root / "runtime" / "databases" / target_name
        target_path.parent.mkdir(parents=True, exist_ok=True)

        print(f"\nğŸ”¨ {target_name} çµ±åˆé–‹å§‹...")
        print(f"ç›®çš„: {config['description']}")

        # æ–°ã—ã„çµ±åˆDBã‚’ä½œæˆ
        with sqlite3.connect(target_path) as target_conn:
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
            target_conn.execute(
                """
                CREATE TABLE IF NOT EXISTS _consolidation_metadata (
                    id INTEGER PRIMARY KEY,
                    source_db TEXT NOT NULL,
                    source_table TEXT NOT NULL,
                    target_table TEXT NOT NULL,
                    migration_time TEXT NOT NULL,
                    row_count INTEGER DEFAULT 0
                )
            """
            )

            # å„ã‚½ãƒ¼ã‚¹DBã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ç§»è¡Œ
            for source_db_path in config["source_dbs"]:
                full_source_path = self.project_root / source_db_path

                if not full_source_path.exists():
                    print(f"  âš ï¸ ã‚¹ã‚­ãƒƒãƒ—: {source_db_path} (å­˜åœ¨ã—ãªã„)")
                    continue

                print(f"  ğŸ“Š {source_db_path} ã‹ã‚‰ç§»è¡Œä¸­...")

                # ã‚½ãƒ¼ã‚¹DBè§£æ
                tables = self.analyze_table_schemas(full_source_path)

                if not tables:
                    print("    âŒ ãƒ†ãƒ¼ãƒ–ãƒ«ãªã—")
                    continue

                # ã‚½ãƒ¼ã‚¹DBã‚’ã‚¢ã‚¿ãƒƒãƒ
                target_conn.execute(
                    f"ATTACH DATABASE '{full_source_path}' AS source_db"
                )

                try:
                    # å„ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ç§»è¡Œ
                    for table_name, table_info in tables.items():
                        if table_name.startswith("sqlite_"):
                            continue

                        # ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆï¼ˆå­˜åœ¨ç¢ºèªï¼‰
                        target_conn.execute(
                            f"CREATE TABLE IF NOT EXISTS {table_name} AS SELECT * FROM source_db.{table_name} WHERE 1=0"
                        )

                        # ãƒ‡ãƒ¼ã‚¿æŒ¿å…¥ï¼ˆé‡è¤‡å›é¿ï¼‰
                        target_conn.execute(
                            f"INSERT OR IGNORE INTO {table_name} SELECT * FROM source_db.{table_name}"
                        )

                        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¨˜éŒ²
                        target_conn.execute(
                            """
                            INSERT INTO _consolidation_metadata
                            (source_db, source_table, target_table, migration_time, row_count)
                            VALUES (?, ?, ?, ?, ?)
                        """,
                            (
                                source_db_path,
                                table_name,
                                table_name,
                                datetime.now().isoformat(),
                                table_info["rows"],
                            ),
                        )

                        print(f"    âœ… {table_name}: {table_info['rows']}è¡Œ")

                finally:
                    target_conn.execute("DETACH DATABASE source_db")

        print(f"  ğŸ¯ {target_name} çµ±åˆå®Œäº†")
        return target_path

    def generate_architecture_report(self):
        """DBã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        report = {
            "consolidation_timestamp": datetime.now().isoformat(),
            "architecture": "3-Database Unified Design (o3 Best Practices)",
            "databases": {},
            "backup_location": str(self.backup_dir),
        }

        for db_name, config in self.target_dbs.items():
            db_path = self.project_root / "runtime" / "databases" / db_name

            if db_path.exists():
                tables = self.analyze_table_schemas(db_path)
                total_rows = sum(table["rows"] for table in tables.values())

                report["databases"][db_name] = {
                    "description": config["description"],
                    "tables": len(tables),
                    "total_rows": total_rows,
                    "file_size_kb": round(db_path.stat().st_size / 1024, 1),
                    "table_details": {
                        name: {"rows": info["rows"], "columns": info["columns"]}
                        for name, info in tables.items()
                    },
                }

        return report

    def consolidate_all(self):
        """å…¨DBçµ±åˆå®Ÿè¡Œ"""
        print("ğŸš€ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±åˆé–‹å§‹")
        print("=" * 60)

        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
        self.backup_existing_dbs()

        # çµ±åˆå®Ÿè¡Œ
        created_dbs = []
        for db_name, config in self.target_dbs.items():
            target_path = self.create_consolidated_db(db_name, config)
            created_dbs.append(target_path)

        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        report = self.generate_architecture_report()

        report_path = self.project_root / "runtime" / "db_consolidation_report.json"
        with open(report_path, "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        print(f"\nğŸ“Š çµ±åˆãƒ¬ãƒãƒ¼ãƒˆ: {report_path}")

        # çµ±è¨ˆè¡¨ç¤º
        print("\nğŸ¯ çµ±åˆçµæœ:")
        for db_name, info in report["databases"].items():
            print(
                f"  - {db_name}: {info['tables']}ãƒ†ãƒ¼ãƒ–ãƒ«, {info['total_rows']}è¡Œ, {info['file_size_kb']}KB"
            )

        return report


def main():
    consolidation = DatabaseConsolidation()
    consolidation.consolidate_all()

    print("\nâœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±åˆå®Œäº†ï¼")
    print("o3ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã«åŸºã¥ã3DBçµ±ä¸€ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’å®Ÿç¾")


if __name__ == "__main__":
    main()
