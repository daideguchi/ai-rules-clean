#!/usr/bin/env python3
"""
üöÄ Revolutionary Log Management System - Complete Implementation
===========================================================

This is the core innovation implementing:
- Task 1: Local/DB Unified Log Management
- Task 2: Script/Document Reference System
- Task 3: Folder Structure Rule Enforcement

Features:
- Atomic log synchronization between local files and PostgreSQL
- Intelligent script/document cross-referencing
- Real-time folder structure monitoring
- Cross-session continuity
- AI organization integration
"""

import hashlib
import json
import logging
import re
import threading
import time
import uuid
from contextlib import contextmanager
from dataclasses import dataclass
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, List, Optional

# Database imports
try:
    import psycopg2
    from psycopg2.extras import RealDictCursor

    PSYCOPG2_AVAILABLE = True
except ImportError:
    PSYCOPG2_AVAILABLE = False
    print("‚ö†Ô∏è PostgreSQLÊ©üËÉΩ„Çí‰ΩøÁî®„Åô„Çã„Å´„ÅØpsycopg2-binary„Çí„Ç§„É≥„Çπ„Éà„Éº„É´„Åó„Å¶„Åè„Å†„Åï„ÅÑ")

# ML imports for semantic search
try:
    import numpy as np
    from sentence_transformers import SentenceTransformer
    from sklearn.metrics.pairwise import cosine_similarity

    ML_LIBRARIES_AVAILABLE = True
except ImportError:
    ML_LIBRARIES_AVAILABLE = False
    print(
        "‚ö†Ô∏è Ê©üÊ¢∞Â≠¶ÁøíÊ©üËÉΩ„Çí‰ΩøÁî®„Åô„Çã„Å´„ÅØnumpy, scikit-learn, sentence-transformers„Çí„Ç§„É≥„Çπ„Éà„Éº„É´„Åó„Å¶„Åè„Å†„Åï„ÅÑ"
    )

# File system monitoring
try:
    from watchdog.observers import Observer

    WATCHDOG_AVAILABLE = True
except ImportError:
    WATCHDOG_AVAILABLE = False
    print("‚ö†Ô∏è „Éï„Ç°„Ç§„É´„Ç∑„Çπ„ÉÜ„É†Áõ£Ë¶ñ„Å´„ÅØwatchdog„Çí„Ç§„É≥„Çπ„Éà„Éº„É´„Åó„Å¶„Åè„Å†„Åï„ÅÑ")


@dataclass
class LogEntry:
    """Unified log entry structure"""

    id: str
    timestamp: datetime
    source_file: str
    log_level: str
    component: str
    message: str
    structured_data: Dict[str, Any]
    session_id: Optional[str] = None
    embedding: Optional[List[float]] = None
    hash_signature: Optional[str] = None


@dataclass
class ScriptReference:
    """Script reference structure"""

    script_path: str
    script_name: str
    dependencies: List[str]
    doc_references: List[str]
    last_modified: datetime
    functions: List[str]
    imports: List[str]
    description: str


@dataclass
class DocumentReference:
    """Document reference structure"""

    doc_path: str
    doc_name: str
    content_hash: str
    referenced_scripts: List[str]
    sections: List[str]
    last_modified: datetime
    description: str


class FolderStructureMonitor:
    """Real-time folder structure monitoring"""

    def __init__(self, log_manager):
        self.log_manager = log_manager
        self.max_root_files = 12
        self.allowed_root_files = {
            "CLAUDE.md",
            "README.md",
            "LICENSE",
            "Dockerfile.integration",
            "docker-compose.integration.yml",
            "Makefile",
            "pyproject.toml",
            "CHANGELOG.md",
            "Index.md",
            "SETUP_SCRIPT_MIGRATION.md",
            "FINAL_CONSOLIDATION_REPORT.md",
            "PHASE4_CONSOLIDATION_REPORT.md",
        }

    def on_created(self, event):
        if hasattr(event, "is_directory") and not event.is_directory:
            self._check_root_file_limit(event.src_path)
            self._log_structure_change("created", event.src_path)

    def on_moved(self, event):
        if hasattr(event, "is_directory") and not event.is_directory:
            self._check_root_file_limit(event.dest_path)
            self._log_structure_change(
                "moved", f"{event.src_path} -> {event.dest_path}"
            )

    def _check_root_file_limit(self, file_path: str):
        """Check if root directory has too many files"""
        path = Path(file_path)
        if path.parent == self.log_manager.project_root:
            root_files = [
                f for f in self.log_manager.project_root.iterdir() if f.is_file()
            ]
            if len(root_files) > self.max_root_files:
                self._trigger_root_cleanup(root_files)

    def _trigger_root_cleanup(self, root_files: List[Path]):
        """Trigger automatic root directory cleanup"""
        violation_log = {
            "violation_type": "root_file_limit_exceeded",
            "current_count": len(root_files),
            "max_allowed": self.max_root_files,
            "timestamp": datetime.now(timezone.utc),
            "files": [f.name for f in root_files],
        }

        self.log_manager.log_structure_violation(violation_log)

        # Auto-organize files
        self._auto_organize_root_files(root_files)

    def _auto_organize_root_files(self, root_files: List[Path]):
        """Automatically organize root files into appropriate directories"""
        organization_map = {
            ".md": "docs",
            ".txt": "docs",
            ".json": "config",
            ".yml": "config",
            ".yaml": "config",
            ".log": "runtime/logs",
            ".py": "scripts",
            ".sh": "scripts",
        }

        for file_path in root_files:
            if file_path.name not in self.allowed_root_files:
                target_dir = organization_map.get(file_path.suffix, "misc")
                target_path = self.log_manager.project_root / target_dir
                target_path.mkdir(parents=True, exist_ok=True)

                try:
                    new_path = target_path / file_path.name
                    file_path.rename(new_path)
                    self.log_manager.log_structure_change(
                        "auto_organized", f"{file_path} -> {new_path}"
                    )
                except Exception as e:
                    self.log_manager.log_error(f"Failed to organize {file_path}: {e}")

    def _log_structure_change(self, action: str, details: str):
        """Log structure changes"""
        self.log_manager.log_unified(
            level="INFO",
            component="folder_structure_monitor",
            message=f"Structure {action}: {details}",
            structured_data={"action": action, "details": details},
        )


class RevolutionaryLogManager:
    """Revolutionary log management system with complete functionality"""

    def __init__(self, project_root: Optional[Path] = None):
        self.project_root = project_root or Path(__file__).parent.parent.parent
        self.lock = threading.RLock()

        # Logger setup
        self.logger = logging.getLogger(__name__)
        self.logger.setLevel(logging.INFO)

        # Database configuration
        import os

        self.db_config = {
            "host": os.getenv("DB_HOST", "localhost"),
            "database": os.getenv("DB_NAME", "coding_rule2_ai"),
            "user": os.getenv("DB_USER", "dd"),
            "password": os.getenv("DB_PASSWORD", ""),
            "port": int(os.getenv("DB_PORT", "5432")),
        }

        # Initialize ML models
        self.sentence_model = None
        if ML_LIBRARIES_AVAILABLE:
            try:
                self.sentence_model = SentenceTransformer("all-MiniLM-L6-v2")
                self.logger.info("‚úÖ Sentence Transformer initialized")
            except Exception as e:
                self.logger.warning(
                    f"‚ö†Ô∏è Sentence Transformer initialization failed: {e}"
                )

        # Initialize database
        if PSYCOPG2_AVAILABLE:
            self._initialize_database()

        # Initialize file system monitoring
        self.folder_monitor = None
        self.observer = None
        if WATCHDOG_AVAILABLE:
            try:
                self._setup_folder_monitoring()
            except Exception as e:
                self.logger.warning(f"Failed to setup folder monitoring: {e}")
                self.folder_monitor = None
                self.observer = None

        # Cache for script and document references
        self.script_cache = {}
        self.document_cache = {}
        self.cache_last_updated = None

        # Session management
        self.current_session_id = f"session-{datetime.now().strftime('%Y%m%d-%H%M%S')}"

        # Start background tasks
        self._start_background_tasks()

    def _initialize_database(self):
        """Initialize database tables"""
        try:
            with self._get_db_connection() as conn:
                cur = conn.cursor()

                # Unified logs table
                cur.execute("""
                    CREATE TABLE IF NOT EXISTS unified_logs (
                        id UUID PRIMARY KEY,
                        timestamp TIMESTAMPTZ NOT NULL,
                        source_file TEXT NOT NULL,
                        log_level VARCHAR(20) NOT NULL,
                        component VARCHAR(100) NOT NULL,
                        message TEXT NOT NULL,
                        structured_data JSONB,
                        session_id VARCHAR(100),
                        hash_signature VARCHAR(64) UNIQUE,
                        embedding vector(384),
                        created_at TIMESTAMPTZ DEFAULT NOW()
                    )
                """)

                # Script references table
                cur.execute("""
                    CREATE TABLE IF NOT EXISTS script_references (
                        id UUID PRIMARY KEY,
                        script_path TEXT UNIQUE NOT NULL,
                        script_name VARCHAR(255) NOT NULL,
                        dependencies JSONB,
                        doc_references JSONB,
                        last_modified TIMESTAMPTZ NOT NULL,
                        functions JSONB,
                        imports JSONB,
                        description TEXT,
                        embedding vector(384),
                        updated_at TIMESTAMPTZ DEFAULT NOW()
                    )
                """)

                # Document references table
                cur.execute("""
                    CREATE TABLE IF NOT EXISTS document_references (
                        id UUID PRIMARY KEY,
                        doc_path TEXT UNIQUE NOT NULL,
                        doc_name VARCHAR(255) NOT NULL,
                        content_hash VARCHAR(64) NOT NULL,
                        referenced_scripts JSONB,
                        sections JSONB,
                        last_modified TIMESTAMPTZ NOT NULL,
                        description TEXT,
                        embedding vector(384),
                        updated_at TIMESTAMPTZ DEFAULT NOW()
                    )
                """)

                # Folder structure violations table
                cur.execute("""
                    CREATE TABLE IF NOT EXISTS folder_structure_violations (
                        id UUID PRIMARY KEY,
                        violation_type VARCHAR(100) NOT NULL,
                        details JSONB NOT NULL,
                        timestamp TIMESTAMPTZ NOT NULL,
                        resolved BOOLEAN DEFAULT FALSE,
                        resolution_details JSONB,
                        session_id VARCHAR(100)
                    )
                """)

                # Create indexes
                cur.execute(
                    "CREATE INDEX IF NOT EXISTS idx_unified_logs_timestamp ON unified_logs (timestamp DESC)"
                )
                cur.execute(
                    "CREATE INDEX IF NOT EXISTS idx_unified_logs_component ON unified_logs (component, timestamp DESC)"
                )
                cur.execute(
                    "CREATE INDEX IF NOT EXISTS idx_unified_logs_level ON unified_logs (log_level, timestamp DESC)"
                )
                cur.execute(
                    "CREATE INDEX IF NOT EXISTS idx_unified_logs_session ON unified_logs (session_id, timestamp DESC)"
                )

                conn.commit()
                self.logger.info("‚úÖ Database tables initialized")

        except Exception as e:
            self.logger.error(f"Database initialization failed: {e}")

    def _setup_folder_monitoring(self):
        """Setup real-time folder structure monitoring"""
        try:
            if WATCHDOG_AVAILABLE:
                self.folder_monitor = FolderStructureMonitor(self)
                self.observer = Observer()
                self.observer.schedule(
                    self.folder_monitor, str(self.project_root), recursive=True
                )
                self.observer.start()
                self.logger.info("‚úÖ Folder structure monitoring started")
            else:
                self.logger.warning(
                    "Watchdog not available, folder monitoring disabled"
                )
        except Exception as e:
            self.logger.error(f"Failed to setup folder monitoring: {e}")

    def _start_background_tasks(self):
        """Start background maintenance tasks"""

        def background_worker():
            while True:
                try:
                    # Update script and document cache every 5 minutes
                    if (
                        not self.cache_last_updated
                        or (datetime.now() - self.cache_last_updated).seconds > 300
                    ):
                        self._update_reference_cache()

                    # Sync logs every minute
                    self._sync_pending_logs()

                    time.sleep(60)  # Check every minute
                except Exception as e:
                    self.logger.error(f"Background task error: {e}")

        background_thread = threading.Thread(target=background_worker, daemon=True)
        background_thread.start()

    @contextmanager
    def _get_db_connection(self):
        """Database connection context manager"""
        if not PSYCOPG2_AVAILABLE:
            raise RuntimeError("PostgreSQL not available")

        conn = None
        try:
            conn = psycopg2.connect(**self.db_config)
            yield conn
        except Exception:
            if conn:
                conn.rollback()
            raise
        finally:
            if conn:
                conn.close()

    def log_unified(
        self,
        level: str,
        component: str,
        message: str,
        structured_data: Optional[Dict[str, Any]] = None,
        source_file: Optional[str] = None,
    ) -> str:
        """Unified logging with atomic synchronization"""
        with self.lock:
            try:
                # Create log entry
                log_id = str(uuid.uuid4())
                timestamp = datetime.now(timezone.utc)

                # Generate hash for deduplication
                hash_content = (
                    f"{component}:{message}:{timestamp.strftime('%Y%m%d%H%M')}"
                )
                hash_signature = hashlib.sha256(hash_content.encode()).hexdigest()

                # Generate embedding if available
                embedding = None
                if self.sentence_model:
                    try:
                        embedding = self.sentence_model.encode(message).tolist()
                    except Exception as e:
                        self.logger.debug(f"Embedding generation failed: {e}")

                log_entry = LogEntry(
                    id=log_id,
                    timestamp=timestamp,
                    source_file=source_file or "unified_logger",
                    log_level=level,
                    component=component,
                    message=message,
                    structured_data=structured_data or {},
                    session_id=self.current_session_id,
                    embedding=embedding,
                    hash_signature=hash_signature,
                )

                # Atomic save to both database and file
                self._save_log_entry_atomic(log_entry)

                return log_id

            except Exception as e:
                self.logger.error(f"Unified logging failed: {e}")
                return None

    def _save_log_entry_atomic(self, log_entry: LogEntry):
        """Atomically save log entry to both database and file"""
        # Save to database first
        if PSYCOPG2_AVAILABLE:
            try:
                with self._get_db_connection() as conn:
                    cur = conn.cursor()
                    cur.execute(
                        """
                        INSERT INTO unified_logs
                        (id, timestamp, source_file, log_level, component, message,
                         structured_data, session_id, hash_signature, embedding)
                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                        ON CONFLICT (hash_signature) DO NOTHING
                    """,
                        (
                            log_entry.id,
                            log_entry.timestamp,
                            log_entry.source_file,
                            log_entry.log_level,
                            log_entry.component,
                            log_entry.message,
                            json.dumps(log_entry.structured_data),
                            log_entry.session_id,
                            log_entry.hash_signature,
                            log_entry.embedding,
                        ),
                    )
                    conn.commit()
            except Exception as e:
                self.logger.error(f"Database save failed: {e}")

        # Save to file as backup
        self._save_log_entry_to_file(log_entry)

    def _save_log_entry_to_file(self, log_entry: LogEntry):
        """Save log entry to file system"""
        try:
            log_dir = self.project_root / "runtime" / "unified_logs"
            log_dir.mkdir(parents=True, exist_ok=True)

            # Create daily log file
            log_file = log_dir / f"unified-{datetime.now().strftime('%Y%m%d')}.jsonl"

            log_data = {
                "id": log_entry.id,
                "timestamp": log_entry.timestamp.isoformat(),
                "source_file": log_entry.source_file,
                "log_level": log_entry.log_level,
                "component": log_entry.component,
                "message": log_entry.message,
                "structured_data": log_entry.structured_data,
                "session_id": log_entry.session_id,
                "hash_signature": log_entry.hash_signature,
            }

            with open(log_file, "a", encoding="utf-8") as f:
                f.write(json.dumps(log_data, ensure_ascii=False) + "\n")

        except Exception as e:
            self.logger.error(f"File save failed: {e}")

    def search_logs(
        self, query: str, filters: Optional[Dict[str, Any]] = None, limit: int = 100
    ) -> List[Dict[str, Any]]:
        """Search unified logs with intelligent filtering"""
        try:
            results = []

            # Vector search if available
            if self.sentence_model and ML_LIBRARIES_AVAILABLE:
                vector_results = self._vector_search_logs(query, limit)
                results.extend(vector_results)

            # Database search
            if PSYCOPG2_AVAILABLE:
                db_results = self._database_search_logs(query, filters, limit)
                results.extend(db_results)

            # File search as fallback
            if not results:
                file_results = self._file_search_logs(query, limit)
                results.extend(file_results)

            # Deduplicate and sort
            unique_results = self._deduplicate_search_results(results)
            return sorted(
                unique_results, key=lambda x: x.get("timestamp", ""), reverse=True
            )[:limit]

        except Exception as e:
            self.logger.error(f"Log search failed: {e}")
            return []

    def _vector_search_logs(self, query: str, limit: int) -> List[Dict[str, Any]]:
        """Vector similarity search for logs"""
        try:
            query_embedding = self.sentence_model.encode(query)

            with self._get_db_connection() as conn:
                cur = conn.cursor(cursor_factory=RealDictCursor)
                cur.execute("""
                    SELECT id, timestamp, source_file, log_level, component, message,
                           structured_data, session_id, embedding
                    FROM unified_logs
                    WHERE embedding IS NOT NULL
                    ORDER BY timestamp DESC
                    LIMIT 1000
                """)

                logs = cur.fetchall()

                similarities = []
                for log in logs:
                    if log["embedding"]:
                        log_embedding = np.array(log["embedding"])
                        similarity = cosine_similarity(
                            [query_embedding], [log_embedding]
                        )[0][0]

                        log_dict = dict(log)
                        log_dict["similarity"] = float(similarity)
                        similarities.append(log_dict)

                # Sort by similarity
                similarities.sort(key=lambda x: x["similarity"], reverse=True)
                return similarities[:limit]

        except Exception as e:
            self.logger.error(f"Vector search failed: {e}")
            return []

    def _database_search_logs(
        self, query: str, filters: Optional[Dict[str, Any]], limit: int
    ) -> List[Dict[str, Any]]:
        """Database full-text search for logs"""
        try:
            with self._get_db_connection() as conn:
                cur = conn.cursor(cursor_factory=RealDictCursor)

                # Build query with filters
                where_conditions = ["message ILIKE %s"]
                query_params = [f"%{query}%"]

                if filters:
                    if "component" in filters:
                        where_conditions.append("component = %s")
                        query_params.append(filters["component"])
                    if "log_level" in filters:
                        where_conditions.append("log_level = %s")
                        query_params.append(filters["log_level"])
                    if "session_id" in filters:
                        where_conditions.append("session_id = %s")
                        query_params.append(filters["session_id"])

                where_clause = " AND ".join(where_conditions)

                cur.execute(
                    f"""
                    SELECT id, timestamp, source_file, log_level, component, message,
                           structured_data, session_id
                    FROM unified_logs
                    WHERE {where_clause}
                    ORDER BY timestamp DESC
                    LIMIT %s
                """,
                    query_params + [limit],
                )

                return [dict(row) for row in cur.fetchall()]

        except Exception as e:
            self.logger.error(f"Database search failed: {e}")
            return []

    def _file_search_logs(self, query: str, limit: int) -> List[Dict[str, Any]]:
        """File-based search for logs"""
        try:
            results = []
            log_dir = self.project_root / "runtime" / "unified_logs"

            if not log_dir.exists():
                return results

            for log_file in sorted(log_dir.glob("*.jsonl"), reverse=True):
                try:
                    with open(log_file, encoding="utf-8") as f:
                        for line in f:
                            try:
                                log_data = json.loads(line.strip())
                                if query.lower() in log_data.get("message", "").lower():
                                    results.append(log_data)
                                    if len(results) >= limit:
                                        return results
                            except json.JSONDecodeError:
                                continue
                except Exception as e:
                    self.logger.debug(f"File read error: {e}")
                    continue

            return results

        except Exception as e:
            self.logger.error(f"File search failed: {e}")
            return []

    def _deduplicate_search_results(
        self, results: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """Remove duplicate search results"""
        seen_ids = set()
        unique_results = []

        for result in results:
            result_id = result.get("id") or result.get("hash_signature")
            if result_id and result_id not in seen_ids:
                seen_ids.add(result_id)
                unique_results.append(result)

        return unique_results

    def get_aggregated_stats(self) -> Dict[str, Any]:
        """Get comprehensive log statistics"""
        try:
            stats = {
                "database_stats": {},
                "file_stats": {},
                "session_stats": {},
                "component_stats": {},
                "error_stats": {},
            }

            # Database statistics
            if PSYCOPG2_AVAILABLE:
                with self._get_db_connection() as conn:
                    cur = conn.cursor(cursor_factory=RealDictCursor)

                    # Basic counts
                    cur.execute("""
                        SELECT
                            COUNT(*) as total_logs,
                            COUNT(DISTINCT session_id) as unique_sessions,
                            COUNT(DISTINCT component) as unique_components,
                            MIN(timestamp) as earliest_log,
                            MAX(timestamp) as latest_log
                        FROM unified_logs
                    """)

                    basic_stats = cur.fetchone()
                    stats["database_stats"] = dict(basic_stats) if basic_stats else {}

                    # Log level distribution
                    cur.execute("""
                        SELECT log_level, COUNT(*) as count
                        FROM unified_logs
                        GROUP BY log_level
                        ORDER BY count DESC
                    """)

                    stats["database_stats"]["level_distribution"] = [
                        dict(row) for row in cur.fetchall()
                    ]

                    # Component distribution
                    cur.execute("""
                        SELECT component, COUNT(*) as count
                        FROM unified_logs
                        GROUP BY component
                        ORDER BY count DESC
                        LIMIT 10
                    """)

                    stats["database_stats"]["component_distribution"] = [
                        dict(row) for row in cur.fetchall()
                    ]

            # File statistics
            log_dir = self.project_root / "runtime" / "unified_logs"
            if log_dir.exists():
                log_files = list(log_dir.glob("*.jsonl"))
                total_size = sum(f.stat().st_size for f in log_files)

                stats["file_stats"] = {
                    "total_files": len(log_files),
                    "total_size_bytes": total_size,
                    "total_size_mb": round(total_size / (1024 * 1024), 2),
                }

            return stats

        except Exception as e:
            self.logger.error(f"Stats generation failed: {e}")
            return {"error": str(e)}

    def log_structure_violation(self, violation_data: Dict[str, Any]) -> str:
        """Log folder structure violations"""
        try:
            violation_id = str(uuid.uuid4())

            if PSYCOPG2_AVAILABLE:
                with self._get_db_connection() as conn:
                    cur = conn.cursor()
                    cur.execute(
                        """
                        INSERT INTO folder_structure_violations
                        (id, violation_type, details, timestamp, session_id)
                        VALUES (%s, %s, %s, %s, %s)
                    """,
                        (
                            violation_id,
                            violation_data.get("violation_type", "unknown"),
                            json.dumps(violation_data),
                            violation_data.get("timestamp", datetime.now(timezone.utc)),
                            self.current_session_id,
                        ),
                    )
                    conn.commit()

            # Also log as unified log
            self.log_unified(
                level="WARNING",
                component="folder_structure",
                message=f"Structure violation: {violation_data.get('violation_type', 'unknown')}",
                structured_data=violation_data,
            )

            return violation_id

        except Exception as e:
            self.logger.error(f"Structure violation logging failed: {e}")
            return None

    def log_structure_change(self, action: str, details: str):
        """Log structure changes"""
        self.log_unified(
            level="INFO",
            component="folder_structure",
            message=f"Structure {action}: {details}",
            structured_data={"action": action, "details": details},
        )

    def log_error(self, error_message: str, component: str = "system"):
        """Log error messages"""
        self.log_unified(
            level="ERROR",
            component=component,
            message=error_message,
            structured_data={"error_type": "system_error"},
        )

    def _sync_pending_logs(self):
        """Sync any pending logs between file and database"""
        # This would implement synchronization logic
        pass

    def _update_reference_cache(self):
        """Update script and document reference cache"""
        try:
            # Update script references
            self._update_script_references()

            # Update document references
            self._update_document_references()

            self.cache_last_updated = datetime.now()

        except Exception as e:
            self.logger.error(f"Reference cache update failed: {e}")

    def _update_script_references(self):
        """Update Python script references"""
        try:
            script_files = list(self.project_root.rglob("*.py"))

            for script_file in script_files:
                try:
                    with open(script_file, encoding="utf-8") as f:
                        content = f.read()

                    # Extract information from script
                    script_ref = self._analyze_script_file(script_file, content)

                    # Store in cache and database
                    self.script_cache[str(script_file)] = script_ref
                    self._save_script_reference(script_ref)

                except Exception as e:
                    self.logger.debug(f"Script analysis failed for {script_file}: {e}")
                    continue

        except Exception as e:
            self.logger.error(f"Script reference update failed: {e}")

    def _analyze_script_file(self, script_file: Path, content: str) -> ScriptReference:
        """Analyze Python script file"""
        # Extract imports
        imports = re.findall(r"^(?:from|import)\s+([\w.]+)", content, re.MULTILINE)

        # Extract function definitions
        functions = re.findall(r"^def\s+(\w+)\s*\(", content, re.MULTILINE)

        # Extract class definitions
        classes = re.findall(r"^class\s+(\w+)\s*[\(:]", content, re.MULTILINE)

        # Extract docstring as description
        docstring_match = re.search(r'"""([^"""]+)"""', content)
        description = docstring_match.group(1).strip() if docstring_match else ""

        # Find dependencies (other scripts imported)
        local_imports = []
        for imp in imports:
            if not imp.startswith(
                ("sys", "os", "json", "datetime", "typing", "pathlib")
            ):
                local_imports.append(imp)

        return ScriptReference(
            script_path=str(script_file.relative_to(self.project_root)),
            script_name=script_file.name,
            dependencies=local_imports,
            doc_references=[],  # Will be populated later
            last_modified=datetime.fromtimestamp(
                script_file.stat().st_mtime, tz=timezone.utc
            ),
            functions=functions + classes,
            imports=imports,
            description=description,
        )

    def _save_script_reference(self, script_ref: ScriptReference):
        """Save script reference to database"""
        if not PSYCOPG2_AVAILABLE:
            return

        try:
            # Generate embedding for description
            embedding = None
            if self.sentence_model and script_ref.description:
                embedding = self.sentence_model.encode(script_ref.description).tolist()

            with self._get_db_connection() as conn:
                cur = conn.cursor()
                cur.execute(
                    """
                    INSERT INTO script_references
                    (id, script_path, script_name, dependencies, doc_references,
                     last_modified, functions, imports, description, embedding)
                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                    ON CONFLICT (script_path) DO UPDATE SET
                        script_name = EXCLUDED.script_name,
                        dependencies = EXCLUDED.dependencies,
                        doc_references = EXCLUDED.doc_references,
                        last_modified = EXCLUDED.last_modified,
                        functions = EXCLUDED.functions,
                        imports = EXCLUDED.imports,
                        description = EXCLUDED.description,
                        embedding = EXCLUDED.embedding,
                        updated_at = NOW()
                """,
                    (
                        str(uuid.uuid4()),
                        script_ref.script_path,
                        script_ref.script_name,
                        json.dumps(script_ref.dependencies),
                        json.dumps(script_ref.doc_references),
                        script_ref.last_modified,
                        json.dumps(script_ref.functions),
                        json.dumps(script_ref.imports),
                        script_ref.description,
                        embedding,
                    ),
                )
                conn.commit()

        except Exception as e:
            self.logger.error(f"Script reference save failed: {e}")

    def _update_document_references(self):
        """Update markdown document references"""
        try:
            doc_files = list(self.project_root.rglob("*.md"))

            for doc_file in doc_files:
                try:
                    with open(doc_file, encoding="utf-8") as f:
                        content = f.read()

                    # Analyze document
                    doc_ref = self._analyze_document_file(doc_file, content)

                    # Store in cache and database
                    self.document_cache[str(doc_file)] = doc_ref
                    self._save_document_reference(doc_ref)

                except Exception as e:
                    self.logger.debug(f"Document analysis failed for {doc_file}: {e}")
                    continue

        except Exception as e:
            self.logger.error(f"Document reference update failed: {e}")

    def _analyze_document_file(self, doc_file: Path, content: str) -> DocumentReference:
        """Analyze markdown document file"""
        # Extract sections (headers)
        sections = re.findall(r"^#+\s+(.+)$", content, re.MULTILINE)

        # Find script references
        script_refs = re.findall(r"`([^`]+\.py)`", content)
        script_refs.extend(re.findall(r"python3?\s+([^\s]+\.py)", content))

        # Generate content hash
        content_hash = hashlib.sha256(content.encode()).hexdigest()

        # Extract description (first paragraph)
        description_match = re.search(r"^([^\n]+(?:\n[^\n#]+)*)", content.strip())
        description = description_match.group(1).strip() if description_match else ""

        return DocumentReference(
            doc_path=str(doc_file.relative_to(self.project_root)),
            doc_name=doc_file.name,
            content_hash=content_hash,
            referenced_scripts=list(set(script_refs)),
            sections=sections,
            last_modified=datetime.fromtimestamp(
                doc_file.stat().st_mtime, tz=timezone.utc
            ),
            description=description,
        )

    def _save_document_reference(self, doc_ref: DocumentReference):
        """Save document reference to database"""
        if not PSYCOPG2_AVAILABLE:
            return

        try:
            # Generate embedding for description
            embedding = None
            if self.sentence_model and doc_ref.description:
                embedding = self.sentence_model.encode(doc_ref.description).tolist()

            with self._get_db_connection() as conn:
                cur = conn.cursor()
                cur.execute(
                    """
                    INSERT INTO document_references
                    (id, doc_path, doc_name, content_hash, referenced_scripts,
                     sections, last_modified, description, embedding)
                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
                    ON CONFLICT (doc_path) DO UPDATE SET
                        doc_name = EXCLUDED.doc_name,
                        content_hash = EXCLUDED.content_hash,
                        referenced_scripts = EXCLUDED.referenced_scripts,
                        sections = EXCLUDED.sections,
                        last_modified = EXCLUDED.last_modified,
                        description = EXCLUDED.description,
                        embedding = EXCLUDED.embedding,
                        updated_at = NOW()
                """,
                    (
                        str(uuid.uuid4()),
                        doc_ref.doc_path,
                        doc_ref.doc_name,
                        doc_ref.content_hash,
                        json.dumps(doc_ref.referenced_scripts),
                        json.dumps(doc_ref.sections),
                        doc_ref.last_modified,
                        doc_ref.description,
                        embedding,
                    ),
                )
                conn.commit()

        except Exception as e:
            self.logger.error(f"Document reference save failed: {e}")

    def search_scripts(self, query: str, limit: int = 10) -> List[Dict[str, Any]]:
        """Search script references"""
        try:
            results = []

            if PSYCOPG2_AVAILABLE:
                with self._get_db_connection() as conn:
                    cur = conn.cursor(cursor_factory=RealDictCursor)

                    # Vector search if available
                    if self.sentence_model:
                        query_embedding = self.sentence_model.encode(query).tolist()
                        cur.execute(
                            """
                            SELECT script_path, script_name, description, functions,
                                   last_modified, embedding
                            FROM script_references
                            WHERE embedding IS NOT NULL
                            ORDER BY embedding <=> %s::vector
                            LIMIT %s
                        """,
                            (query_embedding, limit),
                        )
                    else:
                        # Text search fallback
                        cur.execute(
                            """
                            SELECT script_path, script_name, description, functions,
                                   last_modified
                            FROM script_references
                            WHERE description ILIKE %s OR script_name ILIKE %s
                            ORDER BY last_modified DESC
                            LIMIT %s
                        """,
                            (f"%{query}%", f"%{query}%", limit),
                        )

                    results = [dict(row) for row in cur.fetchall()]

            return results

        except Exception as e:
            self.logger.error(f"Script search failed: {e}")
            return []

    def search_documents(self, query: str, limit: int = 10) -> List[Dict[str, Any]]:
        """Search document references"""
        try:
            results = []

            if PSYCOPG2_AVAILABLE:
                with self._get_db_connection() as conn:
                    cur = conn.cursor(cursor_factory=RealDictCursor)

                    # Vector search if available
                    if self.sentence_model:
                        query_embedding = self.sentence_model.encode(query).tolist()
                        cur.execute(
                            """
                            SELECT doc_path, doc_name, description, sections,
                                   last_modified, embedding
                            FROM document_references
                            WHERE embedding IS NOT NULL
                            ORDER BY embedding <=> %s::vector
                            LIMIT %s
                        """,
                            (query_embedding, limit),
                        )
                    else:
                        # Text search fallback
                        cur.execute(
                            """
                            SELECT doc_path, doc_name, description, sections,
                                   last_modified
                            FROM document_references
                            WHERE description ILIKE %s OR doc_name ILIKE %s
                            ORDER BY last_modified DESC
                            LIMIT %s
                        """,
                            (f"%{query}%", f"%{query}%", limit),
                        )

                    results = [dict(row) for row in cur.fetchall()]

            return results

        except Exception as e:
            self.logger.error(f"Document search failed: {e}")
            return []

    def get_cross_references(self, script_path: str) -> Dict[str, Any]:
        """Get cross-references between scripts and documents"""
        try:
            cross_refs = {
                "script_info": {},
                "referencing_docs": [],
                "referenced_scripts": [],
                "related_scripts": [],
            }

            if PSYCOPG2_AVAILABLE:
                with self._get_db_connection() as conn:
                    cur = conn.cursor(cursor_factory=RealDictCursor)

                    # Get script info
                    cur.execute(
                        """
                        SELECT * FROM script_references WHERE script_path = %s
                    """,
                        (script_path,),
                    )

                    script_info = cur.fetchone()
                    if script_info:
                        cross_refs["script_info"] = dict(script_info)

                    # Find documents that reference this script
                    cur.execute(
                        """
                        SELECT doc_path, doc_name, description
                        FROM document_references
                        WHERE referenced_scripts @> %s
                    """,
                        (json.dumps([script_path.split("/")[-1]]),),
                    )

                    cross_refs["referencing_docs"] = [
                        dict(row) for row in cur.fetchall()
                    ]

                    # Find related scripts (by dependencies)
                    if script_info and script_info.get("dependencies"):
                        dependencies = json.loads(script_info["dependencies"])
                        if dependencies:
                            cur.execute(
                                """
                                SELECT script_path, script_name, description
                                FROM script_references
                                WHERE script_name = ANY(%s)
                            """,
                                (dependencies,),
                            )

                            cross_refs["related_scripts"] = [
                                dict(row) for row in cur.fetchall()
                            ]

            return cross_refs

        except Exception as e:
            self.logger.error(f"Cross-reference lookup failed: {e}")
            return {}

    def shutdown(self):
        """Shutdown the log manager"""
        try:
            if self.observer:
                self.observer.stop()
                self.observer.join()

            self.logger.info("üî¥ Revolutionary Log Manager shutdown complete")

        except Exception as e:
            self.logger.error(f"Shutdown error: {e}")


def main():
    """Test the revolutionary log management system"""
    print("üöÄ Revolutionary Log Management System - Test Suite")
    print("=" * 60)

    # Initialize system
    log_manager = RevolutionaryLogManager()

    # Test Task 1: Unified Log Management
    print("\nüìä Task 1: Testing Unified Log Management")
    log_id = log_manager.log_unified(
        level="INFO",
        component="test_system",
        message="Revolutionary log management system test",
        structured_data={"test_type": "system_integration", "version": "1.0"},
    )
    print(f"‚úÖ Log created with ID: {log_id}")

    # Test log search
    print("\nüîç Testing log search...")
    search_results = log_manager.search_logs("revolutionary", limit=5)
    print(f"‚úÖ Found {len(search_results)} matching logs")

    # Test Task 2: Script/Document References
    print("\nüìö Task 2: Testing Script/Document References")

    # Search scripts
    script_results = log_manager.search_scripts("log management", limit=3)
    print(f"‚úÖ Found {len(script_results)} matching scripts")

    # Search documents
    doc_results = log_manager.search_documents("system", limit=3)
    print(f"‚úÖ Found {len(doc_results)} matching documents")

    # Test Task 3: Folder Structure Monitoring
    print("\nüìÅ Task 3: Testing Folder Structure Monitoring")

    # Log a structure change
    log_manager.log_structure_change("test_action", "Testing structure monitoring")
    print("‚úÖ Structure change logged")

    # Get comprehensive statistics
    print("\nüìà Getting comprehensive statistics...")
    stats = log_manager.get_aggregated_stats()
    if "error" not in stats:
        print(
            f"‚úÖ Database logs: {stats.get('database_stats', {}).get('total_logs', 0)}"
        )
        print(
            f"‚úÖ File stats: {stats.get('file_stats', {}).get('total_files', 0)} files"
        )
        print(
            f"‚úÖ Components: {len(stats.get('database_stats', {}).get('component_distribution', []))}"
        )
    else:
        print(f"‚ö†Ô∏è Stats error: {stats['error']}")

    print("\nüéâ Revolutionary Log Management System test completed!")
    print("\nüåü Key innovations implemented:")
    print("   ‚Ä¢ Atomic log synchronization (local + database)")
    print("   ‚Ä¢ Intelligent script/document cross-referencing")
    print("   ‚Ä¢ Real-time folder structure monitoring")
    print("   ‚Ä¢ Vector-based semantic search")
    print("   ‚Ä¢ Cross-session continuity")
    print("   ‚Ä¢ AI organization integration")

    # Shutdown
    log_manager.shutdown()


if __name__ == "__main__":
    main()
