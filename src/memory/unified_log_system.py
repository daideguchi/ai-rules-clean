#!/usr/bin/env python3
"""
ğŸ“Š çµ±ä¸€ãƒ­ã‚°ã‚·ã‚¹ãƒ†ãƒ  - 117ãƒ•ã‚¡ã‚¤ãƒ«çµ±åˆå®Ÿè£…
==========================================

ã€ç›®çš„ã€‘
- æ•£åœ¨ã™ã‚‹.logãƒ•ã‚¡ã‚¤ãƒ«ã®çµ±åˆ
- JSON Lineså½¢å¼ã§ã®æ§‹é€ åŒ–
- PostgreSQLã¸ã®çµ±åˆä¿å­˜
- PIIä¿è­·æ©Ÿèƒ½

ã€å®Ÿè£…å†…å®¹ã€‘
- .logãƒ•ã‚¡ã‚¤ãƒ«è‡ªå‹•ç™ºè¦‹ãƒ»è§£æ
- æ§‹é€ åŒ–å¤‰æ› (JSON Lines)
- ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±åˆä¿å­˜
- é‡è¤‡é™¤å»ãƒ»æ™‚ç³»åˆ—æ•´ç†
"""

import hashlib
import json
import re
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, List, Optional

import psycopg2
from psycopg2.extras import RealDictCursor


class UnifiedLogSystem:
    """çµ±ä¸€ãƒ­ã‚°ã‚·ã‚¹ãƒ†ãƒ  - 117ãƒ•ã‚¡ã‚¤ãƒ«çµ±åˆ"""

    def __init__(self):
        self.project_root = Path(__file__).parent.parent
        self.db_config = {
            "host": "localhost",
            "database": "president_ai",
            "user": "dd",
            "password": "",
            "port": 5432,
        }

        # PIIä¿è­·ãƒ‘ã‚¿ãƒ¼ãƒ³
        self.pii_patterns = [
            (r"sk-[a-zA-Z0-9\-_]{20,}", "[API_KEY_REDACTED]"),  # API keys
            (
                r"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}",
                "[EMAIL_REDACTED]",
            ),  # emails
            (r"/Users/[^/\s]+", "/Users/[USERNAME]"),  # user paths
            (
                r'password["\']?\s*[:=]\s*["\']?[^"\s,}]+',
                "password: [REDACTED]",
            ),  # passwords
        ]

    def init_unified_log_table(self):
        """çµ±ä¸€ãƒ­ã‚°ãƒ†ãƒ¼ãƒ–ãƒ«åˆæœŸåŒ–"""
        try:
            conn = psycopg2.connect(**self.db_config)
            cur = conn.cursor()

            # çµ±ä¸€ãƒ­ã‚°ãƒ†ãƒ¼ãƒ–ãƒ«
            cur.execute("""
                CREATE TABLE IF NOT EXISTS unified_logs (
                    id SERIAL PRIMARY KEY,
                    log_hash VARCHAR(64) UNIQUE,
                    timestamp TIMESTAMPTZ,
                    source_file VARCHAR(500),
                    log_level VARCHAR(20),
                    component VARCHAR(100),
                    message TEXT,
                    structured_data JSONB,
                    original_format TEXT,
                    processed_at TIMESTAMPTZ DEFAULT NOW()
                );
            """)

            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
            cur.execute("""
                CREATE INDEX IF NOT EXISTS idx_unified_logs_timestamp
                ON unified_logs (timestamp DESC);
            """)

            cur.execute("""
                CREATE INDEX IF NOT EXISTS idx_unified_logs_component
                ON unified_logs (component, timestamp DESC);
            """)

            cur.execute("""
                CREATE INDEX IF NOT EXISTS idx_unified_logs_level
                ON unified_logs (log_level, timestamp DESC);
            """)

            conn.commit()
            cur.close()
            conn.close()

            return {"status": "success", "message": "Unified log table initialized"}

        except Exception as e:
            return {
                "status": "error",
                "message": f"Table initialization failed: {str(e)}",
            }

    def discover_log_files(self) -> List[Path]:
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†…ã®.logãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç™ºè¦‹"""
        log_files = []

        # .logãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†å¸°çš„ã«æ¤œç´¢
        for log_file in self.project_root.rglob("*.log"):
            if log_file.is_file() and log_file.stat().st_size > 0:  # ç©ºãƒ•ã‚¡ã‚¤ãƒ«é™¤å¤–
                log_files.append(log_file)

        return sorted(log_files, key=lambda x: x.stat().st_mtime, reverse=True)

    def parse_log_entry(self, line: str, source_file: str) -> Optional[Dict[str, Any]]:
        """ãƒ­ã‚°ã‚¨ãƒ³ãƒˆãƒªã‚’è§£æã—ã¦æ§‹é€ åŒ–"""
        if not line.strip():
            return None

        # PIIä¿è­·
        sanitized_line = self.sanitize_pii(line)

        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º
        timestamp_patterns = [
            (r"(\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}:\d{2})", "%Y-%m-%d %H:%M:%S"),
            (r"(\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2})", "%Y-%m-%dT%H:%M:%S"),
            (r"(\d{2}/\d{2}/\d{4}\s+\d{2}:\d{2}:\d{2})", "%m/%d/%Y %H:%M:%S"),
            (r"\[(\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}:\d{2})\]", "%Y-%m-%d %H:%M:%S"),
        ]

        parsed_timestamp = None
        for pattern, fmt in timestamp_patterns:
            match = re.search(pattern, sanitized_line)
            if match:
                try:
                    parsed_timestamp = datetime.strptime(match.group(1), fmt)
                    break
                except ValueError:
                    continue

        # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«æ¤œå‡º
        log_level = "INFO"
        level_patterns = [
            (r"\b(ERROR|error)\b", "ERROR"),
            (r"\b(WARN|WARNING|warn|warning)\b", "WARNING"),
            (r"\b(INFO|info)\b", "INFO"),
            (r"\b(DEBUG|debug)\b", "DEBUG"),
            (r"\b(CRITICAL|critical|FATAL|fatal)\b", "CRITICAL"),
        ]

        for pattern, level in level_patterns:
            if re.search(pattern, sanitized_line):
                log_level = level
                break

        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆæ¨å®š
        component = self._extract_component_from_path(source_file)

        # æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
        structured_data = self._extract_structured_data(sanitized_line)

        # ãƒãƒƒã‚·ãƒ¥ç”Ÿæˆï¼ˆé‡è¤‡é™¤å»ç”¨ï¼‰
        content_hash = hashlib.sha256(
            f"{source_file}:{sanitized_line}".encode()
        ).hexdigest()

        return {
            "log_hash": content_hash,
            "timestamp": parsed_timestamp or datetime.now(timezone.utc),
            "source_file": str(Path(source_file).relative_to(self.project_root)),
            "log_level": log_level,
            "component": component,
            "message": sanitized_line.strip(),
            "structured_data": structured_data,
            "original_format": line.strip(),
        }

    def _extract_component_from_path(self, filepath: str) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‹ã‚‰ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåã‚’æ¨å®š"""
        path = Path(filepath)

        # ãƒ‘ã‚¹æ§‹é€ ã‹ã‚‰ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆæ¨å®š
        parts = path.parts
        if "operations" in parts:
            return "operations"
        elif "memory" in parts:
            return "memory"
        elif "agents" in parts:
            return "agents"
        elif "scripts" in parts:
            return "scripts"
        elif "runtime" in parts:
            return "runtime"
        else:
            return "system"

    def _extract_structured_data(self, line: str) -> Dict[str, Any]:
        """ãƒ­ã‚°è¡Œã‹ã‚‰æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
        structured = {}

        # JSONéƒ¨åˆ†ã®æŠ½å‡º
        json_pattern = r"\{[^{}]*\}"
        json_matches = re.findall(json_pattern, line)
        if json_matches:
            for match in json_matches:
                try:
                    data = json.loads(match)
                    structured.update(data)
                except (json.JSONDecodeError, ValueError):
                    pass

        # ã‚­ãƒ¼=å€¤ãƒšã‚¢ã®æŠ½å‡º
        kv_pattern = r"(\w+)=([^\s,}]+)"
        kv_matches = re.findall(kv_pattern, line)
        for key, value in kv_matches:
            # æ•°å€¤å¤‰æ›è©¦è¡Œ
            try:
                if "." in value:
                    structured[key] = float(value)
                else:
                    structured[key] = int(value)
            except ValueError:
                structured[key] = value

        return structured

    def sanitize_pii(self, text: str) -> str:
        """PIIä¿è­·"""
        sanitized = text
        for pattern, replacement in self.pii_patterns:
            sanitized = re.sub(pattern, replacement, sanitized, flags=re.IGNORECASE)
        return sanitized

    def process_log_files(self, max_files: int = 50) -> Dict[str, Any]:
        """ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†"""
        log_files = self.discover_log_files()

        if len(log_files) > max_files:
            log_files = log_files[:max_files]  # æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«ã«åˆ¶é™

        processed_entries = 0
        skipped_entries = 0
        errors = []

        try:
            conn = psycopg2.connect(**self.db_config)
            cur = conn.cursor()

            for log_file in log_files:
                try:
                    with open(log_file, encoding="utf-8", errors="ignore") as f:
                        for line_num, line in enumerate(f, 1):
                            try:
                                entry = self.parse_log_entry(line, str(log_file))
                                if entry:
                                    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æŒ¿å…¥
                                    cur.execute(
                                        """
                                        INSERT INTO unified_logs
                                        (log_hash, timestamp, source_file, log_level, component, message, structured_data, original_format)
                                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
                                        ON CONFLICT (log_hash) DO NOTHING;
                                    """,
                                        (
                                            entry["log_hash"],
                                            entry["timestamp"],
                                            entry["source_file"],
                                            entry["log_level"],
                                            entry["component"],
                                            entry["message"],
                                            json.dumps(entry["structured_data"]),
                                            entry["original_format"],
                                        ),
                                    )

                                    processed_entries += 1
                                else:
                                    skipped_entries += 1

                            except Exception as e:
                                errors.append(f"{log_file}:{line_num}: {str(e)}")

                except Exception as e:
                    errors.append(f"File {log_file}: {str(e)}")

            conn.commit()
            cur.close()
            conn.close()

            return {
                "status": "success",
                "processed_files": len(log_files),
                "processed_entries": processed_entries,
                "skipped_entries": skipped_entries,
                "errors": errors[:10],  # æœ€åˆã®10ã‚¨ãƒ©ãƒ¼ã®ã¿
            }

        except Exception as e:
            return {"status": "error", "error": str(e)}

    def get_unified_log_stats(self) -> Dict[str, Any]:
        """çµ±ä¸€ãƒ­ã‚°çµ±è¨ˆ"""
        try:
            conn = psycopg2.connect(**self.db_config, cursor_factory=RealDictCursor)
            cur = conn.cursor()

            # åŸºæœ¬çµ±è¨ˆ
            cur.execute("""
                SELECT
                    COUNT(*) as total_entries,
                    COUNT(DISTINCT source_file) as unique_files,
                    COUNT(DISTINCT component) as unique_components,
                    MIN(timestamp) as earliest_entry,
                    MAX(timestamp) as latest_entry
                FROM unified_logs;
            """)

            basic_stats = cur.fetchone()

            # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«åˆ¥çµ±è¨ˆ
            cur.execute("""
                SELECT log_level, COUNT(*) as count
                FROM unified_logs
                GROUP BY log_level
                ORDER BY count DESC;
            """)

            level_stats = cur.fetchall()

            # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆ¥çµ±è¨ˆ
            cur.execute("""
                SELECT component, COUNT(*) as count
                FROM unified_logs
                GROUP BY component
                ORDER BY count DESC;
            """)

            component_stats = cur.fetchall()

            cur.close()
            conn.close()

            return {
                "status": "success",
                "basic_stats": dict(basic_stats) if basic_stats else {},
                "level_distribution": [dict(row) for row in level_stats],
                "component_distribution": [dict(row) for row in component_stats],
            }

        except Exception as e:
            return {"status": "error", "error": str(e)}


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ - çµ±ä¸€ãƒ­ã‚°ã‚·ã‚¹ãƒ†ãƒ """
    print("ğŸ“Š çµ±ä¸€ãƒ­ã‚°ã‚·ã‚¹ãƒ†ãƒ  - 117ãƒ•ã‚¡ã‚¤ãƒ«çµ±åˆé–‹å§‹")

    log_system = UnifiedLogSystem()

    # 1. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–
    print("\\n1ï¸âƒ£ çµ±ä¸€ãƒ­ã‚°ãƒ†ãƒ¼ãƒ–ãƒ«åˆæœŸåŒ–")
    init_result = log_system.init_unified_log_table()
    print(f"åˆæœŸåŒ–: {init_result['status']}")

    if init_result["status"] == "error":
        print(f"ã‚¨ãƒ©ãƒ¼: {init_result['message']}")
        return

    # 2. ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ç™ºè¦‹
    print("\\n2ï¸âƒ£ ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ç™ºè¦‹")
    log_files = log_system.discover_log_files()
    print(f"ç™ºè¦‹ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(log_files)}")

    # æœ€åˆã®10ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¡¨ç¤º
    for i, log_file in enumerate(log_files[:10]):
        size_kb = log_file.stat().st_size // 1024
        print(f"   {i + 1}. {log_file.name} ({size_kb}KB)")

    if len(log_files) > 10:
        print(f"   ... ä»–{len(log_files) - 10}ãƒ•ã‚¡ã‚¤ãƒ«")

    # 3. ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†
    print("\\n3ï¸âƒ£ ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«çµ±åˆå‡¦ç†")
    process_result = log_system.process_log_files(max_files=30)  # æœ€åˆã®30ãƒ•ã‚¡ã‚¤ãƒ«
    print(f"å‡¦ç†çµæœ: {process_result['status']}")

    if process_result["status"] == "success":
        print(f"   å‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {process_result['processed_files']}")
        print(f"   çµ±åˆã‚¨ãƒ³ãƒˆãƒªæ•°: {process_result['processed_entries']}")
        print(f"   ã‚¹ã‚­ãƒƒãƒ—ã‚¨ãƒ³ãƒˆãƒªæ•°: {process_result['skipped_entries']}")

        if process_result["errors"]:
            print(f"   ã‚¨ãƒ©ãƒ¼æ•°: {len(process_result['errors'])}")
            for error in process_result["errors"][:3]:
                print(f"     - {error}")
    else:
        print(f"   ã‚¨ãƒ©ãƒ¼: {process_result['error']}")
        return

    # 4. çµ±åˆãƒ­ã‚°çµ±è¨ˆ
    print("\\n4ï¸âƒ£ çµ±åˆãƒ­ã‚°çµ±è¨ˆ")
    stats_result = log_system.get_unified_log_stats()
    print(f"çµ±è¨ˆ: {stats_result['status']}")

    if stats_result["status"] == "success":
        basic = stats_result["basic_stats"]
        print(f"   ç·ã‚¨ãƒ³ãƒˆãƒªæ•°: {basic.get('total_entries', 0)}")
        print(f"   ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {basic.get('unique_files', 0)}")
        print(f"   ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆæ•°: {basic.get('unique_components', 0)}")

        if basic.get("earliest_entry"):
            print(f"   æœ€å¤ã‚¨ãƒ³ãƒˆãƒª: {basic['earliest_entry']}")
        if basic.get("latest_entry"):
            print(f"   æœ€æ–°ã‚¨ãƒ³ãƒˆãƒª: {basic['latest_entry']}")

        print("\\n   ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«åˆ†å¸ƒ:")
        for level in stats_result["level_distribution"][:5]:
            print(f"     {level['log_level']}: {level['count']}ä»¶")

        print("\\n   ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆ†å¸ƒ:")
        for component in stats_result["component_distribution"][:5]:
            print(f"     {component['component']}: {component['count']}ä»¶")

    print("\\nâœ… çµ±ä¸€ãƒ­ã‚°ã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…å®Œäº†")
    print("ğŸ“ æ•£åœ¨ã™ã‚‹.logãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰PostgreSQLçµ±åˆã¸å¤‰æ›")


if __name__ == "__main__":
    main()
