#!/usr/bin/env python3
"""
üìä Áµ±‰∏Ä„É≠„Ç∞„Ç∑„Çπ„ÉÜ„É† - 117„Éï„Ç°„Ç§„É´Áµ±ÂêàÂÆüË£Ö
==========================================

„ÄêÁõÆÁöÑ„Äë
- Êï£Âú®„Åô„Çã.log„Éï„Ç°„Ç§„É´„ÅÆÁµ±Âêà
- JSON LinesÂΩ¢Âºè„Åß„ÅÆÊßãÈÄ†Âåñ
- PostgreSQL„Å∏„ÅÆÁµ±Âêà‰øùÂ≠ò
- PII‰øùË≠∑Ê©üËÉΩ

„ÄêÂÆüË£ÖÂÜÖÂÆπ„Äë
- .log„Éï„Ç°„Ç§„É´Ëá™ÂãïÁô∫Ë¶ã„ÉªËß£Êûê
- ÊßãÈÄ†ÂåñÂ§âÊèõ (JSON Lines)
- „Éá„Éº„Çø„Éô„Éº„ÇπÁµ±Âêà‰øùÂ≠ò
- ÈáçË§áÈô§Âéª„ÉªÊôÇÁ≥ªÂàóÊï¥ÁêÜ
"""

import hashlib
import json
import re
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, List, Optional

import psycopg2
from psycopg2.extras import RealDictCursor


class UnifiedLogSystem:
    """Áµ±‰∏Ä„É≠„Ç∞„Ç∑„Çπ„ÉÜ„É† - 117„Éï„Ç°„Ç§„É´Áµ±Âêà"""

    def __init__(self):
        self.project_root = Path(__file__).parent.parent
        self.db_config = {
            "host": "localhost",
            "database": "president_ai",
            "user": "dd",
            "password": "",
            "port": 5432,
        }

        # PII‰øùË≠∑„Éë„Çø„Éº„É≥
        self.pii_patterns = [
            (r"sk-[a-zA-Z0-9\-_]{20,}", "[API_KEY_REDACTED]"),  # API keys
            (
                r"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}",
                "[EMAIL_REDACTED]",
            ),  # emails
            (r"/Users/[^/\s]+", "/Users/[USERNAME]"),  # user paths
            (
                r'password["\']?\s*[:=]\s*["\']?[^"\s,}]+',
                "password: [REDACTED]",
            ),  # passwords
        ]

    def init_unified_log_table(self):
        """Áµ±‰∏Ä„É≠„Ç∞„ÉÜ„Éº„Éñ„É´ÂàùÊúüÂåñ"""
        try:
            conn = psycopg2.connect(**self.db_config)
            cur = conn.cursor()

            # Áµ±‰∏Ä„É≠„Ç∞„ÉÜ„Éº„Éñ„É´
            cur.execute("""
                CREATE TABLE IF NOT EXISTS unified_logs (
                    id SERIAL PRIMARY KEY,
                    log_hash VARCHAR(64) UNIQUE,
                    timestamp TIMESTAMPTZ,
                    source_file VARCHAR(500),
                    log_level VARCHAR(20),
                    component VARCHAR(100),
                    message TEXT,
                    structured_data JSONB,
                    original_format TEXT,
                    processed_at TIMESTAMPTZ DEFAULT NOW()
                );
            """)

            # „Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ‰ΩúÊàê
            cur.execute("""
                CREATE INDEX IF NOT EXISTS idx_unified_logs_timestamp
                ON unified_logs (timestamp DESC);
            """)

            cur.execute("""
                CREATE INDEX IF NOT EXISTS idx_unified_logs_component
                ON unified_logs (component, timestamp DESC);
            """)

            cur.execute("""
                CREATE INDEX IF NOT EXISTS idx_unified_logs_level
                ON unified_logs (log_level, timestamp DESC);
            """)

            conn.commit()
            cur.close()
            conn.close()

            return {"status": "success", "message": "Unified log table initialized"}

        except Exception as e:
            return {
                "status": "error",
                "message": f"Table initialization failed: {str(e)}",
            }

    def discover_log_files(self) -> List[Path]:
        """„Éó„É≠„Ç∏„Çß„ÇØ„ÉàÂÜÖ„ÅÆ.log„Éï„Ç°„Ç§„É´„ÇíÁô∫Ë¶ã"""
        log_files = []

        # .log„Éï„Ç°„Ç§„É´„ÇíÂÜçÂ∏∞ÁöÑ„Å´Ê§úÁ¥¢
        for log_file in self.project_root.rglob("*.log"):
            if log_file.is_file() and log_file.stat().st_size > 0:  # Á©∫„Éï„Ç°„Ç§„É´Èô§Â§ñ
                log_files.append(log_file)

        return sorted(log_files, key=lambda x: x.stat().st_mtime, reverse=True)

    def parse_log_entry(self, line: str, source_file: str) -> Optional[Dict[str, Any]]:
        """„É≠„Ç∞„Ç®„É≥„Éà„É™„ÇíËß£Êûê„Åó„Å¶ÊßãÈÄ†Âåñ"""
        if not line.strip():
            return None

        # PII‰øùË≠∑
        sanitized_line = self.sanitize_pii(line)

        # „Çø„Ç§„É†„Çπ„Çø„É≥„Éó„Éë„Çø„Éº„É≥Ê§úÂá∫
        timestamp_patterns = [
            (r"(\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}:\d{2})", "%Y-%m-%d %H:%M:%S"),
            (r"(\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2})", "%Y-%m-%dT%H:%M:%S"),
            (r"(\d{2}/\d{2}/\d{4}\s+\d{2}:\d{2}:\d{2})", "%m/%d/%Y %H:%M:%S"),
            (r"\[(\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}:\d{2})\]", "%Y-%m-%d %H:%M:%S"),
        ]

        parsed_timestamp = None
        for pattern, fmt in timestamp_patterns:
            match = re.search(pattern, sanitized_line)
            if match:
                try:
                    parsed_timestamp = datetime.strptime(match.group(1), fmt)
                    break
                except ValueError:
                    continue

        # „É≠„Ç∞„É¨„Éô„É´Ê§úÂá∫
        log_level = "INFO"
        level_patterns = [
            (r"\b(ERROR|error)\b", "ERROR"),
            (r"\b(WARN|WARNING|warn|warning)\b", "WARNING"),
            (r"\b(INFO|info)\b", "INFO"),
            (r"\b(DEBUG|debug)\b", "DEBUG"),
            (r"\b(CRITICAL|critical|FATAL|fatal)\b", "CRITICAL"),
        ]

        for pattern, level in level_patterns:
            if re.search(pattern, sanitized_line):
                log_level = level
                break

        # „Ç≥„É≥„Éù„Éº„Éç„É≥„ÉàÊé®ÂÆö
        component = self._extract_component_from_path(source_file)

        # ÊßãÈÄ†Âåñ„Éá„Éº„ÇøÊäΩÂá∫
        structured_data = self._extract_structured_data(sanitized_line)

        # „Éè„ÉÉ„Ç∑„É•ÁîüÊàêÔºàÈáçË§áÈô§ÂéªÁî®Ôºâ
        content_hash = hashlib.sha256(
            f"{source_file}:{sanitized_line}".encode()
        ).hexdigest()

        return {
            "log_hash": content_hash,
            "timestamp": parsed_timestamp or datetime.now(timezone.utc),
            "source_file": str(Path(source_file).relative_to(self.project_root)),
            "log_level": log_level,
            "component": component,
            "message": sanitized_line.strip(),
            "structured_data": structured_data,
            "original_format": line.strip(),
        }

    def _extract_component_from_path(self, filepath: str) -> str:
        """„Éï„Ç°„Ç§„É´„Éë„Çπ„Åã„Çâ„Ç≥„É≥„Éù„Éº„Éç„É≥„ÉàÂêç„ÇíÊé®ÂÆö"""
        path = Path(filepath)

        # „Éë„ÇπÊßãÈÄ†„Åã„Çâ„Ç≥„É≥„Éù„Éº„Éç„É≥„ÉàÊé®ÂÆö
        parts = path.parts
        if "operations" in parts:
            return "operations"
        elif "memory" in parts:
            return "memory"
        elif "agents" in parts:
            return "agents"
        elif "scripts" in parts:
            return "scripts"
        elif "runtime" in parts:
            return "runtime"
        else:
            return "system"

    def _extract_structured_data(self, line: str) -> Dict[str, Any]:
        """„É≠„Ç∞Ë°å„Åã„ÇâÊßãÈÄ†Âåñ„Éá„Éº„Çø„ÇíÊäΩÂá∫"""
        structured = {}

        # JSONÈÉ®ÂàÜ„ÅÆÊäΩÂá∫
        json_pattern = r"\{[^{}]*\}"
        json_matches = re.findall(json_pattern, line)
        if json_matches:
            for match in json_matches:
                try:
                    data = json.loads(match)
                    structured.update(data)
                except (json.JSONDecodeError, ValueError):
                    pass

        # „Ç≠„Éº=ÂÄ§„Éö„Ç¢„ÅÆÊäΩÂá∫
        kv_pattern = r"(\w+)=([^\s,}]+)"
        kv_matches = re.findall(kv_pattern, line)
        for key, value in kv_matches:
            # Êï∞ÂÄ§Â§âÊèõË©¶Ë°å
            try:
                if "." in value:
                    structured[key] = float(value)
                else:
                    structured[key] = int(value)
            except ValueError:
                structured[key] = value

        return structured

    def sanitize_pii(self, text: str) -> str:
        """PII‰øùË≠∑"""
        sanitized = text
        for pattern, replacement in self.pii_patterns:
            sanitized = re.sub(pattern, replacement, sanitized, flags=re.IGNORECASE)
        return sanitized

    def process_log_files(self, max_files: int = 50) -> Dict[str, Any]:
        """„É≠„Ç∞„Éï„Ç°„Ç§„É´Âá¶ÁêÜ"""
        log_files = self.discover_log_files()

        if len(log_files) > max_files:
            log_files = log_files[:max_files]  # ÊúÄÊñ∞„Éï„Ç°„Ç§„É´„Å´Âà∂Èôê

        processed_entries = 0
        skipped_entries = 0
        errors = []

        try:
            conn = psycopg2.connect(**self.db_config)
            cur = conn.cursor()

            for log_file in log_files:
                try:
                    with open(log_file, encoding="utf-8", errors="ignore") as f:
                        for line_num, line in enumerate(f, 1):
                            try:
                                entry = self.parse_log_entry(line, str(log_file))
                                if entry:
                                    # „Éá„Éº„Çø„Éô„Éº„Çπ„Å´ÊåøÂÖ•
                                    cur.execute(
                                        """
                                        INSERT INTO unified_logs
                                        (log_hash, timestamp, source_file, log_level, component, message, structured_data, original_format)
                                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
                                        ON CONFLICT (log_hash) DO NOTHING;
                                    """,
                                        (
                                            entry["log_hash"],
                                            entry["timestamp"],
                                            entry["source_file"],
                                            entry["log_level"],
                                            entry["component"],
                                            entry["message"],
                                            json.dumps(entry["structured_data"]),
                                            entry["original_format"],
                                        ),
                                    )

                                    processed_entries += 1
                                else:
                                    skipped_entries += 1

                            except Exception as e:
                                errors.append(f"{log_file}:{line_num}: {str(e)}")

                except Exception as e:
                    errors.append(f"File {log_file}: {str(e)}")

            conn.commit()
            cur.close()
            conn.close()

            return {
                "status": "success",
                "processed_files": len(log_files),
                "processed_entries": processed_entries,
                "skipped_entries": skipped_entries,
                "errors": errors[:10],  # ÊúÄÂàù„ÅÆ10„Ç®„É©„Éº„ÅÆ„Åø
            }

        except Exception as e:
            return {"status": "error", "error": str(e)}

    def get_unified_log_stats(self) -> Dict[str, Any]:
        """Áµ±‰∏Ä„É≠„Ç∞Áµ±Ë®à"""
        try:
            conn = psycopg2.connect(**self.db_config, cursor_factory=RealDictCursor)
            cur = conn.cursor()

            # Âü∫Êú¨Áµ±Ë®à
            cur.execute("""
                SELECT
                    COUNT(*) as total_entries,
                    COUNT(DISTINCT source_file) as unique_files,
                    COUNT(DISTINCT component) as unique_components,
                    MIN(timestamp) as earliest_entry,
                    MAX(timestamp) as latest_entry
                FROM unified_logs;
            """)

            basic_stats = cur.fetchone()

            # „É≠„Ç∞„É¨„Éô„É´Âà•Áµ±Ë®à
            cur.execute("""
                SELECT log_level, COUNT(*) as count
                FROM unified_logs
                GROUP BY log_level
                ORDER BY count DESC;
            """)

            level_stats = cur.fetchall()

            # „Ç≥„É≥„Éù„Éº„Éç„É≥„ÉàÂà•Áµ±Ë®à
            cur.execute("""
                SELECT component, COUNT(*) as count
                FROM unified_logs
                GROUP BY component
                ORDER BY count DESC;
            """)

            component_stats = cur.fetchall()

            cur.close()
            conn.close()

            return {
                "status": "success",
                "basic_stats": dict(basic_stats) if basic_stats else {},
                "level_distribution": [dict(row) for row in level_stats],
                "component_distribution": [dict(row) for row in component_stats],
            }

        except Exception as e:
            return {"status": "error", "error": str(e)}


def main():
    """„É°„Ç§„É≥ÂÆüË°å - Áµ±‰∏Ä„É≠„Ç∞„Ç∑„Çπ„ÉÜ„É†"""
    print("üìä Áµ±‰∏Ä„É≠„Ç∞„Ç∑„Çπ„ÉÜ„É† - 117„Éï„Ç°„Ç§„É´Áµ±ÂêàÈñãÂßã")

    log_system = UnifiedLogSystem()

    # 1. „Éá„Éº„Çø„Éô„Éº„ÇπÂàùÊúüÂåñ
    print("\\n1Ô∏è‚É£ Áµ±‰∏Ä„É≠„Ç∞„ÉÜ„Éº„Éñ„É´ÂàùÊúüÂåñ")
    init_result = log_system.init_unified_log_table()
    print(f"ÂàùÊúüÂåñ: {init_result['status']}")

    if init_result["status"] == "error":
        print(f"„Ç®„É©„Éº: {init_result['message']}")
        return

    # 2. „É≠„Ç∞„Éï„Ç°„Ç§„É´Áô∫Ë¶ã
    print("\\n2Ô∏è‚É£ „É≠„Ç∞„Éï„Ç°„Ç§„É´Áô∫Ë¶ã")
    log_files = log_system.discover_log_files()
    print(f"Áô∫Ë¶ã„Éï„Ç°„Ç§„É´Êï∞: {len(log_files)}")

    # ÊúÄÂàù„ÅÆ10„Éï„Ç°„Ç§„É´„ÇíË°®Á§∫
    for i, log_file in enumerate(log_files[:10]):
        size_kb = log_file.stat().st_size // 1024
        print(f"   {i + 1}. {log_file.name} ({size_kb}KB)")

    if len(log_files) > 10:
        print(f"   ... ‰ªñ{len(log_files) - 10}„Éï„Ç°„Ç§„É´")

    # 3. „É≠„Ç∞„Éï„Ç°„Ç§„É´Âá¶ÁêÜ
    print("\\n3Ô∏è‚É£ „É≠„Ç∞„Éï„Ç°„Ç§„É´Áµ±ÂêàÂá¶ÁêÜ")
    process_result = log_system.process_log_files(max_files=30)  # ÊúÄÂàù„ÅÆ30„Éï„Ç°„Ç§„É´
    print(f"Âá¶ÁêÜÁµêÊûú: {process_result['status']}")

    if process_result["status"] == "success":
        print(f"   Âá¶ÁêÜ„Éï„Ç°„Ç§„É´Êï∞: {process_result['processed_files']}")
        print(f"   Áµ±Âêà„Ç®„É≥„Éà„É™Êï∞: {process_result['processed_entries']}")
        print(f"   „Çπ„Ç≠„ÉÉ„Éó„Ç®„É≥„Éà„É™Êï∞: {process_result['skipped_entries']}")

        if process_result["errors"]:
            print(f"   „Ç®„É©„ÉºÊï∞: {len(process_result['errors'])}")
            for error in process_result["errors"][:3]:
                print(f"     - {error}")
    else:
        print(f"   „Ç®„É©„Éº: {process_result['error']}")
        return

    # 4. Áµ±Âêà„É≠„Ç∞Áµ±Ë®à
    print("\\n4Ô∏è‚É£ Áµ±Âêà„É≠„Ç∞Áµ±Ë®à")
    stats_result = log_system.get_unified_log_stats()
    print(f"Áµ±Ë®à: {stats_result['status']}")

    if stats_result["status"] == "success":
        basic = stats_result["basic_stats"]
        print(f"   Á∑è„Ç®„É≥„Éà„É™Êï∞: {basic.get('total_entries', 0)}")
        print(f"   „É¶„Éã„Éº„ÇØ„Éï„Ç°„Ç§„É´Êï∞: {basic.get('unique_files', 0)}")
        print(f"   „Ç≥„É≥„Éù„Éº„Éç„É≥„ÉàÊï∞: {basic.get('unique_components', 0)}")

        if basic.get("earliest_entry"):
            print(f"   ÊúÄÂè§„Ç®„É≥„Éà„É™: {basic['earliest_entry']}")
        if basic.get("latest_entry"):
            print(f"   ÊúÄÊñ∞„Ç®„É≥„Éà„É™: {basic['latest_entry']}")

        print("\\n   „É≠„Ç∞„É¨„Éô„É´ÂàÜÂ∏É:")
        for level in stats_result["level_distribution"][:5]:
            print(f"     {level['log_level']}: {level['count']}‰ª∂")

        print("\\n   „Ç≥„É≥„Éù„Éº„Éç„É≥„ÉàÂàÜÂ∏É:")
        for component in stats_result["component_distribution"][:5]:
            print(f"     {component['component']}: {component['count']}‰ª∂")

    print("\\n‚úÖ Áµ±‰∏Ä„É≠„Ç∞„Ç∑„Çπ„ÉÜ„É†ÂÆüË£ÖÂÆå‰∫Ü")
    print("üìç Êï£Âú®„Åô„Çã.log„Éï„Ç°„Ç§„É´„Åã„ÇâPostgreSQLÁµ±Âêà„Å∏Â§âÊèõ")


if __name__ == "__main__":
    main()
