#!/usr/bin/env python3
"""
ğŸŒŠ CSAæ–‡è„ˆã‚·ã‚¹ãƒ†ãƒ å¼·åŒ– - ãƒ‡ãƒ¼ã‚¿è“„ç©ã«ã‚ˆã‚‹åŠ¹æœå‘ä¸Š
=============================================

ã€ç›®çš„ã€‘
- çµ±ä¸€ãƒ­ã‚°ã‹ã‚‰CSAæ–‡è„ˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
- å®Ÿéš›ã®ä½œæ¥­å±¥æ­´ã‚’ã‚¤ãƒ™ãƒ³ãƒˆåŒ–
- æ–‡è„ˆæ¤œç´¢ã®åŠ¹æœå‘ä¸Š

ã€å®Ÿè£…å†…å®¹ã€‘
- çµ±ä¸€ãƒ­ã‚° â†’ CSAã‚¤ãƒ™ãƒ³ãƒˆå¤‰æ›
- å®Ÿä½œæ¥­ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å­¦ç¿’
- æ–‡è„ˆç¶™ç¶šæ€§ã®å‘ä¸Š
"""

import json
import uuid
from typing import Any, Dict, List, Optional

import openai
import psycopg2
from psycopg2.extras import RealDictCursor


class CSADataAccumulator:
    """CSAæ–‡è„ˆãƒ‡ãƒ¼ã‚¿è“„ç©ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self):
        self.db_config = {
            "host": "localhost",
            "database": "president_ai",
            "user": "dd",
            "password": "",
            "port": 5432,
        }
        self.embedding_model = "text-embedding-ada-002"
        self.session_id = str(uuid.uuid4())

    def convert_logs_to_csa_events(self, limit: int = 100) -> Dict[str, Any]:
        """çµ±ä¸€ãƒ­ã‚°ã‚’CSAã‚¤ãƒ™ãƒ³ãƒˆã«å¤‰æ›"""

        try:
            conn = psycopg2.connect(**self.db_config, cursor_factory=RealDictCursor)
            cur = conn.cursor()

            # çµ±ä¸€ãƒ­ã‚°ã‹ã‚‰æ„å‘³ã®ã‚ã‚‹ã‚¨ãƒ³ãƒˆãƒªã‚’å–å¾—
            cur.execute(
                """
                SELECT
                    timestamp,
                    source_file,
                    log_level,
                    component,
                    message,
                    structured_data
                FROM unified_logs
                WHERE log_level IN ('ERROR', 'WARNING', 'INFO')
                  AND LENGTH(message) > 20
                ORDER BY timestamp DESC
                LIMIT %s;
            """,
                (limit,),
            )

            log_entries = cur.fetchall()

            converted_events = 0
            for log_entry in log_entries:
                # ãƒ­ã‚°ã‚¨ãƒ³ãƒˆãƒªã‚’CSAã‚¤ãƒ™ãƒ³ãƒˆã«å¤‰æ›
                csa_event = self._convert_log_to_csa_event(log_entry)
                if csa_event:
                    # CSAã‚¤ãƒ™ãƒ³ãƒˆã¨ã—ã¦ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
                    success = self._save_csa_event(cur, csa_event)
                    if success:
                        converted_events += 1

            conn.commit()
            cur.close()
            conn.close()

            return {
                "status": "success",
                "processed_logs": len(log_entries),
                "converted_events": converted_events,
                "session_id": self.session_id,
            }

        except Exception as e:
            return {"status": "error", "error": str(e)}

    def _convert_log_to_csa_event(
        self, log_entry: Dict[str, Any]
    ) -> Optional[Dict[str, Any]]:
        """ãƒ­ã‚°ã‚¨ãƒ³ãƒˆãƒªã‚’CSAã‚¤ãƒ™ãƒ³ãƒˆã«å¤‰æ›"""

        message = log_entry["message"]
        component = log_entry["component"]

        # ã‚¤ãƒ™ãƒ³ãƒˆã‚¿ã‚¤ãƒ—ã®æ¨å®š
        event_type = self._infer_event_type(message, log_entry["log_level"])

        # æ„å‘³ã®ãªã„ã‚¤ãƒ™ãƒ³ãƒˆã¯ã‚¹ã‚­ãƒƒãƒ—
        if not event_type:
            return None

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ§‹ç¯‰
        metadata = {
            "original_log_level": log_entry["log_level"],
            "source_component": component,
            "source_file": log_entry["source_file"],
        }

        # structured_dataãŒã‚ã‚Œã°è¿½åŠ 
        if log_entry["structured_data"]:
            try:
                structured = (
                    json.loads(log_entry["structured_data"])
                    if isinstance(log_entry["structured_data"], str)
                    else log_entry["structured_data"]
                )
                metadata.update(structured)
            except (json.JSONDecodeError, ValueError):
                pass

        # ãƒ™ã‚¯ãƒˆãƒ«åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
        embedding = self._generate_embedding(f"{event_type}: {message}")

        return {
            "id": str(uuid.uuid4()),
            "timestamp": log_entry["timestamp"],
            "source": "system_log",
            "event_type": event_type,
            "content": message,
            "metadata": metadata,
            "session_id": self.session_id,
            "vector_embedding": embedding,
        }

    def _infer_event_type(self, message: str, log_level: str) -> Optional[str]:
        """ãƒ­ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰ã‚¤ãƒ™ãƒ³ãƒˆã‚¿ã‚¤ãƒ—ã‚’æ¨å®š"""

        message_lower = message.lower()

        # ã‚¨ãƒ©ãƒ¼é–¢é€£
        if (
            log_level == "ERROR"
            or "error" in message_lower
            or "failed" in message_lower
        ):
            if "connection" in message_lower or "database" in message_lower:
                return "database_error"
            elif "file" in message_lower:
                return "file_error"
            else:
                return "system_error"

        # è­¦å‘Šé–¢é€£
        if log_level == "WARNING" or "warning" in message_lower or "é‡è¤‡" in message:
            return "warning_alert"

        # ã‚·ã‚¹ãƒ†ãƒ æ“ä½œ
        if any(
            keyword in message_lower
            for keyword in ["è¨­å®š", "åˆæœŸåŒ–", "èµ·å‹•", "setup", "init", "start"]
        ):
            return "system_operation"

        # ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œ
        if any(
            keyword in message_lower
            for keyword in ["ãƒ•ã‚¡ã‚¤ãƒ«", "file", "ä½œæˆ", "å‰Šé™¤", "create", "delete"]
        ):
            return "file_operation"

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ“ä½œ
        if any(
            keyword in message_lower
            for keyword in ["database", "postgresql", "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹", "ãƒ†ãƒ¼ãƒ–ãƒ«"]
        ):
            return "database_operation"

        # é‡è¤‡æ¤œå‡º
        if "é‡è¤‡" in message or "duplicate" in message_lower:
            return "duplicate_detection"

        # ãƒ—ãƒ­ã‚»ã‚¹ç®¡ç†
        if any(
            keyword in message_lower
            for keyword in ["ãƒ—ãƒ­ã‚»ã‚¹", "process", "å®Ÿè¡Œ", "execute"]
        ):
            return "process_management"

        # ä¸€èˆ¬çš„ãªæƒ…å ±ãƒ­ã‚°ã¯é™¤å¤–
        if len(message) < 30:
            return None

        return "general_activity"

    def _generate_embedding(self, text: str) -> List[float]:
        """ãƒ†ã‚­ã‚¹ãƒˆã®ãƒ™ã‚¯ãƒˆãƒ«åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ"""
        try:
            client = openai.OpenAI()
            response = client.embeddings.create(model=self.embedding_model, input=text)
            return response.data[0].embedding
        except Exception as e:
            print(f"Embedding generation failed: {e}")
            return [0.0] * 1536

    def _save_csa_event(self, cursor, event: Dict[str, Any]) -> bool:
        """CSAã‚¤ãƒ™ãƒ³ãƒˆã‚’ä¿å­˜"""
        try:
            cursor.execute(
                """
                INSERT INTO context_stream
                (id, timestamp, source, event_type, content, metadata, session_id, vector_embedding)
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
                ON CONFLICT (id) DO NOTHING;
            """,
                (
                    event["id"],
                    event["timestamp"],
                    event["source"],
                    event["event_type"],
                    event["content"],
                    json.dumps(event["metadata"]),
                    event["session_id"],
                    event["vector_embedding"],
                ),
            )
            return True
        except Exception as e:
            print(f"CSA event save failed: {e}")
            return False

    def enhance_context_search(self, query: str, limit: int = 10) -> Dict[str, Any]:
        """å¼·åŒ–ã•ã‚ŒãŸæ–‡è„ˆæ¤œç´¢"""

        # ã‚¯ã‚¨ãƒªã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
        query_embedding = self._generate_embedding(query)

        try:
            conn = psycopg2.connect(**self.db_config, cursor_factory=RealDictCursor)
            cur = conn.cursor()

            # è¤‡åˆæ¤œç´¢: ãƒ™ã‚¯ãƒˆãƒ«é¡ä¼¼åº¦ + ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ + æ™‚ç³»åˆ—
            cur.execute(
                """
                SELECT
                    id,
                    timestamp,
                    source,
                    event_type,
                    content,
                    metadata,
                    1 - (vector_embedding <=> %s::vector) as similarity
                FROM context_stream
                WHERE vector_embedding IS NOT NULL
                  AND (
                    1 - (vector_embedding <=> %s::vector) > 0.6
                    OR content ILIKE %s
                    OR event_type ILIKE %s
                  )
                ORDER BY
                    (1 - (vector_embedding <=> %s::vector)) DESC,
                    timestamp DESC
                LIMIT %s;
            """,
                (
                    query_embedding,
                    query_embedding,
                    f"%{query}%",
                    f"%{query}%",
                    query_embedding,
                    limit,
                ),
            )

            results = cur.fetchall()
            cur.close()
            conn.close()

            # çµæœã®åˆ†é¡
            categorized_results = self._categorize_search_results(
                [dict(row) for row in results]
            )

            return {
                "status": "success",
                "query": query,
                "total_results": len(results),
                "categorized_results": categorized_results,
                "search_metadata": {
                    "search_type": "enhanced_semantic_temporal",
                    "embedding_model": self.embedding_model,
                },
            }

        except Exception as e:
            return {"status": "error", "error": str(e)}

    def _categorize_search_results(
        self, results: List[Dict[str, Any]]
    ) -> Dict[str, List[Dict[str, Any]]]:
        """æ¤œç´¢çµæœã‚’ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«åˆ†é¡"""
        categories = {
            "errors": [],
            "operations": [],
            "file_activities": [],
            "database_activities": [],
            "warnings": [],
            "general": [],
        }

        for result in results:
            event_type = result["event_type"]

            if "error" in event_type:
                categories["errors"].append(result)
            elif "operation" in event_type:
                categories["operations"].append(result)
            elif "file" in event_type:
                categories["file_activities"].append(result)
            elif "database" in event_type:
                categories["database_activities"].append(result)
            elif "warning" in event_type:
                categories["warnings"].append(result)
            else:
                categories["general"].append(result)

        return categories

    def get_csa_enhancement_stats(self) -> Dict[str, Any]:
        """CSAå¼·åŒ–çµ±è¨ˆ"""
        try:
            conn = psycopg2.connect(**self.db_config, cursor_factory=RealDictCursor)
            cur = conn.cursor()

            # åŸºæœ¬çµ±è¨ˆ
            cur.execute("""
                SELECT
                    COUNT(*) as total_events,
                    COUNT(DISTINCT event_type) as unique_event_types,
                    COUNT(DISTINCT source) as unique_sources,
                    COUNT(DISTINCT session_id) as unique_sessions,
                    MIN(timestamp) as earliest_event,
                    MAX(timestamp) as latest_event
                FROM context_stream;
            """)

            basic_stats = cur.fetchone()

            # ã‚¤ãƒ™ãƒ³ãƒˆã‚¿ã‚¤ãƒ—åˆ¥çµ±è¨ˆ
            cur.execute("""
                SELECT event_type, COUNT(*) as count
                FROM context_stream
                GROUP BY event_type
                ORDER BY count DESC;
            """)

            event_type_stats = cur.fetchall()

            # ã‚½ãƒ¼ã‚¹åˆ¥çµ±è¨ˆ
            cur.execute("""
                SELECT source, COUNT(*) as count
                FROM context_stream
                GROUP BY source
                ORDER BY count DESC;
            """)

            source_stats = cur.fetchall()

            cur.close()
            conn.close()

            return {
                "status": "success",
                "basic_stats": dict(basic_stats) if basic_stats else {},
                "event_type_distribution": [dict(row) for row in event_type_stats],
                "source_distribution": [dict(row) for row in source_stats],
            }

        except Exception as e:
            return {"status": "error", "error": str(e)}


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ - CSAå¼·åŒ–ã‚·ã‚¹ãƒ†ãƒ """
    print("ğŸŒŠ CSAæ–‡è„ˆã‚·ã‚¹ãƒ†ãƒ å¼·åŒ– - ãƒ‡ãƒ¼ã‚¿è“„ç©é–‹å§‹")

    accumulator = CSADataAccumulator()

    # 1. ãƒ­ã‚°ã‹ã‚‰CSAã‚¤ãƒ™ãƒ³ãƒˆå¤‰æ›
    print("\\n1ï¸âƒ£ çµ±ä¸€ãƒ­ã‚° â†’ CSAã‚¤ãƒ™ãƒ³ãƒˆå¤‰æ›")
    conversion_result = accumulator.convert_logs_to_csa_events(limit=50)
    print(f"å¤‰æ›çµæœ: {conversion_result['status']}")

    if conversion_result["status"] == "success":
        print(f"   å‡¦ç†ãƒ­ã‚°æ•°: {conversion_result['processed_logs']}")
        print(f"   å¤‰æ›ã‚¤ãƒ™ãƒ³ãƒˆæ•°: {conversion_result['converted_events']}")
        print(f"   ã‚»ãƒƒã‚·ãƒ§ãƒ³ID: {conversion_result['session_id'][:8]}...")
    else:
        print(f"   ã‚¨ãƒ©ãƒ¼: {conversion_result['error']}")
        return

    # 2. å¼·åŒ–ã•ã‚ŒãŸæ–‡è„ˆæ¤œç´¢ãƒ†ã‚¹ãƒˆ
    print("\\n2ï¸âƒ£ å¼·åŒ–ã•ã‚ŒãŸæ–‡è„ˆæ¤œç´¢ãƒ†ã‚¹ãƒˆ")
    test_queries = [
        "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚¨ãƒ©ãƒ¼",
        "ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œ",
        "é‡è¤‡æ¤œå‡º",
        "ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–",
    ]

    for query in test_queries:
        search_result = accumulator.enhance_context_search(query, limit=5)
        print(f"\\n   æ¤œç´¢: '{query}'")
        print(f"   çµæœ: {search_result.get('total_results', 0)}ä»¶")

        if search_result["status"] == "success":
            categorized = search_result["categorized_results"]
            for category, events in categorized.items():
                if events:
                    print(f"     {category}: {len(events)}ä»¶")
                    for event in events[:2]:  # æœ€åˆã®2ä»¶è¡¨ç¤º
                        print(
                            f"       - [{event['event_type']}] {event['content'][:50]}..."
                        )

    # 3. CSAå¼·åŒ–çµ±è¨ˆ
    print("\\n3ï¸âƒ£ CSAå¼·åŒ–çµ±è¨ˆ")
    stats_result = accumulator.get_csa_enhancement_stats()
    print(f"çµ±è¨ˆ: {stats_result['status']}")

    if stats_result["status"] == "success":
        basic = stats_result["basic_stats"]
        print(f"   ç·ã‚¤ãƒ™ãƒ³ãƒˆæ•°: {basic.get('total_events', 0)}")
        print(f"   ã‚¤ãƒ™ãƒ³ãƒˆã‚¿ã‚¤ãƒ—æ•°: {basic.get('unique_event_types', 0)}")
        print(f"   ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°: {basic.get('unique_sessions', 0)}")

        if basic.get("earliest_event"):
            print(f"   æœ€å¤ã‚¤ãƒ™ãƒ³ãƒˆ: {basic['earliest_event']}")
        if basic.get("latest_event"):
            print(f"   æœ€æ–°ã‚¤ãƒ™ãƒ³ãƒˆ: {basic['latest_event']}")

        print("\\n   ã‚¤ãƒ™ãƒ³ãƒˆã‚¿ã‚¤ãƒ—åˆ†å¸ƒ:")
        for event_type in stats_result["event_type_distribution"][:5]:
            print(f"     {event_type['event_type']}: {event_type['count']}ä»¶")

        print("\\n   ã‚½ãƒ¼ã‚¹åˆ†å¸ƒ:")
        for source in stats_result["source_distribution"]:
            print(f"     {source['source']}: {source['count']}ä»¶")

    print("\\nâœ… CSAæ–‡è„ˆã‚·ã‚¹ãƒ†ãƒ å¼·åŒ–å®Œäº†")
    print("ğŸ“ çµ±ä¸€ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã‚‹æ–‡è„ˆæ¤œç´¢åŠ¹æœå‘ä¸Š")


if __name__ == "__main__":
    main()
