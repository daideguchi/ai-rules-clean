#!/usr/bin/env python3
"""
ğŸ“ ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ  - å®¹é‡ãƒ™ãƒ¼ã‚¹è‡ªå‹•å‰Šé™¤
================================================

ã€ç›®çš„ã€‘
- ãƒ­ãƒ¼ã‚«ãƒ«.logãƒ•ã‚¡ã‚¤ãƒ«ã®å®¹é‡ç›£è¦–
- ä¸€å®šå®¹é‡è¶…éæ™‚ã®è‡ªå‹•å‰Šé™¤
- é‡è¦ãƒ•ã‚¡ã‚¤ãƒ«ä¿è­·
- DBã¸ã®äº‹å‰ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—

ã€å®Ÿè£…å†…å®¹ã€‘
- å®¹é‡ãƒ™ãƒ¼ã‚¹ç›£è¦–
- å¤ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å„ªå…ˆå‰Šé™¤
- æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«ä¿è­·
- å‰Šé™¤å‰DBä¿å­˜ç¢ºèª
"""

from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, List, Optional

import psycopg2


class LocalFileManager:
    """o3æ¨å¥¨ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ç®¡ç† - å®‰å…¨æ€§å„ªå…ˆäºŒæ®µéšå‰Šé™¤ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self):
        self.project_root = Path(__file__).parent.parent
        self.db_config = {
            "host": "localhost",
            "database": "president_ai",
            "user": "dd",
            "password": "",
            "port": 5432,
        }

        # o3æ¨å¥¨ç¾å®Ÿçš„å®¹é‡è¨­å®š
        self.max_total_size_mb = 8192  # ãƒãƒ¼ãƒ‰ä¸Šé™ 8GB (o3æ¨å¥¨é–‹ç™ºç’°å¢ƒæ¨™æº–)
        self.warning_threshold_mb = 6553  # è­¦å‘Šé–¾å€¤ 6.4GB (80%)
        self.target_cleanup_mb = 5242  # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ç›®æ¨™ 5.1GB (64%)

        # o3æ¨å¥¨éšå±¤åŒ–ä¿è­·æœŸé–“
        self.hot_retention_days = 14  # ãƒ›ãƒƒãƒˆå±¤ï¼š14æ—¥é–“é«˜é€Ÿã‚¢ã‚¯ã‚»ã‚¹
        self.warm_retention_days = 365  # ã‚¦ã‚©ãƒ¼ãƒ å±¤ï¼š1å¹´é–“ä¿æŒ
        self.critical_preserve_days = 730  # ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ï¼š2å¹´é–“æ°¸ç¶šä¿è­·

        # o3æ¨å¥¨å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ä¿è­·è¨­å®š
        self.learning_data_protection = True  # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¯æ°¸ç¶šä¿è­·
        self.documentation_protection = True  # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯æ°¸ç¶šä¿è­·

        # å®‰å…¨æ€§è¨­å®š
        self.trash_directory = self.project_root / ".trash"
        self.verification_delay_seconds = 2  # å‰Šé™¤å‰æ¤œè¨¼å¾…æ©Ÿ
        self.max_batch_delete = 20  # ä¸€å›ã®æœ€å¤§å‰Šé™¤æ•°

        # o3æ¨å¥¨éšå±¤åŒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªç›£è¦–
        self.hot_tier_paths = [
            self.project_root / "operations" / "runtime-logs",
            self.project_root / "runtime",
            self.project_root / "logs",
            self.project_root / "tmp",
        ]

        self.warm_tier_path = self.project_root / "data" / "warm"
        self.learning_data_paths = [
            self.project_root / "docs",
            self.project_root / "ai-instructions",
            self.project_root / "memory",
        ]

        # o3æ¨å¥¨ä¿è­·å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³
        self.learning_protected_patterns = [
            "mistake*",
            "president*",
            "*learning*",
            "*report*",
            "*analysis*",
        ]

        self.documentation_protected_patterns = [
            "README*",
            "*.md",
            "docs/*",
            "ai-instructions/*",
            "*manual*",
            "*guide*",
        ]

        self.critical_system_patterns = [
            "*error*",
            "*critical*",
            ".git*",
            "config/*",
            "*.py",
            "*.json",
        ]

    def calculate_tiered_storage_stats(self) -> Dict[str, Any]:
        """o3æ¨å¥¨éšå±¤åŒ–ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸çµ±è¨ˆè¨ˆç®—"""

        hot_size = 0
        warm_size = 0
        protected_size = 0
        total_file_count = 0
        tier_stats = {
            "hot_tier": {},
            "warm_tier": {},
            "learning_data": {},
            "total_summary": {},
        }

        # ãƒ›ãƒƒãƒˆå±¤çµ±è¨ˆ
        for monitor_path in self.hot_tier_paths:
            if not monitor_path.exists():
                continue

            dir_size = 0
            dir_files = 0

            for file_path in monitor_path.rglob("*"):
                if file_path.is_file():
                    try:
                        file_size = file_path.stat().st_size
                        hot_size += file_size
                        dir_size += file_size
                        total_file_count += 1
                        dir_files += 1
                    except (OSError, PermissionError):
                        continue

            tier_stats["hot_tier"][str(monitor_path.relative_to(self.project_root))] = {
                "size_mb": round(dir_size / (1024 * 1024), 2),
                "file_count": dir_files,
            }

        # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿å±¤çµ±è¨ˆ
        for learning_path in self.learning_data_paths:
            if not learning_path.exists():
                continue

            dir_size = 0
            dir_files = 0

            for file_path in learning_path.rglob("*"):
                if file_path.is_file():
                    try:
                        file_size = file_path.stat().st_size
                        protected_size += file_size
                        dir_size += file_size
                        total_file_count += 1
                        dir_files += 1
                    except (OSError, PermissionError):
                        continue

            tier_stats["learning_data"][
                str(learning_path.relative_to(self.project_root))
            ] = {"size_mb": round(dir_size / (1024 * 1024), 2), "file_count": dir_files}

        # ã‚¦ã‚©ãƒ¼ãƒ å±¤çµ±è¨ˆ
        if self.warm_tier_path.exists():
            for file_path in self.warm_tier_path.rglob("*"):
                if file_path.is_file():
                    try:
                        warm_size += file_path.stat().st_size
                    except (OSError, PermissionError):
                        continue

        total_size = hot_size + warm_size + protected_size
        total_size_mb = total_size / (1024 * 1024)
        hot_size_mb = hot_size / (1024 * 1024)
        warm_size_mb = warm_size / (1024 * 1024)
        protected_size_mb = protected_size / (1024 * 1024)

        tier_stats["total_summary"] = {
            "total_size_mb": round(total_size_mb, 2),
            "hot_tier_mb": round(hot_size_mb, 2),
            "warm_tier_mb": round(warm_size_mb, 2),
            "protected_data_mb": round(protected_size_mb, 2),
            "total_files": total_file_count,
            "capacity_analysis": {
                "max_limit_mb": self.max_total_size_mb,
                "warning_threshold_mb": self.warning_threshold_mb,
                "current_usage_percent": round(
                    (hot_size_mb / self.max_total_size_mb) * 100, 1
                ),
                "needs_cleanup": hot_size_mb > self.warning_threshold_mb,
                "tier_distribution": {
                    "hot_percent": round((hot_size_mb / total_size_mb) * 100, 1)
                    if total_size_mb > 0
                    else 0,
                    "warm_percent": round((warm_size_mb / total_size_mb) * 100, 1)
                    if total_size_mb > 0
                    else 0,
                    "protected_percent": round(
                        (protected_size_mb / total_size_mb) * 100, 1
                    )
                    if total_size_mb > 0
                    else 0,
                },
            },
        }

        return tier_stats

    def identify_tiered_cleanup_candidates(self) -> List[Dict[str, Any]]:
        """DISABLED: Memory inheritance system never deletes memories"""
        return []  # Always return empty - no cleanup candidates

        candidates = []
        now = datetime.now()
        preserve_cutoff = now - timedelta(days=self.preserve_recent_days)
        critical_preserve_cutoff = now - timedelta(days=self.critical_preserve_days)

        for monitor_path in self.monitored_paths:
            if not monitor_path.exists():
                continue

            for file_path in monitor_path.rglob("*"):
                if not file_path.is_file():
                    continue

                try:
                    stat = file_path.stat()
                    modified_time = datetime.fromtimestamp(stat.st_mtime)
                    file_size = stat.st_size

                    # ä¿è­·ãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚§ãƒƒã‚¯
                    is_protected = self._is_protected_file(file_path)
                    is_recent = modified_time > preserve_cutoff
                    is_critical_recent = modified_time > critical_preserve_cutoff

                    # å‰Šé™¤å€™è£œæ¡ä»¶
                    if not is_recent and not (is_protected and is_critical_recent):
                        candidates.append(
                            {
                                "path": file_path,
                                "relative_path": str(
                                    file_path.relative_to(self.project_root)
                                ),
                                "size_mb": round(file_size / (1024 * 1024), 3),
                                "modified_time": modified_time,
                                "age_days": (now - modified_time).days,
                                "is_protected": is_protected,
                                "file_type": self._classify_file_type(file_path),
                                "deletion_priority": self._calculate_deletion_priority(
                                    file_path, modified_time, file_size, is_protected
                                ),
                            }
                        )

                except (OSError, PermissionError):
                    continue

        # å‰Šé™¤å„ªå…ˆåº¦é †ã«ã‚½ãƒ¼ãƒˆ
        candidates.sort(key=lambda x: x["deletion_priority"], reverse=True)

        return candidates

    def _is_protected_file(self, file_path: Path) -> bool:
        """ä¿è­·ãƒ•ã‚¡ã‚¤ãƒ«åˆ¤å®š"""

        file_name_lower = file_path.name.lower()

        # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
        for pattern in self.protected_patterns:
            if pattern.replace("*", "") in file_name_lower:
                return True

        # æ‹¡å¼µå­ãƒã‚§ãƒƒã‚¯
        if file_path.suffix in [".md", ".json", ".py"]:
            return True

        # ãƒ‘ã‚¹å†…ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        path_str = str(file_path).lower()
        if any(
            keyword in path_str
            for keyword in ["important", "critical", "backup", "recovery"]
        ):
            return True

        return False

    def _classify_file_type(self, file_path: Path) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—åˆ†é¡"""

        suffix = file_path.suffix.lower()
        name_lower = file_path.name.lower()

        if suffix == ".log":
            if "error" in name_lower:
                return "error_log"
            elif "debug" in name_lower:
                return "debug_log"
            else:
                return "system_log"
        elif suffix == ".tmp":
            return "temporary"
        elif suffix in [".bak", ".backup"]:
            return "backup"
        elif suffix == ".json":
            return "data_file"
        elif suffix == ".md":
            return "documentation"
        else:
            return "other"

    def _calculate_deletion_priority(
        self,
        file_path: Path,
        modified_time: datetime,
        file_size: int,
        is_protected: bool,
    ) -> float:
        """o3æ¨å¥¨è¤‡åˆã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°å‰Šé™¤å„ªå…ˆåº¦è¨ˆç®— (é«˜ã„ã»ã©å‰Šé™¤å„ªå…ˆ)"""

        now = datetime.now()
        age_days = (now - modified_time).days
        size_mb = file_size / (1024 * 1024)

        # o3æ¨å¥¨åŸºæœ¬ã‚¹ã‚³ã‚¢è¨ˆç®—
        age_score = min(age_days / 30.0, 10.0)  # å¹´é½¢ã‚¹ã‚³ã‚¢ (æœ€å¤§10ç‚¹)
        size_score = min(size_mb / 5.0, 5.0)  # ã‚µã‚¤ã‚ºã‚¹ã‚³ã‚¢ (æœ€å¤§5ç‚¹)

        # o3æ¨å¥¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—é‡ã¿
        file_type = self._classify_file_type(file_path)
        type_weight = {
            "temporary": 3.0,  # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã¯æœ€å„ªå…ˆå‰Šé™¤
            "debug_log": 2.5,  # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ã¯å‰Šé™¤ã—ã‚„ã™ã„
            "system_log": 2.0,  # ã‚·ã‚¹ãƒ†ãƒ ãƒ­ã‚°ã¯ä¸­ç¨‹åº¦
            "backup": 1.5,  # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã¯é‡è¦åº¦ã«ã‚ˆã‚‹
            "error_log": 0.5,  # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã¯é‡è¦
            "documentation": 0.3,  # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ä¿æŒå„ªå…ˆ
            "data_file": 0.2,  # ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã¯æœ€é‡è¦
            "other": 1.0,
        }.get(file_type, 1.0)

        # o3æ¨å¥¨é‡è¦åº¦èª¿æ•´
        importance_penalty = 0.1 if is_protected else 1.0

        # o3æ¨å¥¨ã‚¢ã‚¯ã‚»ã‚¹é »åº¦æ¨å®š (ãƒ•ã‚¡ã‚¤ãƒ«åãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ™ãƒ¼ã‚¹)
        access_freq_boost = 1.0
        name_lower = file_path.name.lower()
        if any(pattern in name_lower for pattern in ["temp", "tmp", "cache", ".log"]):
            access_freq_boost = 1.5  # ã‚¢ã‚¯ã‚»ã‚¹é »åº¦ä½ã„ãƒ•ã‚¡ã‚¤ãƒ«ã¯å‰Šé™¤ã—ã‚„ã™ã„
        elif any(
            pattern in name_lower for pattern in ["config", "setting", "important"]
        ):
            access_freq_boost = 0.5  # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã¯å‰Šé™¤ã—ã«ãã„

        # o3è¤‡åˆã‚¹ã‚³ã‚¢è¨ˆç®—å¼
        composite_score = (
            (age_score * 0.4 + size_score * 0.3)
            * type_weight
            * importance_penalty
            * access_freq_boost
        )

        return composite_score

    def execute_cleanup_o3_safe(
        self, target_size_mb: Optional[float] = None, dry_run: bool = True
    ) -> Dict[str, Any]:
        """DISABLED: Memory inheritance system never executes cleanup"""
        return {
            "status": "disabled",
            "message": "Memory inheritance system preserves all memories",
            "cleaned_files": 0,
            "bytes_freed": 0,
        }

        if target_size_mb is None:
            target_size_mb = self.target_cleanup_mb  # o3æ¨å¥¨ç›®æ¨™å€¤

        current_stats = self.calculate_total_size()
        current_size_mb = current_stats["total_size_mb"]

        # o3æ¨å¥¨é–¾å€¤ãƒã‚§ãƒƒã‚¯
        if current_size_mb <= self.warning_threshold_mb:
            return {
                "status": "no_cleanup_needed",
                "current_size_mb": current_size_mb,
                "warning_threshold_mb": self.warning_threshold_mb,
                "message": "ç¾åœ¨ã®å®¹é‡ã¯è­¦å‘Šé–¾å€¤ä»¥ä¸‹ã§ã™",
            }

        # o3æ¨å¥¨Trashãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæº–å‚™
        if not dry_run:
            self.trash_directory.mkdir(exist_ok=True)

        candidates = self.identify_cleanup_candidates()
        processed_files = []
        moved_files = []
        deleted_files = []
        deleted_size_mb = 0.0
        errors = []
        rollback_operations = []  # o3æ¨å¥¨ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨

        # o3æ¨å¥¨äºŒæ®µéšå‰Šé™¤é‡è¨ˆç®—
        size_to_delete_mb = current_size_mb - target_size_mb

        try:
            # Phase 1: o3æ¨å¥¨Move â†’ Verify ãƒ‘ã‚¿ãƒ¼ãƒ³
            batch_count = 0
            for candidate in candidates:
                if (
                    deleted_size_mb >= size_to_delete_mb
                    or batch_count >= self.max_batch_delete
                ):
                    break

                file_path = candidate["path"]

                try:
                    # o3æ¨å¥¨DBäº‹å‰ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ— (é‡è¦ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆ)
                    if candidate["is_protected"]:
                        backup_success = self._backup_to_database_o3(file_path)
                        if not backup_success:
                            errors.append(
                                f"DBä¿å­˜å¤±æ•—ã®ãŸã‚å‰Šé™¤ã‚¹ã‚­ãƒƒãƒ—: {candidate['relative_path']}"
                            )
                            continue

                        # o3æ¨å¥¨DBä¿å­˜ãƒ­ã‚°è¨˜éŒ²
                        self._log_deletion_to_database(candidate, "db_backup_completed")

                    if not dry_run:
                        # o3æ¨å¥¨å®‰å…¨ãªatomic renameæ“ä½œ
                        trash_path = (
                            self.trash_directory
                            / f"{file_path.name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                        )
                        file_path.rename(trash_path)  # atomic operation
                        rollback_operations.append((trash_path, file_path))

                        moved_files.append(
                            {
                                "original_path": candidate["relative_path"],
                                "trash_path": str(
                                    trash_path.relative_to(self.project_root)
                                ),
                                "size_mb": candidate["size_mb"],
                            }
                        )

                    processed_files.append(
                        {
                            "path": candidate["relative_path"],
                            "size_mb": candidate["size_mb"],
                            "age_days": candidate["age_days"],
                            "file_type": candidate["file_type"],
                            "composite_score": candidate["deletion_priority"],
                        }
                    )
                    deleted_size_mb += candidate["size_mb"]
                    batch_count += 1

                except (OSError, PermissionError) as e:
                    errors.append(f"ç§»å‹•å¤±æ•—: {candidate['relative_path']}: {str(e)}")
                    continue

            # Phase 2: o3æ¨å¥¨Verification â†’ Delete ãƒ‘ã‚¿ãƒ¼ãƒ³
            if not dry_run and moved_files:
                import time

                time.sleep(self.verification_delay_seconds)  # o3æ¨å¥¨æ¤œè¨¼å¾…æ©Ÿ

                # o3æ¨å¥¨æœ€çµ‚æ¤œè¨¼å¾Œã®å®Ÿå‰Šé™¤
                for moved_file in moved_files:
                    trash_path = self.project_root / moved_file["trash_path"]
                    if trash_path.exists():
                        trash_path.unlink()  # å®Ÿéš›ã®å‰Šé™¤
                        deleted_files.append(moved_file)
                        # o3æ¨å¥¨å‰Šé™¤ãƒ­ã‚°è¨˜éŒ²
                        self._log_deletion_to_database(moved_file, "file_deleted")

                rollback_operations.clear()  # å‰Šé™¤å®Œäº†ã§ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ä¸è¦

        except Exception as e:
            # o3æ¨å¥¨ã‚¨ãƒ©ãƒ¼æ™‚è‡ªå‹•ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯
            if not dry_run and rollback_operations:
                for trash_path, original_path in rollback_operations:
                    try:
                        if trash_path.exists():
                            trash_path.rename(original_path)
                    except Exception:
                        pass
            errors.append(f"ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {str(e)}")

        final_size_mb = current_size_mb - deleted_size_mb

        return {
            "status": "cleanup_completed" if not dry_run else "dry_run_completed",
            "initial_size_mb": current_size_mb,
            "target_size_mb": target_size_mb,
            "final_size_mb": final_size_mb,
            "deleted_files_count": len(deleted_files),
            "deleted_size_mb": round(deleted_size_mb, 2),
            "space_freed_percent": round((deleted_size_mb / current_size_mb) * 100, 1),
            "deleted_files": deleted_files,
            "errors": errors,
            "dry_run": dry_run,
        }

    def _backup_to_database_o3(self, file_path: Path) -> bool:
        """o3æ¨å¥¨é‡è¦ãƒ•ã‚¡ã‚¤ãƒ«ã®å®‰å…¨ãªDBä¿å­˜"""
        try:
            with open(file_path, encoding="utf-8", errors="ignore") as f:
                content = f.read()

            conn = psycopg2.connect(**self.db_config)
            cur = conn.cursor()

            # o3æ¨å¥¨æ‹¡å¼µãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ†ãƒ¼ãƒ–ãƒ«
            cur.execute("""
                CREATE TABLE IF NOT EXISTS file_backups_o3 (
                    id SERIAL PRIMARY KEY,
                    file_path TEXT UNIQUE,
                    file_content TEXT,
                    file_size INTEGER,
                    original_modified TIMESTAMPTZ,
                    backed_up_at TIMESTAMPTZ DEFAULT NOW(),
                    file_type VARCHAR(50),
                    deletion_reason VARCHAR(100),
                    composite_score FLOAT,
                    is_protected BOOLEAN DEFAULT FALSE,
                    backup_hash VARCHAR(64)
                );
            """)

            stat = file_path.stat()
            modified_time = datetime.fromtimestamp(stat.st_mtime)

            # o3æ¨å¥¨ãƒãƒƒã‚·ãƒ¥ä»˜ããƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
            import hashlib

            content_hash = hashlib.sha256(content.encode("utf-8")).hexdigest()

            cur.execute(
                """
                INSERT INTO file_backups_o3
                (file_path, file_content, file_size, original_modified, file_type,
                 deletion_reason, is_protected, backup_hash)
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
                ON CONFLICT (file_path) DO UPDATE SET
                    file_content = EXCLUDED.file_content,
                    file_size = EXCLUDED.file_size,
                    backed_up_at = NOW(),
                    backup_hash = EXCLUDED.backup_hash;
            """,
                (
                    str(file_path.relative_to(self.project_root)),
                    content,
                    stat.st_size,
                    modified_time,
                    self._classify_file_type(file_path),
                    "capacity_cleanup",
                    self._is_protected_file(file_path),
                    content_hash,
                ),
            )

            conn.commit()
            cur.close()
            conn.close()

            return True

        except Exception as e:
            print(f"DB backup failed for {file_path}: {e}")
            return False

    def setup_automatic_monitoring(self) -> Dict[str, Any]:
        """è‡ªå‹•ç›£è¦–è¨­å®š"""

        monitor_script = (
            self.project_root / "scripts" / "utilities" / "auto_file_cleanup.py"
        )

        script_content = '''#!/usr/bin/env python3
"""è‡ªå‹•ãƒ•ã‚¡ã‚¤ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

from memory.local_file_manager import LocalFileManager

def main():
    manager = LocalFileManager()

    # å®¹é‡ãƒã‚§ãƒƒã‚¯
    stats = manager.calculate_total_size()
    print(f"ç¾åœ¨ã®å®¹é‡: {stats['total_size_mb']}MB")

    if stats['threshold_status']['needs_cleanup']:
        print("ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãŒå¿…è¦ã§ã™ã€‚å®Ÿè¡Œä¸­...")
        result = manager.execute_cleanup(dry_run=False)
        print(f"ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†: {result['deleted_files_count']}ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ ({result['deleted_size_mb']}MB)")
    else:
        print("ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã¯ä¸è¦ã§ã™ã€‚")

if __name__ == "__main__":
    main()
'''

        monitor_script.parent.mkdir(parents=True, exist_ok=True)
        with open(monitor_script, "w", encoding="utf-8") as f:
            f.write(script_content)

        monitor_script.chmod(0o755)

        return {
            "status": "success",
            "monitor_script": str(monitor_script),
            "cron_command": f"0 */6 * * * {monitor_script}",  # 6æ™‚é–“ãŠã
            "setup_instructions": [
                f"chmod +x {monitor_script}",
                f"crontab -e ã§è¿½åŠ : 0 */6 * * * {monitor_script}",
            ],
        }


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ - ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ç®¡ç†"""
    print("ğŸ“ ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹")

    manager = LocalFileManager()

    # 1. ç¾åœ¨ã®å®¹é‡çŠ¶æ³ç¢ºèª
    print("\\n1ï¸âƒ£ ç¾åœ¨ã®å®¹é‡çŠ¶æ³")
    stats = manager.calculate_total_size()
    print(f"ç·å®¹é‡: {stats['total_size_mb']}MB")
    print(f"ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {stats['total_files']}")
    print(f"ä½¿ç”¨ç‡: {stats['threshold_status']['current_usage_percent']}%")
    print(
        f"ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¿…è¦: {'âœ… Yes' if stats['threshold_status']['needs_cleanup'] else 'âŒ No'}"
    )

    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåˆ¥å®¹é‡
    print("\\n   ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåˆ¥å®¹é‡:")
    for dir_name, dir_stats in stats["directory_breakdown"].items():
        print(
            f"     {dir_name}: {dir_stats['size_mb']}MB ({dir_stats['file_count']}ãƒ•ã‚¡ã‚¤ãƒ«)"
        )

    # 2. å‰Šé™¤å€™è£œç¢ºèª
    print("\\n2ï¸âƒ£ å‰Šé™¤å€™è£œãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª")
    candidates = manager.identify_cleanup_candidates()
    print(f"å‰Šé™¤å€™è£œæ•°: {len(candidates)}")

    if candidates:
        print("   ä¸Šä½å‰Šé™¤å€™è£œ:")
        for i, candidate in enumerate(candidates[:10]):
            print(
                f"     {i + 1}. {candidate['relative_path']} ({candidate['size_mb']}MB, {candidate['age_days']}æ—¥å‰)"
            )

    # 3. ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    if stats["threshold_status"]["needs_cleanup"]:
        print("\\n3ï¸âƒ£ ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—")
        dry_result = manager.execute_cleanup(dry_run=True)
        print(f"ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³çµæœ: {dry_result['status']}")
        print(f"   å‰Šé™¤äºˆå®šãƒ•ã‚¡ã‚¤ãƒ«æ•°: {dry_result['deleted_files_count']}")
        print(f"   å‰Šé™¤äºˆå®šå®¹é‡: {dry_result['deleted_size_mb']}MB")
        print(f"   å®¹é‡å‰Šæ¸›ç‡: {dry_result['space_freed_percent']}%")

        # å®Ÿéš›ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Ÿè¡Œç¢ºèª
        print("\\n4ï¸âƒ£ å®Ÿéš›ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Ÿè¡Œ")
        actual_result = manager.execute_cleanup(dry_run=False)
        print(f"å®Ÿè¡Œçµæœ: {actual_result['status']}")
        print(f"   å‰Šé™¤ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {actual_result['deleted_files_count']}")
        print(f"   å‰Šé™¤å®¹é‡: {actual_result['deleted_size_mb']}MB")
        print(f"   æœ€çµ‚å®¹é‡: {actual_result['final_size_mb']}MB")

        if actual_result["errors"]:
            print(f"   ã‚¨ãƒ©ãƒ¼æ•°: {len(actual_result['errors'])}")

    # 5. è‡ªå‹•ç›£è¦–è¨­å®š
    print("\\n5ï¸âƒ£ è‡ªå‹•ç›£è¦–è¨­å®š")
    monitor_setup = manager.setup_automatic_monitoring()
    print(f"ç›£è¦–è¨­å®š: {monitor_setup['status']}")
    print(f"   ç›£è¦–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ: {monitor_setup['monitor_script']}")
    print(f"   Cronè¨­å®š: {monitor_setup['cron_command']}")

    print("\\nâœ… ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…å®Œäº†")
    print("ğŸ“ å®¹é‡ãƒ™ãƒ¼ã‚¹è‡ªå‹•å‰Šé™¤ + é‡è¦ãƒ•ã‚¡ã‚¤ãƒ«ä¿è­· + DBäº‹å‰ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—")


if __name__ == "__main__":
    main()
