#!/usr/bin/env python3
"""
ğŸŒŠ CSAæ–‡è„ˆã‚·ã‚¹ãƒ†ãƒ å®Œå…¨å®Ÿè£… - o3çµ±åˆãƒ‡ãƒ¼ã‚¿è“„ç©åŠ é€Ÿç‰ˆ
=====================================================

ã€o3çµ±åˆæ”¹å–„ã€‘
- ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåˆ¥è¨­å®šå¯¾å¿œ
- 8GBå®¹é‡è¨­è¨ˆçµ±åˆ
- éšå±¤åŒ–ãƒ‡ãƒ¼ã‚¿ç®¡ç†
- å­¦ç¿’ãƒ‡ãƒ¼ã‚¿æ°¸ç¶šä¿è­·

ã€å®Ÿè£…å†…å®¹ã€‘
- å…¨117+ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å¤§é‡CSAãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
- å®Ÿéš›ã®é–‹ç™ºãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’å¼·åŒ–
- o3æ¨å¥¨ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³è¨­è¨ˆé©ç”¨
- ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåˆ¥DBè‡ªå‹•è¨­å®š
- æ–‡è„ˆæ¤œç´¢åŠ¹æœã®åŠ‡çš„å‘ä¸Š
"""

import json
import re
import uuid
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import Any, Dict, List, Optional

import openai
import psycopg2
from psycopg2.extras import RealDictCursor


class CSACompleteSystem:
    """CSAæ–‡è„ˆã‚·ã‚¹ãƒ†ãƒ å®Œå…¨ç‰ˆ - o3çµ±åˆè¨­è¨ˆ"""

    def __init__(
        self, project_root: Optional[Path] = None, config_file: Optional[str] = None
    ):
        """åˆæœŸåŒ– - ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåˆ¥è¨­å®šå¯¾å¿œ"""

        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆè‡ªå‹•æ¤œå‡º
        if project_root:
            self.project_root = project_root
        else:
            self.project_root = Path(__file__).parent.parent

        # o3æ¨å¥¨ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåˆ¥è¨­å®šèª­ã¿è¾¼ã¿
        self.config = self._load_project_config(config_file)

        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåˆ¥DBè¨­å®š
        self.db_config = self.config.get(
            "database",
            {
                "host": "localhost",
                "database": f"{self.project_root.name}_ai",  # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåãƒ™ãƒ¼ã‚¹
                "user": "dd",
                "password": "",
                "port": 5432,
            },
        )

        self.embedding_model = "text-embedding-ada-002"
        self.session_id = str(uuid.uuid4())

        # o3æ¨å¥¨çµ±åˆãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ç®¡ç†è¨­å®š
        capacity_config = self.config.get("capacity", {})
        self.max_local_storage_mb = capacity_config.get(
            "max_size_mb", 8192
        )  # o3æ¨å¥¨8GB
        self.cleanup_threshold_mb = capacity_config.get(
            "warning_mb", int(self.max_local_storage_mb * 0.8)
        )
        self.preserve_recent_days = capacity_config.get("hot_days", 14)  # o3æ¨å¥¨14æ—¥

        # o3æ¨å¥¨éšå±¤åŒ–ãƒ‡ãƒ¼ã‚¿ä¿æŒè¨­å®š
        # DISABLED: Memory inheritance system never expires data
        # retention_config = self.config.get("retention", {})
        self.db_retention_days = -1  # DISABLED: Permanent retention
        self.hot_data_days = -1  # DISABLED: Permanent retention
        self.critical_preserve_days = -1  # DISABLED: Permanent retention

        # o3æ¨å¥¨å­¦ç¿’ãƒ‡ãƒ¼ã‚¿æ°¸ç¶šä¿è­·
        protection_config = self.config.get("protection", {})
        self.learning_data_protection = protection_config.get("learning_data", True)
        self.documentation_protection = protection_config.get("documentation", True)

        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåˆ¥ãƒ‘ã‚¹è¨­å®š
        paths_config = self.config.get("paths", {})
        self.monitored_paths = self._get_monitored_paths(paths_config)

        # UXè¨­å®š
        ux_config = self.config.get("ux", {})
        self.verbose_logging = ux_config.get("verbose_logging", True)

    def _load_project_config(self, config_file: Optional[str] = None) -> Dict[str, Any]:
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨­å®šèª­ã¿è¾¼ã¿ï¼ˆo3çµ±åˆè¨­è¨ˆï¼‰"""

        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«å€™è£œ
        config_candidates = []

        if config_file:
            config_candidates.append(Path(config_file))

        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†…è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«å€™è£œ
        config_candidates.extend(
            [
                self.project_root / "memory_config.json",
                self.project_root / "config" / "memory.json",
                self.project_root / ".memory_config.json",
            ]
        )

        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        for config_path in config_candidates:
            if config_path.exists():
                try:
                    with open(config_path, encoding="utf-8") as f:
                        return json.load(f)
                except Exception:
                    continue

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
        return {
            "database": {
                "host": "localhost",
                "database": f"{self.project_root.name}_ai",
                "user": "dd",
                "password": "",
                "port": 5432,
            },
            "capacity": {"max_size_mb": 8192, "warning_mb": 6553, "hot_days": 14},
            "retention": {"hot_days": 14, "warm_days": 365, "critical_days": 730},
            "protection": {"learning_data": True, "documentation": True},
        }

    def _get_monitored_paths(self, paths_config: Dict[str, Any]) -> List[Path]:
        """ç›£è¦–å¯¾è±¡ãƒ‘ã‚¹å–å¾—"""

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ + ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨­å®š
        default_paths = ["logs", "tmp", "runtime", "operations/runtime-logs"]
        hot_paths = paths_config.get("hot_tier", default_paths)

        # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹ï¼ˆæ°¸ç¶šä¿è­·ï¼‰
        default_learning = ["docs", "ai-instructions", "memory"]
        learning_paths = paths_config.get("learning_data", default_learning)

        all_paths = hot_paths + learning_paths
        return [self.project_root / path for path in all_paths]

    def implement_o3_data_partitioning(self):
        """o3æ¨å¥¨ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³è¨­è¨ˆ + ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåˆ¥å®Ÿè£…"""
        try:
            conn = psycopg2.connect(**self.db_config)
            cur = conn.cursor()

            # æ—¢å­˜ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³å¯¾å¿œã«å¤‰æ›´
            cur.execute("""
                -- o3æ¨å¥¨ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³å¯¾å¿œãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåˆ¥ï¼‰
                CREATE TABLE IF NOT EXISTS context_stream_partitioned (
                    id UUID PRIMARY KEY,
                    timestamp TIMESTAMPTZ NOT NULL,
                    source VARCHAR(50) NOT NULL,
                    event_type VARCHAR(100) NOT NULL,
                    content TEXT NOT NULL,
                    metadata JSONB,
                    session_id UUID NOT NULL,
                    vector_embedding vector(1536),
                    parent_event_id UUID,
                    importance_level VARCHAR(20) DEFAULT 'normal',
                    project_name VARCHAR(100) DEFAULT '{self.project_root.name}',
                    file_tier VARCHAR(20) DEFAULT 'hot',  -- hot/warm/protected
                    retention_category VARCHAR(30) DEFAULT 'standard',
                    created_at TIMESTAMPTZ DEFAULT NOW()
                ) PARTITION BY RANGE (timestamp);
            """)

            # ç¾åœ¨æœˆã®ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ä½œæˆ
            current_month = datetime.now().strftime("%Y%m")
            cur.execute(f"""
                CREATE TABLE IF NOT EXISTS context_stream_{current_month}
                PARTITION OF context_stream_partitioned
                FOR VALUES FROM ('{datetime.now().strftime("%Y-%m-01")}')
                TO ('{(datetime.now().replace(day=1) + timedelta(days=32)).strftime("%Y-%m-01")}');
            """)

            # o3æ¨å¥¨è¤‡åˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
            cur.execute(f"""
                CREATE INDEX IF NOT EXISTS idx_context_{current_month}_project_importance
                ON context_stream_{current_month} (project_name, importance_level, timestamp DESC)
                WHERE importance_level IN ('critical', 'high');
            """)

            cur.execute(f"""
                CREATE INDEX IF NOT EXISTS idx_context_{current_month}_tier_vector
                ON context_stream_{current_month} USING ivfflat (vector_embedding vector_cosine_ops)
                WHERE file_tier = 'hot' AND timestamp >= NOW() - INTERVAL '{self.hot_data_days} days';
            """)

            cur.execute(f"""
                CREATE INDEX IF NOT EXISTS idx_context_{current_month}_learning_protection
                ON context_stream_{current_month} (retention_category, file_tier, timestamp)
                WHERE retention_category IN ('learning', 'documentation', 'critical');
            """)

            conn.commit()
            cur.close()
            conn.close()

            return {
                "status": "success",
                "message": "o3æ¨å¥¨ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³å®Ÿè£…å®Œäº†",
                "project_name": self.project_root.name,
                "database": self.db_config["database"],
                "partitions_created": f"context_stream_{current_month}",
            }

        except Exception as e:
            return {"status": "error", "message": f"Partitioning failed: {str(e)}"}

    def accelerate_csa_data_accumulation_o3(self) -> Dict[str, Any]:
        """o3çµ±åˆCSAãƒ‡ãƒ¼ã‚¿è“„ç©åŠ é€Ÿ - å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—å¯¾å¿œ"""

        # o3æ¨å¥¨å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—å–å¾—
        file_patterns = {
            "logs": ["*.log", "*.txt"],
            "documentation": ["*.md", "*.rst", "*.txt"],
            "data": ["*.json", "*.yaml", "*.yml"],
            "reports": ["*report*", "*analysis*", "*summary*"],
            "learning": ["*mistake*", "*learning*", "*president*"],
        }

        all_files = []
        file_type_stats = {}

        for file_type, patterns in file_patterns.items():
            type_files = []
            for pattern in patterns:
                type_files.extend(list(self.project_root.rglob(pattern)))

            # é‡è¤‡é™¤å»
            unique_files = list(set(type_files))
            all_files.extend(unique_files)
            file_type_stats[file_type] = len(unique_files)

            if self.verbose_logging:
                print(f"ğŸ“‚ {file_type}: {len(unique_files)}ãƒ•ã‚¡ã‚¤ãƒ«")

        # é‡è¤‡é™¤å»ï¼ˆç•°ãªã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã§åŒã˜ãƒ•ã‚¡ã‚¤ãƒ«ãŒç„¡ã„ã‹ç¢ºèªï¼‰
        all_files = list(set(all_files))

        print(f"ğŸ“‚ ç·ç™ºè¦‹ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(all_files)}")
        print(f"ğŸ“ˆ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—åˆ¥: {file_type_stats}")

        processed_events = 0
        skipped_files = 0
        batch_size = 50  # o3æ¨å¥¨ãƒãƒƒãƒå‡¦ç†

        try:
            conn = psycopg2.connect(**self.db_config)
            cur = conn.cursor()

            for i, file_path in enumerate(all_files):
                try:
                    # o3æ¨å¥¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯ (50MBä»¥ä¸‹)
                    file_size = file_path.stat().st_size
                    if file_size > 50 * 1024 * 1024:
                        if self.verbose_logging:
                            print(
                                f"   âš ï¸ ã‚µã‚¤ã‚ºåˆ¶é™ã‚¹ã‚­ãƒƒãƒ—: {file_path.name} ({file_size / (1024 * 1024):.1f}MB)"
                            )
                        skipped_files += 1
                        continue

                    # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹èª­ã¿è¾¼ã¿
                    with open(file_path, encoding="utf-8", errors="ignore") as f:
                        content = f.read()

                    # o3æ¨å¥¨å†…å®¹å“è³ªãƒã‚§ãƒƒã‚¯
                    if len(content.strip()) < 10:  # ç©ºãƒ•ã‚¡ã‚¤ãƒ«
                        skipped_files += 1
                        continue

                    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—åˆ¤å®š
                    file_tier = self._determine_file_tier(file_path)
                    retention_category = self._determine_retention_category(
                        file_path, content
                    )

                    # o3çµ±åˆCSAã‚¤ãƒ™ãƒ³ãƒˆç”Ÿæˆ
                    events = self._generate_csa_events_from_file_o3(
                        file_path, content, file_tier, retention_category
                    )

                    # ä¸€æ‹¬ä¿å­˜
                    for event in events:
                        self._save_csa_event_o3_optimized(cur, event)
                        processed_events += 1

                    # o3æ¨å¥¨é€²æ—è¡¨ç¤º + ãƒãƒƒãƒã‚³ãƒŸãƒƒãƒˆ
                    if (i + 1) % batch_size == 0:
                        print(
                            f"   å‡¦ç†æ¸ˆã¿: {i + 1}/{len(all_files)} ãƒ•ã‚¡ã‚¤ãƒ« ({processed_events}ã‚¤ãƒ™ãƒ³ãƒˆ) [{(i + 1) / len(all_files) * 100:.1f}%]"
                        )
                        conn.commit()  # o3æ¨å¥¨ä¸­é–“ã‚³ãƒŸãƒƒãƒˆ

                except Exception as e:
                    if self.verbose_logging:
                        print(f"   âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {file_path.name}: {str(e)}")
                    skipped_files += 1
                    continue

            conn.commit()
            cur.close()
            conn.close()

            return {
                "status": "success",
                "processed_files": len(all_files) - skipped_files,
                "skipped_files": skipped_files,
                "total_files_discovered": len(all_files),
                "total_events": processed_events,
                "session_id": self.session_id,
                "project_name": self.project_root.name,
                "database": self.db_config["database"],
                "file_type_stats": file_type_stats,
                "events_per_file_avg": round(
                    processed_events / max(len(all_files) - skipped_files, 1), 2
                ),
            }

        except Exception as e:
            return {"status": "error", "error": str(e)}

    def _determine_file_tier(self, file_path: Path) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«å±¤åˆ¤å®šï¼ˆo3æ¨å¥¨éšå±¤åŒ–ï¼‰"""

        path_str = str(file_path).lower()
        name_lower = file_path.name.lower()

        # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿å±¤ï¼ˆæ°¸ç¶šä¿è­·ï¼‰
        if any(
            keyword in path_str for keyword in ["docs/", "ai-instructions/", "memory/"]
        ):
            return "protected"

        if any(
            keyword in name_lower
            for keyword in ["mistake", "learning", "president", "report", "analysis"]
        ):
            return "protected"

        # ã‚¦ã‚©ãƒ¼ãƒ å±¤ï¼ˆä¸­é–“ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ï¼‰
        if any(keyword in path_str for keyword in ["data/warm", "archive/", "backup/"]):
            return "warm"

        # ãƒ›ãƒƒãƒˆå±¤ï¼ˆé«˜é€Ÿã‚¢ã‚¯ã‚»ã‚¹ï¼‰
        if any(keyword in path_str for keyword in ["logs/", "tmp/", "runtime/"]):
            return "hot"

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ãƒ›ãƒƒãƒˆå±¤
        return "hot"

    def _determine_retention_category(self, file_path: Path, content: str) -> str:
        """ä¿æŒã‚«ãƒ†ã‚´ãƒªåˆ¤å®šï¼ˆo3æ¨å¥¨ãƒ‡ãƒ¼ã‚¿ä¿è­·ï¼‰"""

        str(file_path).lower()
        name_lower = file_path.name.lower()
        content_lower = content.lower()

        # ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ï¼ˆæœ€é‡è¦ï¼‰
        if any(
            keyword in content_lower
            for keyword in ["error", "critical", "failed", "exception"]
        ):
            return "critical"

        # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ï¼ˆæ°¸ç¶šä¿è­·ï¼‰
        if any(
            keyword in name_lower for keyword in ["mistake", "learning", "president"]
        ):
            return "learning"

        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆæ°¸ç¶šä¿è­·ï¼‰
        if file_path.suffix in [".md", ".rst"] or "readme" in name_lower:
            return "documentation"

        # ãƒ¬ãƒãƒ¼ãƒˆãƒ»åˆ†æï¼ˆé•·æœŸä¿è­·ï¼‰
        if any(keyword in name_lower for keyword in ["report", "analysis", "summary"]):
            return "report"

        # ã‚·ã‚¹ãƒ†ãƒ ãƒ­ã‚°ï¼ˆæ¨™æº–ä¿è­·ï¼‰
        if file_path.suffix == ".log":
            return "system_log"

        return "standard"

    def _generate_csa_events_from_file_o3(
        self, file_path: Path, content: str, file_tier: str, retention_category: str
    ) -> List[Dict[str, Any]]:
        """o3çµ±åˆãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰CSAã‚¤ãƒ™ãƒ³ãƒˆç¾¤ã‚’ç”Ÿæˆ"""

        events = []
        file_type = self._classify_file_type(file_path)

        # o3çµ±åˆãƒ•ã‚¡ã‚¤ãƒ«å…¨ä½“ã®è¦ç´„ã‚¤ãƒ™ãƒ³ãƒˆ
        summary_event = self._create_file_summary_event_o3(
            file_path, content, file_type, file_tier, retention_category
        )
        if summary_event:
            events.append(summary_event)

        # o3æ¨å¥¨å†…å®¹ã«åŸºã¥ãè©³ç´°ã‚¤ãƒ™ãƒ³ãƒˆ
        detail_events = self._extract_detail_events_o3(
            file_path, content, file_type, file_tier, retention_category
        )
        events.extend(detail_events)

        return events

    def _classify_file_type(self, file_path: Path) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—åˆ†é¡"""

        name_lower = file_path.name.lower()

        if file_path.suffix == ".log":
            if "error" in name_lower or "exception" in name_lower:
                return "error_log"
            elif "debug" in name_lower:
                return "debug_log"
            else:
                return "system_log"

        elif file_path.suffix == ".md":
            if "report" in name_lower:
                return "report"
            elif "readme" in name_lower:
                return "documentation"
            elif "mistake" in name_lower:
                return "learning_record"
            else:
                return "markdown_doc"

        else:
            return "general_file"

    def _create_file_summary_event_o3(
        self,
        file_path: Path,
        content: str,
        file_type: str,
        file_tier: str,
        retention_category: str,
    ) -> Optional[Dict[str, Any]]:
        """o3çµ±åˆãƒ•ã‚¡ã‚¤ãƒ«è¦ç´„ã‚¤ãƒ™ãƒ³ãƒˆä½œæˆ"""

        # ãƒ•ã‚¡ã‚¤ãƒ«çµ±è¨ˆ
        lines = content.split("\\n")
        word_count = len(content.split())

        # é‡è¦åº¦åˆ¤å®š
        importance = self._assess_importance(content, file_type)

        # o3æ¨å¥¨æ‹¡å¼µãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ§‹ç¯‰
        metadata = {
            "file_path": str(file_path.relative_to(self.project_root)),
            "file_type": file_type,
            "file_tier": file_tier,
            "retention_category": retention_category,
            "project_name": self.project_root.name,
            "file_size": file_path.stat().st_size,
            "line_count": len(lines),
            "word_count": word_count,
            "last_modified": datetime.fromtimestamp(
                file_path.stat().st_mtime
            ).isoformat(),
            "file_extension": file_path.suffix,
            "relative_path_depth": len(file_path.relative_to(self.project_root).parts),
        }

        # å†…å®¹è¦ç´„
        summary = self._generate_content_summary(content, file_type)

        # ãƒ™ã‚¯ãƒˆãƒ«åŸ‹ã‚è¾¼ã¿
        embedding = self._generate_embedding(
            f"{file_type}: {file_path.name}: {summary}"
        )

        return {
            "id": str(uuid.uuid4()),
            "timestamp": datetime.now(timezone.utc),
            "source": "file_analysis_o3",
            "event_type": f"{file_type}_summary",
            "content": summary,
            "metadata": metadata,
            "session_id": self.session_id,
            "vector_embedding": embedding,
            "importance_level": importance,
            "project_name": self.project_root.name,
            "file_tier": file_tier,
            "retention_category": retention_category,
        }

    def _extract_detail_events_o3(
        self,
        file_path: Path,
        content: str,
        file_type: str,
        file_tier: str,
        retention_category: str,
    ) -> List[Dict[str, Any]]:
        """o3çµ±åˆè©³ç´°ã‚¤ãƒ™ãƒ³ãƒˆæŠ½å‡º - å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—å¯¾å¿œ"""

        events = []

        # o3æ¨å¥¨åŒ…æ‹¬çš„ãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡º

        # ã‚¨ãƒ©ãƒ¼ãƒ»ä¾‹å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³
        if file_type in ["error_log", "system_log"] or retention_category == "critical":
            error_patterns = re.findall(
                r"(ERROR|Exception|Failed|Error|CRITICAL|Fatal).*",
                content,
                re.IGNORECASE,
            )
            for pattern in error_patterns[:10]:  # o3æ¨å¥¨1ãƒ•ã‚¡ã‚¤ãƒ«10ä»¶ã¾ã§
                events.append(
                    self._create_detail_event_o3(
                        file_path,
                        pattern,
                        "error_detection",
                        "critical",
                        file_tier,
                        retention_category,
                    )
                )

        # ãƒ¬ãƒãƒ¼ãƒˆãƒ»åˆ†æçµæœ
        if file_type == "report" or retention_category == "report":
            # æ—¥æœ¬èª + è‹±èªçµè«–ãƒ‘ã‚¿ãƒ¼ãƒ³
            conclusions = re.findall(
                r"(çµè«–|çµæœ|æˆæœ|åŠ¹æœ|Summary|Conclusion|Result)[:ï¼š\s]*(.*)",
                content,
                re.IGNORECASE,
            )
            for conclusion in conclusions[:5]:
                events.append(
                    self._create_detail_event_o3(
                        file_path,
                        f"{conclusion[0]}: {conclusion[1]}",
                        "report_conclusion",
                        "high",
                        file_tier,
                        retention_category,
                    )
                )

        # å­¦ç¿’ãƒ»ãƒŸã‚¹è¨˜éŒ²
        if retention_category == "learning":
            # ãƒŸã‚¹ç•ªå·ä»˜ããƒ‘ã‚¿ãƒ¼ãƒ³
            mistakes = re.findall(r"###\s*(\d+)\.\s*(.*)", content)
            for mistake in mistakes[:15]:  # o3æ¨å¥¨å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¯å¤šã‚ã«ä¿å­˜
                events.append(
                    self._create_detail_event_o3(
                        file_path,
                        f"ãƒŸã‚¹#{mistake[0]}: {mistake[1]}",
                        "mistake_record",
                        "critical",
                        file_tier,
                        retention_category,
                    )
                )

            # å­¦ç¿’ãƒ‘ã‚¿ãƒ¼ãƒ³
            learning_patterns = re.findall(
                r"(å­¦ç¿’|æ”¹å–„|å¯¾ç­–|Learning|Improvement)[:ï¼š\s]*(.*)",
                content,
                re.IGNORECASE,
            )
            for pattern in learning_patterns[:10]:
                events.append(
                    self._create_detail_event_o3(
                        file_path,
                        f"{pattern[0]}: {pattern[1]}",
                        "learning_insight",
                        "high",
                        file_tier,
                        retention_category,
                    )
                )

        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³é‡è¦ã‚»ã‚¯ã‚·ãƒ§ãƒ³
        if retention_category == "documentation":
            # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ˜ãƒƒãƒ€ãƒ¼
            headers = re.findall(r"^(#{1,3})\s+(.+)", content, re.MULTILINE)
            for header in headers[:8]:  # ä¸»è¦ãƒ˜ãƒƒãƒ€ãƒ¼ã®ã¿
                level = len(header[0])
                if level <= 2:  # H1, H2ã®ã¿
                    events.append(
                        self._create_detail_event_o3(
                            file_path,
                            header[1],
                            "documentation_section",
                            "medium",
                            file_tier,
                            retention_category,
                        )
                    )

        # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ï¼ˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå†…ï¼‰
        if file_type == "markdown_doc":
            code_blocks = re.findall(r"```(\w+)?\s*\n(.*?)\n```", content, re.DOTALL)
            for _i, (lang, code) in enumerate(code_blocks[:3]):  # æœ€åˆã®3å€‹
                if len(code.strip()) > 20:
                    events.append(
                        self._create_detail_event_o3(
                            file_path,
                            f"Code example ({lang or 'unknown'}): {code[:100]}...",
                            "code_example",
                            "medium",
                            file_tier,
                            retention_category,
                        )
                    )

        # æ•°å€¤ãƒ‡ãƒ¼ã‚¿ãƒ»çµ±è¨ˆ
        if "data" in file_type or "analysis" in file_path.name.lower():
            # æ•°å€¤ãƒ‘ã‚¿ãƒ¼ãƒ³
            numbers = re.findall(
                r"(\d+(?:\.\d+)?\s*(?:%|MB|GB|KB|ms|sec|min))", content
            )
            if len(numbers) > 5:  # æ•°å€¤ãŒå¤šã„ãƒ•ã‚¡ã‚¤ãƒ«
                events.append(
                    self._create_detail_event_o3(
                        file_path,
                        f"Numerical data detected: {len(numbers)} metrics",
                        "data_metrics",
                        "medium",
                        file_tier,
                        retention_category,
                    )
                )

        return events

    def _create_detail_event_o3(
        self,
        file_path: Path,
        content: str,
        event_type: str,
        importance: str,
        file_tier: str,
        retention_category: str,
    ) -> Dict[str, Any]:
        """o3çµ±åˆè©³ç´°ã‚¤ãƒ™ãƒ³ãƒˆä½œæˆ"""

        return {
            "id": str(uuid.uuid4()),
            "timestamp": datetime.now(timezone.utc),
            "source": "content_analysis_o3",
            "event_type": event_type,
            "content": content[:1000],  # o3æ¨å¥¨æ‹¡å¼µåˆ¶é™
            "metadata": {
                "source_file": str(file_path.relative_to(self.project_root)),
                "extraction_method": "o3_pattern_matching",
                "file_tier": file_tier,
                "retention_category": retention_category,
                "project_name": self.project_root.name,
            },
            "session_id": self.session_id,
            "vector_embedding": self._generate_embedding(content),
            "importance_level": importance,
            "project_name": self.project_root.name,
            "file_tier": file_tier,
            "retention_category": retention_category,
        }

    def _assess_importance(self, content: str, file_type: str) -> str:
        """é‡è¦åº¦è©•ä¾¡"""

        content_lower = content.lower()

        # ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«æ¡ä»¶
        if any(
            keyword in content_lower
            for keyword in ["error", "critical", "failed", "exception"]
        ):
            return "critical"

        # é«˜é‡è¦åº¦æ¡ä»¶
        if file_type in ["learning_record", "error_log"] or any(
            keyword in content_lower
            for keyword in ["important", "é‡è¦", "urgent", "ç·Šæ€¥"]
        ):
            return "high"

        # ãƒ¬ãƒãƒ¼ãƒˆé¡ã¯ä¸­é‡è¦åº¦
        if file_type == "report":
            return "medium"

        return "normal"

    def _generate_content_summary(self, content: str, file_type: str) -> str:
        """å†…å®¹è¦ç´„ç”Ÿæˆ"""

        lines = content.split("\\n")

        if file_type == "error_log":
            # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã¯æœ€åˆã®ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            for line in lines:
                if any(
                    keyword in line.lower()
                    for keyword in ["error", "exception", "failed"]
                ):
                    return line.strip()[:200]

        elif file_type == "report":
            # ãƒ¬ãƒãƒ¼ãƒˆã¯æœ€åˆã®æ®µè½
            for line in lines:
                if len(line.strip()) > 50:
                    return line.strip()[:200]

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯æœ€åˆã®æ„å‘³ã®ã‚ã‚‹è¡Œ
        for line in lines:
            if len(line.strip()) > 20:
                return line.strip()[:200]

        return f"ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æ: {file_type} ({len(lines)}è¡Œ)"

    def _generate_embedding(self, text: str) -> List[float]:
        """ãƒ™ã‚¯ãƒˆãƒ«åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ"""
        try:
            client = openai.OpenAI()
            response = client.embeddings.create(
                model=self.embedding_model,
                input=text[:8000],  # OpenAIåˆ¶é™å¯¾å¿œ
            )
            return response.data[0].embedding
        except Exception as e:
            print(f"Embedding generation failed: {e}")
            return [0.0] * 1536

    def _save_csa_event_o3_optimized(self, cursor, event: Dict[str, Any]) -> bool:
        """o3çµ±åˆæœ€é©åŒ–CSAã‚¤ãƒ™ãƒ³ãƒˆä¿å­˜"""
        try:
            # o3æ¨å¥¨ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³å¯¾å¿œãƒ†ãƒ¼ãƒ–ãƒ«ã«ä¿å­˜
            cursor.execute(
                """
                INSERT INTO context_stream_partitioned
                (id, timestamp, source, event_type, content, metadata, session_id, vector_embedding,
                 importance_level, project_name, file_tier, retention_category)
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                ON CONFLICT (id) DO NOTHING;
            """,
                (
                    event["id"],
                    event["timestamp"],
                    event["source"],
                    event["event_type"],
                    event["content"],
                    json.dumps(event["metadata"]),
                    event["session_id"],
                    event["vector_embedding"],
                    event["importance_level"],
                    event.get("project_name", self.project_root.name),
                    event.get("file_tier", "hot"),
                    event.get("retention_category", "standard"),
                ),
            )
            return True
        except Exception as e:
            if self.verbose_logging:
                print(f"o3 Event save failed: {e}")
            return False

    def enhanced_context_search_v2(
        self, query: str, importance_filter: List[str] = None, limit: int = 20
    ) -> Dict[str, Any]:
        """å¼·åŒ–ã•ã‚ŒãŸæ–‡è„ˆæ¤œç´¢ v2"""

        query_embedding = self._generate_embedding(query)

        try:
            conn = psycopg2.connect(**self.db_config, cursor_factory=RealDictCursor)
            cur = conn.cursor()

            # é‡è¦åº¦ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶
            importance_condition = ""
            params = [query_embedding, query_embedding]

            if importance_filter:
                placeholders = ",".join(["%s"] * len(importance_filter))
                importance_condition = f"AND importance_level IN ({placeholders})"
                params.extend(importance_filter)

            params.extend([f"%{query}%", f"%{query}%", query_embedding, limit])

            # è¤‡åˆæ¤œç´¢: ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³è€ƒæ…® + é‡è¦åº¦ + ãƒ™ã‚¯ãƒˆãƒ«é¡ä¼¼åº¦
            cur.execute(
                f"""
                SELECT
                    id, timestamp, source, event_type, content, metadata, importance_level,
                    1 - (vector_embedding <=> %s::vector) as similarity
                FROM context_stream_partitioned
                WHERE vector_embedding IS NOT NULL
                  AND timestamp >= NOW() - INTERVAL '{self.data_retention_days} days'
                  {importance_condition}
                  AND (
                    1 - (vector_embedding <=> %s::vector) > 0.5
                    OR content ILIKE %s
                    OR event_type ILIKE %s
                  )
                ORDER BY
                    CASE importance_level
                        WHEN 'critical' THEN 1
                        WHEN 'high' THEN 2
                        WHEN 'medium' THEN 3
                        ELSE 4
                    END,
                    (1 - (vector_embedding <=> %s::vector)) DESC,
                    timestamp DESC
                LIMIT %s;
            """,
                params,
            )

            results = cur.fetchall()
            cur.close()
            conn.close()

            # çµæœåˆ†æ
            categorized = self._categorize_results_v2([dict(row) for row in results])

            return {
                "status": "success",
                "query": query,
                "total_results": len(results),
                "categorized_results": categorized,
                "search_metadata": {
                    "importance_filter": importance_filter,
                    "retention_days": self.data_retention_days,
                    "search_algorithm": "importance_weighted_semantic",
                },
            }

        except Exception as e:
            return {"status": "error", "error": str(e)}

    def _categorize_results_v2(
        self, results: List[Dict[str, Any]]
    ) -> Dict[str, List[Dict[str, Any]]]:
        """çµæœåˆ†æ v2 - é‡è¦åº¦åˆ¥"""

        categories = {
            "critical_events": [],
            "high_importance": [],
            "error_analysis": [],
            "learning_records": [],
            "reports": [],
            "general": [],
        }

        for result in results:
            importance = result["importance_level"]
            event_type = result["event_type"]

            if importance == "critical":
                categories["critical_events"].append(result)
            elif importance == "high":
                categories["high_importance"].append(result)
            elif "error" in event_type:
                categories["error_analysis"].append(result)
            elif "mistake" in event_type or "learning" in event_type:
                categories["learning_records"].append(result)
            elif "report" in event_type:
                categories["reports"].append(result)
            else:
                categories["general"].append(result)

        return categories


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ - CSAå®Œå…¨ã‚·ã‚¹ãƒ†ãƒ """
    print("ğŸŒŠ CSAæ–‡è„ˆã‚·ã‚¹ãƒ†ãƒ å®Œå…¨å®Ÿè£…é–‹å§‹")

    csa_system = CSACompleteSystem()

    # 1. o3æ¨å¥¨ãƒ‡ãƒ¼ã‚¿ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³å®Ÿè£…
    print("\\n1ï¸âƒ£ o3æ¨å¥¨ãƒ‡ãƒ¼ã‚¿ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³å®Ÿè£…")
    partition_result = csa_system.implement_o3_data_partitioning()
    print(f"ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³: {partition_result['status']}")

    if partition_result["status"] == "error":
        print(f"ã‚¨ãƒ©ãƒ¼: {partition_result['message']}")
        return

    # 2. CSAãƒ‡ãƒ¼ã‚¿è“„ç©åŠ é€Ÿ
    print("\\n2ï¸âƒ£ CSAãƒ‡ãƒ¼ã‚¿è“„ç©åŠ é€Ÿå‡¦ç†")
    accumulation_result = csa_system.accelerate_csa_data_accumulation_o3()
    print(f"è“„ç©çµæœ: {accumulation_result['status']}")

    if accumulation_result["status"] == "success":
        print(f"   å‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {accumulation_result['processed_files']}")
        print(f"   ã‚¹ã‚­ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {accumulation_result['skipped_files']}")
        print(f"   ç·ã‚¤ãƒ™ãƒ³ãƒˆæ•°: {accumulation_result['total_events']}")
        print(f"   ã‚»ãƒƒã‚·ãƒ§ãƒ³ID: {accumulation_result['session_id'][:8]}...")
    else:
        print(f"   ã‚¨ãƒ©ãƒ¼: {accumulation_result['error']}")
        return

    # 3. å¼·åŒ–ã•ã‚ŒãŸæ–‡è„ˆæ¤œç´¢ãƒ†ã‚¹ãƒˆ
    print("\\n3ï¸âƒ£ å¼·åŒ–ã•ã‚ŒãŸæ–‡è„ˆæ¤œç´¢ v2 ãƒ†ã‚¹ãƒˆ")
    test_queries = [
        ("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼", ["critical", "high"]),
        ("ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œ", ["high", "medium"]),
        ("å­¦ç¿’è¨˜éŒ²", ["critical"]),
        ("ãƒ¬ãƒãƒ¼ãƒˆåˆ†æ", ["medium"]),
    ]

    for query, importance_filter in test_queries:
        search_result = csa_system.enhanced_context_search_v2(
            query, importance_filter, limit=10
        )
        print(f"\\n   æ¤œç´¢: '{query}' (é‡è¦åº¦: {importance_filter})")
        print(f"   çµæœ: {search_result.get('total_results', 0)}ä»¶")

        if search_result["status"] == "success":
            categorized = search_result["categorized_results"]
            for category, events in categorized.items():
                if events:
                    print(f"     {category}: {len(events)}ä»¶")
                    for event in events[:2]:
                        print(
                            f"       - [{event['importance_level']}] {event['content'][:60]}..."
                        )

    print("\\nâœ… CSAæ–‡è„ˆã‚·ã‚¹ãƒ†ãƒ å®Œå…¨å®Ÿè£…å®Œäº†")
    print("ğŸ“ å…¨ãƒ•ã‚¡ã‚¤ãƒ«è§£æã«ã‚ˆã‚‹å¤§é‡æ–‡è„ˆãƒ‡ãƒ¼ã‚¿è“„ç© + o3æ¨å¥¨ãƒ‡ãƒ¼ã‚¿ç®¡ç†å®Ÿè£…")


if __name__ == "__main__":
    main()
