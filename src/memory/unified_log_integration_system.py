#!/usr/bin/env python3
"""
ğŸ“Š çµ±ä¸€ãƒ­ã‚°çµ±åˆã‚·ã‚¹ãƒ†ãƒ  - å…¨117+ãƒ•ã‚¡ã‚¤ãƒ«å®Œå…¨çµ±åˆ
=================================================

ã€o3çµ±åˆè¨­è¨ˆã€‘
- å…¨ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ­ã‚°è‡ªå‹•æ¤œå‡º
- æ§‹é€ åŒ–ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±åˆ
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¤œç´¢ãƒ»åˆ†æ
- éšå±¤åŒ–ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ç®¡ç†

ã€å®Ÿè£…å†…å®¹ã€‘
- 117+ãƒ•ã‚¡ã‚¤ãƒ«è‡ªå‹•ã‚¹ã‚­ãƒ£ãƒ³ãƒ»è§£æ
- ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ãƒ»ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆè‡ªå‹•åˆ†é¡
- PostgreSQLçµ±åˆæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ 
- ãƒ­ã‚°å“è³ªè©•ä¾¡ãƒ»é‡è¤‡é™¤å»
- ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåˆ¥ãƒ­ã‚°ç®¡ç†
"""

import hashlib
import json
import re
import uuid
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, List, Optional

import psycopg2
from psycopg2.extras import RealDictCursor


class UnifiedLogIntegrationSystem:
    """çµ±ä¸€ãƒ­ã‚°çµ±åˆã‚·ã‚¹ãƒ†ãƒ  - o3æ¨å¥¨ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåˆ¥å®Ÿè£…"""

    def __init__(
        self, project_root: Optional[Path] = None, config_file: Optional[str] = None
    ):
        """åˆæœŸåŒ– - ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåˆ¥è¨­å®šå¯¾å¿œ"""

        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆè‡ªå‹•æ¤œå‡º
        if project_root:
            self.project_root = project_root
        else:
            self.project_root = Path(__file__).parent.parent

        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåˆ¥è¨­å®šèª­ã¿è¾¼ã¿
        self.config = self._load_project_config(config_file)

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­å®šï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåˆ¥ï¼‰
        self.db_config = self.config.get(
            "database",
            {
                "host": "localhost",
                "database": f"{self.project_root.name}_ai",
                "user": "dd",
                "password": "",
                "port": 5432,
            },
        )

        # o3æ¨å¥¨ãƒ­ã‚°åé›†è¨­å®š
        collection_config = self.config.get("log_collection", {})
        self.file_size_limit_mb = collection_config.get("max_file_size_mb", 50)
        self.batch_size = collection_config.get("batch_size", 100)
        self.duplicate_threshold = collection_config.get("duplicate_threshold", 0.95)

        # o3æ¨å¥¨ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³
        patterns_config = self.config.get("log_patterns", {})

        # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«æ¤œå‡ºãƒ‘ã‚¿ãƒ¼ãƒ³
        self.log_file_patterns = patterns_config.get(
            "file_patterns",
            [
                "*.log",
                "*.txt",
                "*log*",
                "*error*",
                "*debug*",
                "*trace*",
                "*output*",
                "*runtime*",
                "*operation*",
                "*system*",
            ],
        )

        # é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³
        self.excluded_patterns = patterns_config.get(
            "excluded",
            [
                "*.tmp",
                "*.cache",
                "*.pyc",
                "*.pyo",
                "__pycache__/*",
                "node_modules/*",
                ".git/*",
                "*.bak",
                "*.backup",
            ],
        )

        # o3æ¨å¥¨ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«æ¤œå‡ºãƒ‘ã‚¿ãƒ¼ãƒ³
        self.log_level_patterns = {
            "CRITICAL": [r"CRITICAL", r"FATAL", r"EMERGENCY", r"è‡´å‘½çš„", r"é‡å¤§"],
            "ERROR": [
                r"ERROR",
                r"EXCEPTION",
                r"FAILED",
                r"FAILURE",
                r"ã‚¨ãƒ©ãƒ¼",
                r"å¤±æ•—",
            ],
            "WARNING": [r"WARNING", r"WARN", r"CAUTION", r"æ³¨æ„", r"è­¦å‘Š"],
            "INFO": [r"INFO", r"INFORMATION", r"æƒ…å ±", r"é€šçŸ¥"],
            "DEBUG": [r"DEBUG", r"TRACE", r"VERBOSE", r"ãƒ‡ãƒãƒƒã‚°", r"è©³ç´°"],
            "UNKNOWN": [],
        }

        # o3æ¨å¥¨ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆæ¤œå‡ºãƒ‘ã‚¿ãƒ¼ãƒ³
        self.component_patterns = {
            "database": [r"database", r"postgres", r"sql", r"db", r"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹"],
            "api": [r"api", r"endpoint", r"request", r"response", r"http", r"rest"],
            "auth": [r"auth", r"login", r"password", r"token", r"èªè¨¼", r"ãƒ­ã‚°ã‚¤ãƒ³"],
            "file_system": [
                r"file",
                r"directory",
                r"path",
                r"disk",
                r"storage",
                r"ãƒ•ã‚¡ã‚¤ãƒ«",
            ],
            "network": [
                r"network",
                r"socket",
                r"connection",
                r"tcp",
                r"udp",
                r"ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯",
            ],
            "security": [
                r"security",
                r"vulnerability",
                r"attack",
                r"ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£",
                r"è„†å¼±æ€§",
            ],
            "performance": [
                r"performance",
                r"latency",
                r"throughput",
                r"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹",
                r"æ€§èƒ½",
            ],
            "operations": [r"operation", r"workflow", r"process", r"æ“ä½œ", r"é‹ç”¨"],
            "memory": [r"memory", r"ram", r"heap", r"stack", r"ãƒ¡ãƒ¢ãƒª"],
            "ai_learning": [
                r"learning",
                r"mistake",
                r"president",
                r"ai",
                r"å­¦ç¿’",
                r"ãƒŸã‚¹",
            ],
        }

        # ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè¨­å®š
        paths_config = self.config.get("paths", {})
        default_log_dirs = [
            "logs",
            "tmp",
            "runtime",
            "operations",
            "memory",
            "src",
            "scripts",
        ]
        log_directories = paths_config.get("log_directories", default_log_dirs)
        self.log_directories = [self.project_root / path for path in log_directories]

        # UXè¨­å®š
        ux_config = self.config.get("ux", {})
        self.verbose_logging = ux_config.get("verbose_logging", True)
        self.progress_reporting = ux_config.get("progress_reporting", True)

        # å‡¦ç†çµ±è¨ˆ
        self.processing_stats = {
            "files_discovered": 0,
            "files_processed": 0,
            "files_skipped": 0,
            "logs_extracted": 0,
            "duplicates_removed": 0,
            "errors_encountered": 0,
        }

    def _load_project_config(self, config_file: Optional[str] = None) -> Dict[str, Any]:
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨­å®šèª­ã¿è¾¼ã¿"""

        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«å€™è£œ
        config_candidates = []

        if config_file:
            config_candidates.append(Path(config_file))

        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†…è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«å€™è£œ
        config_candidates.extend(
            [
                self.project_root / "log_config.json",
                self.project_root / "config" / "logs.json",
                self.project_root / ".log_config.json",
                self.project_root / "memory_config.json",  # æ—¢å­˜è¨­å®šã¨ã®çµ±åˆ
            ]
        )

        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        for config_path in config_candidates:
            if config_path.exists():
                try:
                    with open(config_path, encoding="utf-8") as f:
                        config = json.load(f)
                    if self.verbose_logging:
                        print(f"ğŸ“„ ãƒ­ã‚°è¨­å®šèª­ã¿è¾¼ã¿: {config_path}")
                    return config
                except Exception as e:
                    if self.verbose_logging:
                        print(f"âš ï¸ è¨­å®šèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ {config_path}: {e}")
                    continue

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
        return self._create_default_log_config()

    def _create_default_log_config(self) -> Dict[str, Any]:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ­ã‚°è¨­å®šç”Ÿæˆ"""
        return {
            "database": {
                "host": "localhost",
                "database": f"{self.project_root.name}_ai",
                "user": "dd",
                "password": "",
                "port": 5432,
            },
            "log_collection": {
                "max_file_size_mb": 50,
                "batch_size": 100,
                "duplicate_threshold": 0.95,
            },
            "paths": {
                "log_directories": [
                    "logs",
                    "tmp",
                    "runtime",
                    "operations",
                    "memory",
                    "src",
                    "scripts",
                ]
            },
            "ux": {"verbose_logging": True, "progress_reporting": True},
        }

    def discover_all_log_files(self) -> List[Path]:
        """å…¨ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«è‡ªå‹•ç™ºè¦‹"""

        discovered_files = set()

        # è¨­å®šã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢
        for log_dir in self.log_directories:
            if not log_dir.exists():
                continue

            for pattern in self.log_file_patterns:
                try:
                    files = list(log_dir.rglob(pattern))
                    discovered_files.update(files)
                except Exception as e:
                    if self.verbose_logging:
                        print(f"âš ï¸ ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œç´¢ã‚¨ãƒ©ãƒ¼ {pattern} in {log_dir}: {e}")

        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆç›´ä¸‹ã®é‡è¦ãƒ•ã‚¡ã‚¤ãƒ«
        for pattern in self.log_file_patterns:
            try:
                files = list(self.project_root.glob(pattern))
                discovered_files.update(files)
            except Exception as e:
                if self.verbose_logging:
                    print(f"âš ï¸ ãƒ«ãƒ¼ãƒˆæ¤œç´¢ã‚¨ãƒ©ãƒ¼ {pattern}: {e}")

        # é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        filtered_files = []
        for file_path in discovered_files:
            if self._is_file_excluded(file_path):
                continue

            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
            try:
                file_size_mb = file_path.stat().st_size / (1024 * 1024)
                if file_size_mb > self.file_size_limit_mb:
                    if self.verbose_logging:
                        print(
                            f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºåˆ¶é™ã«ã‚ˆã‚Šã‚¹ã‚­ãƒƒãƒ—: {file_path} ({file_size_mb:.1f}MB)"
                        )
                    continue
            except Exception:
                continue

            filtered_files.append(file_path)

        self.processing_stats["files_discovered"] = len(filtered_files)

        if self.verbose_logging:
            print(f"ğŸ“‚ ç™ºè¦‹ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(filtered_files)}")

        return filtered_files

    def _is_file_excluded(self, file_path: Path) -> bool:
        """ãƒ•ã‚¡ã‚¤ãƒ«é™¤å¤–åˆ¤å®š"""

        path_str = str(file_path).lower()
        name_lower = file_path.name.lower()

        for pattern in self.excluded_patterns:
            pattern_clean = pattern.replace("*", "").lower()
            if pattern_clean in name_lower or pattern_clean in path_str:
                return True

        return False

    def extract_log_entries_from_file(self, file_path: Path) -> List[Dict[str, Any]]:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ­ã‚°ã‚¨ãƒ³ãƒˆãƒªæŠ½å‡º"""

        if not file_path.exists() or not file_path.is_file():
            return []

        try:
            with open(file_path, encoding="utf-8", errors="ignore") as f:
                content = f.read()
        except Exception as e:
            if self.verbose_logging:
                print(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
            self.processing_stats["errors_encountered"] += 1
            return []

        if len(content.strip()) < 10:  # ç©ºãƒ•ã‚¡ã‚¤ãƒ«
            return []

        lines = content.split("\n")
        log_entries = []

        # ãƒ•ã‚¡ã‚¤ãƒ«åŸºæœ¬æƒ…å ±
        file_stat = file_path.stat()
        file_info = {
            "source_file": str(file_path.relative_to(self.project_root)),
            "file_size": file_stat.st_size,
            "file_modified": datetime.fromtimestamp(file_stat.st_mtime),
            "project_name": self.project_root.name,
        }

        # è¡Œã”ã¨ã«ãƒ­ã‚°ã‚¨ãƒ³ãƒˆãƒªè§£æ
        for line_num, line in enumerate(lines, 1):
            line = line.strip()
            if len(line) < 5:  # çŸ­ã™ãã‚‹è¡Œã¯ã‚¹ã‚­ãƒƒãƒ—
                continue

            # ãƒ­ã‚°ã‚¨ãƒ³ãƒˆãƒªä½œæˆ
            log_entry = self._parse_log_line(line, line_num, file_info)
            if log_entry:
                log_entries.append(log_entry)

        # ãƒ•ã‚¡ã‚¤ãƒ«å…¨ä½“ã‚µãƒãƒªãƒ¼ã‚¨ãƒ³ãƒˆãƒª
        summary_entry = self._create_file_summary_entry(file_path, lines, file_info)
        if summary_entry:
            log_entries.append(summary_entry)

        return log_entries

    def _parse_log_line(
        self, line: str, line_num: int, file_info: Dict[str, Any]
    ) -> Optional[Dict[str, Any]]:
        """ãƒ­ã‚°è¡Œè§£æ"""

        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æŠ½å‡ºè©¦è¡Œ
        timestamp = self._extract_timestamp(line)

        # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«æ¤œå‡º
        log_level = self._detect_log_level(line)

        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆæ¤œå‡º
        component = self._detect_component(line, file_info["source_file"])

        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        cleaned_message = self._clean_log_message(line)

        # é‡è¦åº¦è©•ä¾¡
        importance_level = self._evaluate_log_importance(line, log_level)

        # æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
        structured_data = self._extract_structured_data(line)

        # ãƒ­ã‚°ã‚¨ãƒ³ãƒˆãƒªä½œæˆ
        log_entry = {
            "id": str(uuid.uuid4()),
            "timestamp": timestamp or datetime.now(timezone.utc),
            "source_file": file_info["source_file"],
            "line_number": line_num,
            "log_level": log_level,
            "component": component,
            "message": cleaned_message,
            "raw_content": line,
            "importance_level": importance_level,
            "structured_data": structured_data,
            "project_name": file_info["project_name"],
            "file_size": file_info["file_size"],
            "file_modified": file_info["file_modified"],
            "extracted_at": datetime.now(timezone.utc),
            "content_hash": hashlib.sha256(line.encode("utf-8")).hexdigest()[:16],
        }

        return log_entry

    def _extract_timestamp(self, line: str) -> Optional[datetime]:
        """ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æŠ½å‡º"""

        # ä¸€èˆ¬çš„ãªã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãƒ‘ã‚¿ãƒ¼ãƒ³
        timestamp_patterns = [
            r"(\d{4}-\d{2}-\d{2}[T ]\d{2}:\d{2}:\d{2})",  # ISO format
            r"(\d{2}/\d{2}/\d{4} \d{2}:\d{2}:\d{2})",  # US format
            r"(\d{2}-\d{2}-\d{4} \d{2}:\d{2}:\d{2})",  # EU format
            r"(\d{10,13})",  # Unix timestamp
        ]

        for pattern in timestamp_patterns:
            match = re.search(pattern, line)
            if match:
                timestamp_str = match.group(1)
                try:
                    if timestamp_str.isdigit():  # Unix timestamp
                        if len(timestamp_str) == 13:  # milliseconds
                            return datetime.fromtimestamp(
                                int(timestamp_str) / 1000, tz=timezone.utc
                            )
                        else:  # seconds
                            return datetime.fromtimestamp(
                                int(timestamp_str), tz=timezone.utc
                            )
                    else:
                        # æ–‡å­—åˆ—å½¢å¼ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
                        for fmt in [
                            "%Y-%m-%d %H:%M:%S",
                            "%Y-%m-%dT%H:%M:%S",
                            "%m/%d/%Y %H:%M:%S",
                            "%d-%m-%Y %H:%M:%S",
                        ]:
                            try:
                                return datetime.strptime(timestamp_str, fmt).replace(
                                    tzinfo=timezone.utc
                                )
                            except ValueError:
                                continue
                except (ValueError, OSError):
                    continue

        return None

    def _detect_log_level(self, line: str) -> str:
        """ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«æ¤œå‡º"""

        line_upper = line.upper()

        for level, patterns in self.log_level_patterns.items():
            for pattern in patterns:
                if re.search(pattern, line_upper):
                    return level

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ¨å®š
        if any(
            keyword in line_upper for keyword in ["EXCEPTION", "TRACEBACK", "STACK"]
        ):
            return "ERROR"
        elif any(keyword in line_upper for keyword in ["SUCCESS", "COMPLETE", "DONE"]):
            return "INFO"
        else:
            return "UNKNOWN"

    def _detect_component(self, line: str, source_file: str) -> str:
        """ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆæ¤œå‡º"""

        line_lower = line.lower()
        source_lower = source_file.lower()

        # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆæ¨å®š
        for component, patterns in self.component_patterns.items():
            for pattern in patterns:
                if pattern in source_lower or pattern in line_lower:
                    return component

        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåã‹ã‚‰ã®æ¨å®š
        if "memory/" in source_lower:
            return "ai_learning"
        elif "operations/" in source_lower:
            return "operations"
        elif "src/" in source_lower:
            return "application"
        elif "scripts/" in source_lower:
            return "automation"
        else:
            return "general"

    def _clean_log_message(self, line: str) -> str:
        """ãƒ­ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""

        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—é™¤å»
        cleaned = re.sub(r"\d{4}-\d{2}-\d{2}[T ]\d{2}:\d{2}:\d{2}[^\s]*", "", line)
        cleaned = re.sub(r"\d{2}/\d{2}/\d{4} \d{2}:\d{2}:\d{2}", "", cleaned)

        # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«é™¤å»
        for level in self.log_level_patterns.keys():
            cleaned = re.sub(rf"\b{level}\b", "", cleaned, flags=re.IGNORECASE)

        # ä½™åˆ†ãªç©ºç™½ãƒ»æ–‡å­—é™¤å»
        cleaned = re.sub(r"^\s*[\[\]():;,-]+\s*", "", cleaned)
        cleaned = re.sub(r"\s+", " ", cleaned)

        return cleaned.strip()

    def _evaluate_log_importance(self, line: str, log_level: str) -> str:
        """ãƒ­ã‚°é‡è¦åº¦è©•ä¾¡"""

        line_lower = line.lower()

        # Criticalæ¡ä»¶
        if log_level in ["CRITICAL", "ERROR"] or any(
            keyword in line_lower
            for keyword in ["critical", "fatal", "emergency", "exception", "failed"]
        ):
            return "critical"

        # Highæ¡ä»¶
        if log_level == "WARNING" or any(
            keyword in line_lower
            for keyword in ["warning", "error", "timeout", "retry", "è­¦å‘Š", "ã‚¨ãƒ©ãƒ¼"]
        ):
            return "high"

        # Mediumæ¡ä»¶
        if log_level == "INFO" or any(
            keyword in line_lower
            for keyword in ["success", "complete", "start", "finish", "æˆåŠŸ", "å®Œäº†"]
        ):
            return "medium"

        # Lowæ¡ä»¶
        return "low"

    def _extract_structured_data(self, line: str) -> Dict[str, Any]:
        """æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿æŠ½å‡º"""

        structured_data = {}

        # æ•°å€¤ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
        numbers = re.findall(
            r"\b(\d+(?:\.\d+)?)\s*(ms|sec|mb|gb|kb|%|bytes?)\b", line.lower()
        )
        if numbers:
            structured_data["metrics"] = {
                f"{unit}": float(value) for value, unit in numbers
            }

        # IPã‚¢ãƒ‰ãƒ¬ã‚¹æŠ½å‡º
        ip_pattern = r"\b(?:\d{1,3}\.){3}\d{1,3}\b"
        ips = re.findall(ip_pattern, line)
        if ips:
            structured_data["ip_addresses"] = ips

        # URLãƒ‘ã‚¹æŠ½å‡º
        path_pattern = r"(?:GET|POST|PUT|DELETE)\s+([/\w\-\.]+)"
        paths = re.findall(path_pattern, line)
        if paths:
            structured_data["api_paths"] = paths

        # ã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ‰æŠ½å‡º
        error_codes = re.findall(r"\b(4\d{2}|5\d{2})\b", line)
        if error_codes:
            structured_data["error_codes"] = [int(code) for code in error_codes]

        return structured_data

    def _create_file_summary_entry(
        self, file_path: Path, lines: List[str], file_info: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚µãƒãƒªãƒ¼ã‚¨ãƒ³ãƒˆãƒªä½œæˆ"""

        total_lines = len(lines)
        non_empty_lines = len([line for line in lines if line.strip()])

        # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«åˆ†å¸ƒè¨ˆç®—
        level_distribution = {}
        for line in lines:
            level = self._detect_log_level(line)
            level_distribution[level] = level_distribution.get(level, 0) + 1

        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—æ¨å®š
        file_type = "log_file"
        name_lower = file_path.name.lower()
        if "error" in name_lower:
            file_type = "error_log"
        elif "debug" in name_lower:
            file_type = "debug_log"
        elif "operation" in name_lower:
            file_type = "operation_log"
        elif "memory" in str(file_path).lower():
            file_type = "memory_log"

        return {
            "id": str(uuid.uuid4()),
            "timestamp": datetime.now(timezone.utc),
            "source_file": file_info["source_file"],
            "line_number": 0,  # ã‚µãƒãƒªãƒ¼ã‚¨ãƒ³ãƒˆãƒªãƒãƒ¼ã‚«ãƒ¼
            "log_level": "INFO",
            "component": "file_summary",
            "message": f"File analysis summary: {total_lines} total lines, {non_empty_lines} non-empty lines",
            "raw_content": f"FILE_SUMMARY: {file_path.name}",
            "importance_level": "medium",
            "structured_data": {
                "total_lines": total_lines,
                "non_empty_lines": non_empty_lines,
                "level_distribution": level_distribution,
                "file_type": file_type,
                "analysis_timestamp": datetime.now().isoformat(),
            },
            "project_name": file_info["project_name"],
            "file_size": file_info["file_size"],
            "file_modified": file_info["file_modified"],
            "extracted_at": datetime.now(timezone.utc),
            "content_hash": hashlib.sha256(
                f"SUMMARY_{file_path.name}".encode()
            ).hexdigest()[:16],
        }

    def setup_unified_log_database(self) -> Dict[str, Any]:
        """çµ±ä¸€ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""

        try:
            conn = psycopg2.connect(**self.db_config)
            cur = conn.cursor()

            # çµ±ä¸€ãƒ­ã‚°ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
            cur.execute("""
                CREATE TABLE IF NOT EXISTS unified_logs (
                    id UUID PRIMARY KEY,
                    timestamp TIMESTAMPTZ NOT NULL,
                    source_file TEXT NOT NULL,
                    line_number INTEGER,
                    log_level VARCHAR(20) NOT NULL,
                    component VARCHAR(50) NOT NULL,
                    message TEXT NOT NULL,
                    raw_content TEXT,
                    importance_level VARCHAR(20) DEFAULT 'low',
                    structured_data JSONB,
                    project_name VARCHAR(100) NOT NULL,
                    file_size INTEGER,
                    file_modified TIMESTAMPTZ,
                    extracted_at TIMESTAMPTZ DEFAULT NOW(),
                    content_hash VARCHAR(16),
                    created_at TIMESTAMPTZ DEFAULT NOW()
                );
            """)

            # o3æ¨å¥¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
            indexes = [
                "CREATE INDEX IF NOT EXISTS idx_unified_logs_timestamp ON unified_logs (timestamp DESC);",
                "CREATE INDEX IF NOT EXISTS idx_unified_logs_project_level ON unified_logs (project_name, log_level, timestamp DESC);",
                "CREATE INDEX IF NOT EXISTS idx_unified_logs_component ON unified_logs (component, importance_level);",
                "CREATE INDEX IF NOT EXISTS idx_unified_logs_source ON unified_logs (source_file, line_number);",
                "CREATE INDEX IF NOT EXISTS idx_unified_logs_content_hash ON unified_logs (content_hash);",
                "CREATE INDEX IF NOT EXISTS idx_unified_logs_importance ON unified_logs (importance_level, timestamp DESC);",
                "CREATE INDEX IF NOT EXISTS idx_unified_logs_structured_data ON unified_logs USING GIN (structured_data);",
            ]

            for index_sql in indexes:
                cur.execute(index_sql)

            # é‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨ãƒ“ãƒ¥ãƒ¼
            cur.execute("""
                CREATE OR REPLACE VIEW log_duplicates AS
                SELECT content_hash, COUNT(*) as duplicate_count,
                       MIN(id) as original_id, MAX(timestamp) as latest_timestamp
                FROM unified_logs
                WHERE content_hash IS NOT NULL
                GROUP BY content_hash
                HAVING COUNT(*) > 1;
            """)

            # ãƒ­ã‚°çµ±è¨ˆãƒ“ãƒ¥ãƒ¼
            cur.execute("""
                CREATE OR REPLACE VIEW log_statistics AS
                SELECT
                    project_name,
                    log_level,
                    component,
                    importance_level,
                    COUNT(*) as log_count,
                    MIN(timestamp) as earliest_log,
                    MAX(timestamp) as latest_log,
                    COUNT(DISTINCT source_file) as file_count
                FROM unified_logs
                GROUP BY project_name, log_level, component, importance_level
                ORDER BY log_count DESC;
            """)

            conn.commit()
            cur.close()
            conn.close()

            return {
                "status": "success",
                "message": "çµ±ä¸€ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†",
                "database": self.db_config["database"],
                "project_name": self.project_root.name,
            }

        except Exception as e:
            return {"status": "error", "error": str(e)}

    def batch_insert_log_entries(
        self, log_entries: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """ãƒ­ã‚°ã‚¨ãƒ³ãƒˆãƒªä¸€æ‹¬æŒ¿å…¥"""

        if not log_entries:
            return {"status": "no_data", "inserted_count": 0}

        try:
            conn = psycopg2.connect(**self.db_config)
            cur = conn.cursor()

            # é‡è¤‡ãƒã‚§ãƒƒã‚¯ãƒ»é™¤å»
            unique_entries = self._remove_duplicate_entries(cur, log_entries)

            if not unique_entries:
                cur.close()
                conn.close()
                return {"status": "all_duplicates", "inserted_count": 0}

            # ä¸€æ‹¬æŒ¿å…¥
            insert_sql = """
                INSERT INTO unified_logs
                (id, timestamp, source_file, line_number, log_level, component, message,
                 raw_content, importance_level, structured_data, project_name, file_size,
                 file_modified, extracted_at, content_hash)
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s);
            """

            insert_data = []
            for entry in unique_entries:
                insert_data.append(
                    (
                        entry["id"],
                        entry["timestamp"],
                        entry["source_file"],
                        entry["line_number"],
                        entry["log_level"],
                        entry["component"],
                        entry["message"],
                        entry["raw_content"],
                        entry["importance_level"],
                        json.dumps(entry["structured_data"])
                        if entry["structured_data"]
                        else None,
                        entry["project_name"],
                        entry["file_size"],
                        entry["file_modified"],
                        entry["extracted_at"],
                        entry["content_hash"],
                    )
                )

            cur.executemany(insert_sql, insert_data)

            conn.commit()
            cur.close()
            conn.close()

            self.processing_stats["logs_extracted"] += len(unique_entries)
            self.processing_stats["duplicates_removed"] += len(log_entries) - len(
                unique_entries
            )

            return {
                "status": "success",
                "inserted_count": len(unique_entries),
                "duplicates_removed": len(log_entries) - len(unique_entries),
                "total_processed": len(log_entries),
            }

        except Exception as e:
            self.processing_stats["errors_encountered"] += 1
            return {
                "status": "error",
                "error": str(e),
                "attempted_count": len(log_entries),
            }

    def _remove_duplicate_entries(
        self, cursor, log_entries: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """é‡è¤‡ãƒ­ã‚°ã‚¨ãƒ³ãƒˆãƒªé™¤å»"""

        if not log_entries:
            return []

        # æ—¢å­˜ãƒãƒƒã‚·ãƒ¥ç¢ºèª
        hashes = [
            entry["content_hash"] for entry in log_entries if entry.get("content_hash")
        ]

        if hashes:
            placeholders = ",".join(["%s"] * len(hashes))
            cursor.execute(
                f"""
                SELECT content_hash FROM unified_logs
                WHERE content_hash IN ({placeholders}) AND project_name = %s;
            """,
                hashes + [self.project_root.name],
            )

            existing_hashes = {row[0] for row in cursor.fetchall()}
        else:
            existing_hashes = set()

        # é‡è¤‡é™¤å»
        unique_entries = []
        seen_hashes = set()

        for entry in log_entries:
            content_hash = entry.get("content_hash")

            if content_hash and (
                content_hash in existing_hashes or content_hash in seen_hashes
            ):
                continue  # é‡è¤‡ã‚’ã‚¹ã‚­ãƒƒãƒ—

            unique_entries.append(entry)
            if content_hash:
                seen_hashes.add(content_hash)

        return unique_entries

    def process_all_log_files(self) -> Dict[str, Any]:
        """å…¨ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å®Ÿè¡Œ"""

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
        db_setup = self.setup_unified_log_database()
        if db_setup["status"] != "success":
            return {
                "status": "database_setup_failed",
                "error": db_setup.get("error", "Unknown database error"),
            }

        # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ç™ºè¦‹
        log_files = self.discover_all_log_files()

        if not log_files:
            return {
                "status": "no_log_files_found",
                "message": "å‡¦ç†å¯¾è±¡ã®ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ",
            }

        # ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†
        processed_files = []
        batch_entries = []

        for i, log_file in enumerate(log_files):
            try:
                if self.progress_reporting and (i + 1) % 10 == 0:
                    print(
                        f"   å‡¦ç†ä¸­: {i + 1}/{len(log_files)} ãƒ•ã‚¡ã‚¤ãƒ« ({(i + 1) / len(log_files) * 100:.1f}%)"
                    )

                # ãƒ­ã‚°ã‚¨ãƒ³ãƒˆãƒªæŠ½å‡º
                entries = self.extract_log_entries_from_file(log_file)

                if entries:
                    batch_entries.extend(entries)
                    processed_files.append(
                        {
                            "file_path": str(log_file.relative_to(self.project_root)),
                            "entries_count": len(entries),
                        }
                    )

                # ãƒãƒƒãƒã‚µã‚¤ã‚ºã«é”ã—ãŸã‚‰æŒ¿å…¥
                if len(batch_entries) >= self.batch_size:
                    self.batch_insert_log_entries(batch_entries)
                    batch_entries = []  # ãƒãƒƒãƒã‚¯ãƒªã‚¢

                self.processing_stats["files_processed"] += 1

            except Exception as e:
                if self.verbose_logging:
                    print(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼ {log_file}: {e}")
                self.processing_stats["files_skipped"] += 1
                self.processing_stats["errors_encountered"] += 1

        # æ®‹ã‚Šã®ãƒãƒƒãƒã‚’æŒ¿å…¥
        if batch_entries:
            self.batch_insert_log_entries(batch_entries)

        return {
            "status": "processing_completed",
            "project_name": self.project_root.name,
            "database": self.db_config["database"],
            "statistics": self.processing_stats,
            "processed_files": processed_files,
            "total_log_files": len(log_files),
        }

    def generate_log_integration_report(self) -> Dict[str, Any]:
        """ãƒ­ã‚°çµ±åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""

        try:
            conn = psycopg2.connect(**self.db_config, cursor_factory=RealDictCursor)
            cur = conn.cursor()

            # åŸºæœ¬çµ±è¨ˆ
            cur.execute(
                """
                SELECT
                    COUNT(*) as total_logs,
                    COUNT(DISTINCT source_file) as unique_files,
                    COUNT(DISTINCT component) as unique_components,
                    MIN(timestamp) as earliest_log,
                    MAX(timestamp) as latest_log
                FROM unified_logs
                WHERE project_name = %s;
            """,
                (self.project_root.name,),
            )

            basic_stats = cur.fetchone()

            # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«åˆ†å¸ƒ
            cur.execute(
                """
                SELECT log_level, COUNT(*) as count
                FROM unified_logs
                WHERE project_name = %s
                GROUP BY log_level
                ORDER BY count DESC;
            """,
                (self.project_root.name,),
            )

            level_distribution = dict(cur.fetchall())

            # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆ†å¸ƒ
            cur.execute(
                """
                SELECT component, COUNT(*) as count
                FROM unified_logs
                WHERE project_name = %s
                GROUP BY component
                ORDER BY count DESC
                LIMIT 10;
            """,
                (self.project_root.name,),
            )

            component_distribution = dict(cur.fetchall())

            # é‡è¦åº¦åˆ†å¸ƒ
            cur.execute(
                """
                SELECT importance_level, COUNT(*) as count
                FROM unified_logs
                WHERE project_name = %s
                GROUP BY importance_level
                ORDER BY count DESC;
            """,
                (self.project_root.name,),
            )

            importance_distribution = dict(cur.fetchall())

            # ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥çµ±è¨ˆ (ãƒˆãƒƒãƒ—10)
            cur.execute(
                """
                SELECT source_file, COUNT(*) as log_count,
                       COUNT(DISTINCT log_level) as level_variety,
                       MAX(timestamp) as latest_entry
                FROM unified_logs
                WHERE project_name = %s
                GROUP BY source_file
                ORDER BY log_count DESC
                LIMIT 10;
            """,
                (self.project_root.name,),
            )

            top_files = [dict(row) for row in cur.fetchall()]

            # é‡è¤‡çµ±è¨ˆ
            cur.execute("""
                SELECT COUNT(*) as duplicate_groups, SUM(duplicate_count) as total_duplicates
                FROM log_duplicates;
            """)

            duplicate_stats = cur.fetchone()

            cur.close()
            conn.close()

            return {
                "project_name": self.project_root.name,
                "database": self.db_config["database"],
                "report_generated_at": datetime.now().isoformat(),
                "basic_statistics": dict(basic_stats) if basic_stats else {},
                "processing_statistics": self.processing_stats,
                "distributions": {
                    "log_levels": level_distribution,
                    "components": component_distribution,
                    "importance_levels": importance_distribution,
                },
                "top_files": top_files,
                "duplicate_statistics": dict(duplicate_stats)
                if duplicate_stats
                else {},
                "integration_quality": {
                    "log_coverage": round(
                        (
                            self.processing_stats["files_processed"]
                            / max(self.processing_stats["files_discovered"], 1)
                        )
                        * 100,
                        1,
                    ),
                    "error_rate": round(
                        (
                            self.processing_stats["errors_encountered"]
                            / max(self.processing_stats["files_discovered"], 1)
                        )
                        * 100,
                        1,
                    ),
                    "duplicate_reduction": round(
                        (
                            self.processing_stats["duplicates_removed"]
                            / max(
                                self.processing_stats["logs_extracted"]
                                + self.processing_stats["duplicates_removed"],
                                1,
                            )
                        )
                        * 100,
                        1,
                    ),
                },
            }

        except Exception as e:
            return {"status": "report_generation_failed", "error": str(e)}


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ - çµ±ä¸€ãƒ­ã‚°çµ±åˆã‚·ã‚¹ãƒ†ãƒ """

    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°å¯¾å¿œ
    import sys

    project_root = None
    config_file = None

    if len(sys.argv) > 1:
        if sys.argv[1] == "--project":
            project_root = Path(sys.argv[2]) if len(sys.argv) > 2 else None
        elif sys.argv[1] == "--config":
            config_file = sys.argv[2] if len(sys.argv) > 2 else None
        elif sys.argv[1] == "--generate-config":
            # è¨­å®šãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç”Ÿæˆãƒ¢ãƒ¼ãƒ‰
            if len(sys.argv) > 2:
                project_root = Path(sys.argv[2])
            else:
                project_root = Path.cwd()

            # ãƒ­ã‚°è¨­å®šãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç”Ÿæˆ
            template_config = {
                "project_name": project_root.name,
                "database": {
                    "host": "localhost",
                    "database": f"{project_root.name}_ai",
                    "user": "dd",
                    "password": "",
                    "port": 5432,
                },
                "log_collection": {
                    "max_file_size_mb": 50,
                    "batch_size": 100,
                    "duplicate_threshold": 0.95,
                },
                "log_patterns": {
                    "file_patterns": [
                        "*.log",
                        "*.txt",
                        "*log*",
                        "*error*",
                        "*debug*",
                        "*trace*",
                    ],
                    "excluded": [
                        "*.tmp",
                        "*.cache",
                        "*.pyc",
                        "__pycache__/*",
                        "node_modules/*",
                        ".git/*",
                    ],
                },
                "paths": {
                    "log_directories": [
                        "logs",
                        "tmp",
                        "runtime",
                        "operations",
                        "memory",
                        "src",
                        "scripts",
                    ]
                },
                "ux": {"verbose_logging": True, "progress_reporting": True},
            }

            config_path = project_root / "log_config.json"
            with open(config_path, "w", encoding="utf-8") as f:
                json.dump(template_config, f, indent=2, ensure_ascii=False)

            print(f"âœ… ãƒ­ã‚°è¨­å®šãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {config_path}")
            print("   è¨­å®šã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã—ã¦ã‹ã‚‰ã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
            return

    print("ğŸ“Š çµ±ä¸€ãƒ­ã‚°çµ±åˆã‚·ã‚¹ãƒ†ãƒ  - å…¨117+ãƒ•ã‚¡ã‚¤ãƒ«å®Œå…¨çµ±åˆé–‹å§‹")

    try:
        log_system = UnifiedLogIntegrationSystem(
            project_root=project_root, config_file=config_file
        )
    except Exception as e:
        print(f"âŒ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        return

    print(f"ğŸ—ï¸ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: {log_system.project_root.name}")
    print(f"ğŸ’¾ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹: {log_system.db_config['database']}")
    print(f"ğŸ“‚ ç›£è¦–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ•°: {len(log_system.log_directories)}")

    # 1. å…¨ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å®Ÿè¡Œ
    print("\n1ï¸âƒ£ å…¨ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«çµ±åˆå‡¦ç†å®Ÿè¡Œ")
    processing_result = log_system.process_all_log_files()
    print(f"å‡¦ç†çµæœ: {processing_result['status']}")

    if processing_result["status"] == "processing_completed":
        stats = processing_result["statistics"]
        print(f"   ç™ºè¦‹ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {stats['files_discovered']}")
        print(f"   å‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {stats['files_processed']}")
        print(f"   ã‚¹ã‚­ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {stats['files_skipped']}")
        print(f"   æŠ½å‡ºãƒ­ã‚°æ•°: {stats['logs_extracted']}")
        print(f"   é‡è¤‡é™¤å»æ•°: {stats['duplicates_removed']}")
        print(f"   ã‚¨ãƒ©ãƒ¼æ•°: {stats['errors_encountered']}")
    else:
        print(f"   ã‚¨ãƒ©ãƒ¼: {processing_result.get('error', 'Unknown error')}")
        return

    # 2. çµ±åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    print("\n2ï¸âƒ£ ãƒ­ã‚°çµ±åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ")
    report = log_system.generate_log_integration_report()

    if report.get("basic_statistics"):
        basic = report["basic_statistics"]
        print(f"   ç·ãƒ­ã‚°æ•°: {basic.get('total_logs', 0)}")
        print(f"   ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {basic.get('unique_files', 0)}")
        print(f"   ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆæ•°: {basic.get('unique_components', 0)}")

        # æœ€ã‚‚å¤ã„ãƒ­ã‚°ã¨æœ€æ–°ãƒ­ã‚°
        if basic.get("earliest_log") and basic.get("latest_log"):
            print(f"   ãƒ­ã‚°æœŸé–“: {basic['earliest_log']} ï½ {basic['latest_log']}")

    # å“è³ªæŒ‡æ¨™
    if report.get("integration_quality"):
        quality = report["integration_quality"]
        print("\n   çµ±åˆå“è³ª:")
        print(f"     ãƒ­ã‚°ã‚«ãƒãƒ¬ãƒƒã‚¸: {quality['log_coverage']}%")
        print(f"     ã‚¨ãƒ©ãƒ¼ç‡: {quality['error_rate']}%")
        print(f"     é‡è¤‡å‰Šæ¸›ç‡: {quality['duplicate_reduction']}%")

    # åˆ†å¸ƒæƒ…å ±
    if report.get("distributions"):
        dist = report["distributions"]

        print("\n   ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«åˆ†å¸ƒ:")
        for level, count in list(dist["log_levels"].items())[:5]:
            print(f"     {level}: {count}")

        print("\n   ä¸»è¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ:")
        for component, count in list(dist["components"].items())[:5]:
            print(f"     {component}: {count}")

    # ãƒˆãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«
    if report.get("top_files"):
        print("\n   ãƒ­ã‚°æœ€å¤šãƒ•ã‚¡ã‚¤ãƒ«:")
        for file_info in report["top_files"][:5]:
            print(f"     {file_info['source_file']}: {file_info['log_count']}ãƒ­ã‚°")

    # 3. ä½¿ç”¨æ–¹æ³•æ¡ˆå†…
    print("\n3ï¸âƒ£ ä½¿ç”¨æ–¹æ³•")
    print(
        "   è¨­å®šç”Ÿæˆ: python unified_log_integration_system.py --generate-config [ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹]"
    )
    print(
        "   ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæŒ‡å®š: python unified_log_integration_system.py --project [ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹]"
    )
    print(
        "   è¨­å®šæŒ‡å®š: python unified_log_integration_system.py --config [è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹]"
    )

    print("\nğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¯ã‚¨ãƒªä¾‹:")
    print(
        "   SELECT * FROM unified_logs WHERE log_level = 'ERROR' ORDER BY timestamp DESC LIMIT 10;"
    )
    print(
        "   SELECT component, COUNT(*) FROM unified_logs GROUP BY component ORDER BY COUNT(*) DESC;"
    )
    print("   SELECT * FROM log_statistics WHERE project_name = 'your_project';")

    print("\nâœ… çµ±ä¸€ãƒ­ã‚°çµ±åˆã‚·ã‚¹ãƒ†ãƒ  - å…¨117+ãƒ•ã‚¡ã‚¤ãƒ«å®Œå…¨çµ±åˆå®Œäº†")
    print("ğŸ“ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“ãƒ­ã‚°çµ±åˆ + æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ + é‡è¤‡é™¤å» + å“è³ªåˆ†æ")


if __name__ == "__main__":
    main()
