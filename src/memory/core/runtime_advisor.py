#!/usr/bin/env python3
"""
Runtime Advisor - 78å›ãƒŸã‚¹å±¥æ­´ã‚’æ´»ç”¨ã—ãŸãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒŸã‚¹é˜²æ­¢ã‚·ã‚¹ãƒ†ãƒ 
o3ã®æŒ‡æ‘˜ã«åŸºã¥ã3å±¤æ§‹é€ å®Ÿè£…
"""

import json
import re
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict

PROJECT_ROOT = Path(__file__).resolve().parents[3]
MISTAKES_DB = PROJECT_ROOT / "src/memory/persistent-learning/mistakes-database.json"
VERIFICATION_LOG = PROJECT_ROOT / "runtime/ai_api_logs/runtime_advisor.log"


class RuntimeAdvisor:
    def __init__(self):
        self.mistakes_patterns = self.load_mistakes_database()
        self.language_rules = self.load_language_rules()
        self.verification_log = []

    def load_mistakes_database(self) -> Dict:
        """78å›ãƒŸã‚¹å±¥æ­´ã®èª­ã¿è¾¼ã¿"""
        if MISTAKES_DB.exists():
            try:
                with open(MISTAKES_DB, encoding="utf-8") as f:
                    data = json.load(f)
                    return data
            except Exception as e:
                print(f"âš ï¸ ãƒŸã‚¹ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}", file=sys.stderr)

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®78å›ãƒŸã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³
        return {
            "total_mistakes": 78,
            "critical_patterns": [
                {
                    "id": "mistake_001",
                    "type": "è™šå½å ±å‘Šè©æ¬º",
                    "pattern": r"(ç¨¼åƒä¸­|èµ·å‹•æ¸ˆã¿|å®Œäº†|æˆåŠŸ)",
                    "severity": "critical",
                    "prevention": "è¨¼æ‹ æ·»ä»˜å¿…é ˆ",
                    "trigger_action": "hard_stop",
                },
                {
                    "id": "mistake_002",
                    "type": "æ¨æ¸¬å›ç­”",
                    "pattern": r"(ãŠãã‚‰ã|ãŸã¶ã‚“|ã¨æ€ã‚ã‚Œ|ã®å¯èƒ½æ€§|ã§ã—ã‚‡ã†)",
                    "severity": "high",
                    "prevention": "5åˆ†æ¤œç´¢ãƒ«ãƒ¼ãƒ«å®Ÿè¡Œ",
                    "trigger_action": "soft_warning",
                },
                {
                    "id": "mistake_003",
                    "type": "ãƒ•ã‚¡ã‚¤ãƒ«æ•£ä¹±",
                    "pattern": r"(ãƒ«ãƒ¼ãƒˆã«|ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç›´ä¸‹|[^/]+\.md$|^[a-zA-Z0-9_-]+/[a-zA-Z0-9_.-]+\.[a-zA-Z0-9]+$)",
                    "severity": "medium",
                    "prevention": "é©åˆ‡ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªç¢ºèª",
                    "trigger_action": "directory_check",
                },
                {
                    "id": "mistake_004",
                    "type": "çµ¶å¯¾ãƒ‘ã‚¹ä½¿ç”¨",
                    "pattern": r"/Users/[^/]+/Desktop",
                    "severity": "medium",
                    "prevention": "ç›¸å¯¾ãƒ‘ã‚¹ä½¿ç”¨",
                    "trigger_action": "path_correction",
                },
                {
                    "id": "mistake_005",
                    "type": "ç¢ºèªå›é¿",
                    "pattern": r"(ãƒã‚§ãƒƒã‚¯æ¸ˆã¿|ç¢ºèªå®Œäº†|ç¢ºèªã§ãã¾ã—ãŸ|å•é¡Œãªã—)",
                    "severity": "high",
                    "prevention": "è¨¼æ‹ æç¤ºè¦æ±‚",
                    "trigger_action": "evidence_request",
                },
                {
                    "id": "mistake_008",
                    "type": "è¨€èªä½¿ç”¨é•å",
                    "pattern": r"(I will|Let me|I'll|I'm going to|I need to).*?(å‡¦ç†|å®Ÿè£…|ä¿®æ­£|å¯¾å¿œ)",
                    "severity": "critical",
                    "prevention": "è¨€èªä½¿ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³è‡ªå‹•æ¤œå‡ºãƒ»å¼·åˆ¶ä¿®æ­£",
                    "trigger_action": "language_enforcement",
                },
            ],
        }

    def load_language_rules(self) -> Dict:
        """è¨€èªä½¿ç”¨ãƒ«ãƒ¼ãƒ«ã®èª­ã¿è¾¼ã¿"""
        return {
            "processing": "english",
            "declaration": "japanese",
            "reporting": "japanese",
            "user_preferred_format": "japanese_declaration_english_process_japanese_report",
            "patterns": {
                "japanese_declaration": r"^##\s*ğŸ¯.*ã“ã‚Œã‹ã‚‰è¡Œã†ã“ã¨",
                "english_processing": r"<function_calls>|<invoke>|def\s+\w+|class\s+\w+",
                "japanese_reporting": r"^##\s*âœ….*å®Œé‚å ±å‘Š",
            },
        }

    def validate_language_usage(self, response_text: str, context_type: str) -> Dict:
        """è¨€èªä½¿ç”¨ãƒ«ãƒ¼ãƒ«ã®æ¤œè¨¼"""
        validation_result = {
            "timestamp": datetime.now().isoformat(),
            "context_type": context_type,
            "compliant": True,
            "violations": [],
            "required_language": self.language_rules.get(context_type, "japanese"),
            "recommendations": [],
        }

        # Pattern-based validation
        patterns = self.language_rules.get("patterns", {})

        if context_type == "declaration":
            if not re.search(
                patterns.get("japanese_declaration", ""), response_text, re.MULTILINE
            ):
                validation_result["compliant"] = False
                validation_result["violations"].append(
                    "Missing Japanese declaration pattern"
                )
                validation_result["recommendations"].append(
                    "Use '## ğŸ¯ ã“ã‚Œã‹ã‚‰è¡Œã†ã“ã¨' format"
                )

        elif context_type == "reporting":
            if not re.search(
                patterns.get("japanese_reporting", ""), response_text, re.MULTILINE
            ):
                validation_result["compliant"] = False
                validation_result["violations"].append(
                    "Missing Japanese reporting pattern"
                )
                validation_result["recommendations"].append(
                    "Use '## âœ… å®Œé‚å ±å‘Š' format"
                )

        return validation_result

    def enforce_language_compliance(self, response_text: str) -> Dict:
        """è¨€èªä½¿ç”¨ã®å¼·åˆ¶ãƒã‚§ãƒƒã‚¯"""
        enforcement_result = {
            "overall_compliant": True,
            "declaration_check": None,
            "processing_check": None,
            "reporting_check": None,
            "enforcement_actions": [],
        }

        # Check for declaration section
        if "ã“ã‚Œã‹ã‚‰è¡Œã†ã“ã¨" in response_text:
            enforcement_result["declaration_check"] = self.validate_language_usage(
                response_text, "declaration"
            )
            if not enforcement_result["declaration_check"]["compliant"]:
                enforcement_result["overall_compliant"] = False

        # Check for reporting section
        if "å®Œé‚å ±å‘Š" in response_text:
            enforcement_result["reporting_check"] = self.validate_language_usage(
                response_text, "reporting"
            )
            if not enforcement_result["reporting_check"]["compliant"]:
                enforcement_result["overall_compliant"] = False

        # Generate enforcement actions
        if not enforcement_result["overall_compliant"]:
            enforcement_result["enforcement_actions"].append(
                "Block response until language compliance"
            )
            enforcement_result["enforcement_actions"].append("Require template usage")

        return enforcement_result

    def analyze_input(self, user_input: str, context: str = "") -> Dict:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã®åˆ†æã¨ãƒŸã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º"""
        analysis_result = {
            "timestamp": datetime.now().isoformat(),
            "input": user_input[:200],  # æœ€åˆã®200æ–‡å­—ã®ã¿è¨˜éŒ²
            "context": context[:100],
            "detected_patterns": [],
            "risk_score": 0,
            "recommendations": [],
            "required_actions": [],
        }

        # å„ãƒŸã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨ã®ç…§åˆ
        for pattern in self.mistakes_patterns.get("critical_patterns", []):
            if re.search(pattern["pattern"], user_input, re.IGNORECASE):
                detection = {
                    "mistake_id": pattern["id"],
                    "mistake_type": pattern["type"],
                    "severity": pattern["severity"],
                    "prevention": pattern["prevention"],
                    "trigger_action": pattern["trigger_action"],
                }

                analysis_result["detected_patterns"].append(detection)

                # ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢è¨ˆç®—
                severity_scores = {"critical": 50, "high": 30, "medium": 15, "low": 5}
                analysis_result["risk_score"] += severity_scores.get(
                    pattern["severity"], 5
                )

                # æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç”Ÿæˆ
                analysis_result["recommendations"].append(pattern["prevention"])
                analysis_result["required_actions"].append(pattern["trigger_action"])

        return analysis_result

    def check_output_safety(self, assistant_output: str) -> Dict:
        """ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆå‡ºåŠ›ã®å®‰å…¨æ€§ãƒã‚§ãƒƒã‚¯"""
        safety_result = {
            "timestamp": datetime.now().isoformat(),
            "output_snippet": assistant_output[:100],
            "safety_violations": [],
            "safety_score": 100,  # 100ãŒæœ€é«˜ç‚¹
            "requires_evidence": False,
            "blocked_phrases": [],
        }

        # å±é™ºãƒ•ãƒ¬ãƒ¼ã‚ºã®æ¤œå‡º
        dangerous_phrases = [
            r"ç¨¼åƒä¸­",
            r"å®Œäº†",
            r"æˆåŠŸ",
            r"ç¢ºèªã§ãã¾ã—ãŸ",
            r"å•é¡Œã‚ã‚Šã¾ã›ã‚“",
            r"æ­£å¸¸",
            r"å…¨ã¦å®Œæˆ",
            r"å®Ÿè£…æ¸ˆã¿",
            r"å¯¾å¿œæ¸ˆã¿",
            r"å®Ÿè£…æ¸ˆã¿ã§ã™",
            r"å¯¾å¿œæ¸ˆã¿ã§ã™",
        ]

        for phrase_pattern in dangerous_phrases:
            matches = re.findall(phrase_pattern, assistant_output, re.IGNORECASE)
            if matches:
                safety_result["safety_violations"].append(
                    {"phrase": phrase_pattern, "matches": matches, "severity": "high"}
                )
                safety_result["safety_score"] -= 20
                safety_result["requires_evidence"] = True
                safety_result["blocked_phrases"].extend(matches)

        # æ¨æ¸¬è¡¨ç¾ã®æ¤œå‡º
        speculation_phrases = [
            r"ãŠãã‚‰ã",
            r"ãŸã¶ã‚“",
            r"ã¨æ€ã‚ã‚Œ",
            r"ã‹ã‚‚ã—ã‚Œ",
            r"ã§ã—ã‚‡ã†",
        ]

        for spec_pattern in speculation_phrases:
            matches = re.findall(spec_pattern, assistant_output, re.IGNORECASE)
            if matches:
                safety_result["safety_violations"].append(
                    {"phrase": spec_pattern, "matches": matches, "severity": "medium"}
                )
                safety_result["safety_score"] -= 10

        return safety_result

    def generate_prevention_instructions(self, analysis: Dict) -> str:
        """é˜²æ­¢æŒ‡ç¤ºã®ç”Ÿæˆ"""
        if not analysis["detected_patterns"]:
            return "âœ… ãƒŸã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºãªã— - å®‰å…¨ã«ä½œæ¥­ã‚’ç¶™ç¶šã—ã¦ãã ã•ã„ã€‚"

        instructions = ["ğŸš¨ ãƒŸã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º - ä»¥ä¸‹ã®å¯¾ç­–ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼š\n"]

        for i, pattern in enumerate(analysis["detected_patterns"], 1):
            instructions.append(
                f"{i}. **{pattern['mistake_type']}** (é‡è¦åº¦: {pattern['severity']})"
            )
            instructions.append(f"   - é˜²æ­¢ç­–: {pattern['prevention']}")
            instructions.append(f"   - å¿…è¦ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: {pattern['trigger_action']}")
            instructions.append("")

        # ç·åˆè©•ä¾¡
        risk_score = analysis["risk_score"]
        if risk_score >= 50:
            instructions.append(
                "ğŸ”´ **é«˜ãƒªã‚¹ã‚¯**: ä½œæ¥­ã‚’ä¸€æ™‚åœæ­¢ã—ã€è¨¼æ‹ ç¢ºèªã‚’å¿…é ˆã¨ã—ã¾ã™ã€‚"
            )
        elif risk_score >= 30:
            instructions.append(
                "ğŸŸ¡ **ä¸­ãƒªã‚¹ã‚¯**: æ…é‡ã«ä½œæ¥­ã‚’é€²ã‚ã€ç¢ºèªã‚’å¾¹åº•ã—ã¦ãã ã•ã„ã€‚"
            )
        else:
            instructions.append("ğŸŸ¢ **ä½ãƒªã‚¹ã‚¯**: é€šå¸¸é€šã‚Šä½œæ¥­ã‚’ç¶™ç¶šã—ã¦ãã ã•ã„ã€‚")

        return "\n".join(instructions)

    def log_verification(self, result: Dict):
        """æ¤œè¨¼çµæœã®ãƒ­ã‚°è¨˜éŒ²"""
        try:
            VERIFICATION_LOG.parent.mkdir(parents=True, exist_ok=True)

            with open(VERIFICATION_LOG, "a", encoding="utf-8") as f:
                f.write(json.dumps(result, ensure_ascii=False) + "\n")

        except Exception as e:
            print(f"âš ï¸ ãƒ­ã‚°è¨˜éŒ²å¤±æ•—: {e}", file=sys.stderr)

    def get_prevention_summary(self) -> Dict:
        """é˜²æ­¢åŠ¹æœã®ã‚µãƒãƒªãƒ¼å–å¾—"""
        if not VERIFICATION_LOG.exists():
            return {"total_checks": 0, "prevented_mistakes": 0, "prevention_rate": 0}

        try:
            total_checks = 0
            prevented_mistakes = 0

            with open(VERIFICATION_LOG, encoding="utf-8") as f:
                for line in f:
                    if line.strip():
                        result = json.loads(line)
                        total_checks += 1
                        if result.get("detected_patterns"):
                            prevented_mistakes += 1

            prevention_rate = (
                (prevented_mistakes / total_checks * 100) if total_checks > 0 else 0
            )

            return {
                "total_checks": total_checks,
                "prevented_mistakes": prevented_mistakes,
                "prevention_rate": round(prevention_rate, 2),
            }

        except Exception as e:
            print(f"âš ï¸ ã‚µãƒãƒªãƒ¼å–å¾—ã‚¨ãƒ©ãƒ¼: {e}", file=sys.stderr)
            return {"total_checks": 0, "prevented_mistakes": 0, "prevention_rate": 0}


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç† - CLIä½¿ç”¨ä¾‹"""
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ³•: python3 runtime-advisor.py 'ãƒã‚§ãƒƒã‚¯å¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ'")
        return

    advisor = RuntimeAdvisor()
    input_text = sys.argv[1]

    # åˆ†æå®Ÿè¡Œ
    analysis = advisor.analyze_input(input_text)

    # çµæœè¡¨ç¤º
    print("ğŸ” Runtime Advisor åˆ†æçµæœ")
    print("=" * 40)
    print(f"ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢: {analysis['risk_score']}")
    print(f"æ¤œå‡ºãƒ‘ã‚¿ãƒ¼ãƒ³æ•°: {len(analysis['detected_patterns'])}")

    # é˜²æ­¢æŒ‡ç¤ºç”Ÿæˆ
    instructions = advisor.generate_prevention_instructions(analysis)
    print("\n" + instructions)

    # ãƒ­ã‚°è¨˜éŒ²
    advisor.log_verification(analysis)

    # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
    summary = advisor.get_prevention_summary()
    print(
        f"\nğŸ“Š é˜²æ­¢åŠ¹æœ: {summary['prevention_rate']}% ({summary['prevented_mistakes']}/{summary['total_checks']})"
    )


if __name__ == "__main__":
    main()
