#!/usr/bin/env python3
"""
ğŸ“Š æ§‹é€ åŒ–ãƒ­ã‚°çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ  - 117å€‹.logâ†’JSON Linesçµ±åˆ
========================================================

ã€ç›®çš„ã€‘
- éæ§‹é€ åŒ–ãƒ­ã‚°ã‚’æ§‹é€ åŒ–JSON Linesã«çµ±ä¸€
- PIIä¿è­·ã¨ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¼·åŒ–
- æ¤œç´¢ãƒ»åˆ†æå¯èƒ½ãªãƒ­ã‚°å½¢å¼ã¸ã®å¤‰æ›
- OpenTelemetryæº–æ‹ ã®åˆ†æ•£ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°å¯¾å¿œ

ã€æ”¹å–„åŠ¹æœã€‘
- ç¾åœ¨: 117å€‹.log + 17å€‹.json (éæ§‹é€ åŒ–æ•£åœ¨)
- ç›®æ¨™: çµ±ä¸€JSON Lines + ãƒã‚¹ã‚­ãƒ³ã‚° + ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³

ã€æŠ€è¡“ä»•æ§˜ã€‘
- JSON Lines (.jsonl) å½¢å¼
- PIIè‡ªå‹•ãƒã‚¹ã‚­ãƒ³ã‚° (API keys, emails, paths)
- ãƒ­ã‚°ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ (æ—¥æ¬¡/ã‚µã‚¤ã‚ºãƒ™ãƒ¼ã‚¹)
- åˆ†æ•£ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°å¯¾å¿œ
"""

import hashlib
import json
import os
import re
import uuid
from dataclasses import asdict, dataclass
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, List, Tuple


@dataclass
class LogEntry:
    """çµ±ä¸€ãƒ­ã‚°ã‚¨ãƒ³ãƒˆãƒªæ§‹é€ """

    timestamp: str
    level: str
    session_id: str
    agent_id: str
    trace_id: str
    span_id: str
    event: str
    message: str
    metadata: Dict[str, Any]
    masked_data: Dict[str, str] = None
    source_file: str = ""

    def to_json(self) -> str:
        """JSON Lineså½¢å¼ã§å‡ºåŠ›"""
        return json.dumps(asdict(self), ensure_ascii=False, separators=(",", ":"))


class PIIMasker:
    """PIIæƒ…å ±è‡ªå‹•ãƒã‚¹ã‚­ãƒ³ã‚°"""

    def __init__(self):
        self.patterns = {
            "api_key": re.compile(r"(sk-[a-zA-Z0-9]{48}|AIza[a-zA-Z0-9_-]{35})"),
            "email": re.compile(r"\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b"),
            "absolute_path": re.compile(r"/Users/[^/]+/[^\s]+"),
            "session_id": re.compile(r"sess_[a-zA-Z0-9]{10,}"),
            "uuid": re.compile(
                r"[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}"
            ),
            "token": re.compile(r"token[\'\":][\s]*[\'\"]([\w\-\.]+)[\'\"]\s"),
            "password": re.compile(
                r"password[\'\":][\s]*[\'\"]([\w\-\.]+)[\'\"]\s", re.IGNORECASE
            ),
        }

    def mask_content(self, content: str) -> Tuple[str, Dict[str, str]]:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰PIIã‚’ãƒã‚¹ã‚¯ã—ã€ãƒã‚¹ã‚¯æƒ…å ±ã‚’è¿”ã™"""
        masked_content = content
        masked_data = {}

        for pii_type, pattern in self.patterns.items():
            matches = pattern.findall(content)
            for match in matches:
                if isinstance(match, tuple):
                    match = match[0]  # æ­£è¦è¡¨ç¾ã‚°ãƒ«ãƒ¼ãƒ—ã®å ´åˆ

                # ãƒã‚¹ã‚¯ç”¨IDç”Ÿæˆ
                mask_id = f"{pii_type}_{hashlib.md5(match.encode()).hexdigest()[:8]}"
                mask_token = f"***{mask_id}***"

                # ãƒã‚¹ã‚¯é©ç”¨
                masked_content = masked_content.replace(match, mask_token)
                masked_data[mask_id] = f"{pii_type.upper()}_MASKED"

        return masked_content, masked_data


class UnifiedLogSystem:
    """çµ±ä¸€ãƒ­ã‚°ã‚·ã‚¹ãƒ†ãƒ  - å…¨ãƒ­ã‚°ã®æ§‹é€ åŒ–ç®¡ç†"""

    def __init__(self, output_dir: Path):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.pii_masker = PIIMasker()
        self.session_id = str(uuid.uuid4())[:8]

        # çµ±ä¸€ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«
        self.unified_log_file = (
            self.output_dir / f"unified_{datetime.now().strftime('%Y%m%d')}.jsonl"
        )

    def discover_log_files(self, project_root: Path) -> List[Path]:
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†…ã®å…¨ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç™ºè¦‹"""
        log_patterns = ["*.log", "*.json", "*_log_*.json", "*conversation*.json"]
        found_files = []

        for pattern in log_patterns:
            found_files.extend(project_root.rglob(pattern))

        # é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³
        exclude_patterns = [
            "node_modules",
            ".git",
            "__pycache__",
            "package-lock.json",
            "package.json",
        ]

        filtered_files = []
        for file_path in found_files:
            if not any(exclude in str(file_path) for exclude in exclude_patterns):
                filtered_files.append(file_path)

        return filtered_files

    def parse_legacy_log(self, file_path: Path) -> List[LogEntry]:
        """ãƒ¬ã‚¬ã‚·ãƒ¼ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æã—ã¦LogEntryã«å¤‰æ›"""
        entries = []

        try:
            with open(file_path, encoding="utf-8") as f:
                content = f.read()

            # ãƒ•ã‚¡ã‚¤ãƒ«ç¨®åˆ¥åˆ¤å®š
            if file_path.suffix == ".json":
                entries.extend(self._parse_json_log(file_path, content))
            else:
                entries.extend(self._parse_text_log(file_path, content))

        except Exception as e:
            # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‚‚è¨˜éŒ²
            error_entry = LogEntry(
                timestamp=datetime.now(timezone.utc).isoformat(),
                level="ERROR",
                session_id=self.session_id,
                agent_id="log_parser",
                trace_id=str(uuid.uuid4()),
                span_id=str(uuid.uuid4())[:8],
                event="log_parse_error",
                message=f"Failed to parse {file_path}: {str(e)}",
                metadata={"file_path": str(file_path), "error": str(e)},
                source_file=str(file_path),
            )
            entries.append(error_entry)

        return entries

    def _parse_json_log(self, file_path: Path, content: str) -> List[LogEntry]:
        """JSONãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®è§£æ"""
        entries = []

        try:
            # JSONé…åˆ—ã®å ´åˆ
            data = json.loads(content)
            if isinstance(data, list):
                for item in data:
                    entries.append(self._json_to_log_entry(file_path, item))
            else:
                entries.append(self._json_to_log_entry(file_path, data))

        except json.JSONDecodeError:
            # JSON Lineså½¢å¼ã®å¯èƒ½æ€§
            for line_num, line in enumerate(content.split("\n")):
                if line.strip():
                    try:
                        data = json.loads(line)
                        entries.append(self._json_to_log_entry(file_path, data))
                    except json.JSONDecodeError:
                        # ä¸æ­£ãªJSONã¯è­¦å‘Šã¨ã—ã¦è¨˜éŒ²
                        warning_entry = LogEntry(
                            timestamp=datetime.now(timezone.utc).isoformat(),
                            level="WARN",
                            session_id=self.session_id,
                            agent_id="log_parser",
                            trace_id=str(uuid.uuid4()),
                            span_id=str(uuid.uuid4())[:8],
                            event="invalid_json_line",
                            message=f"Invalid JSON at line {line_num + 1}",
                            metadata={"file_path": str(file_path), "line": line[:100]},
                            source_file=str(file_path),
                        )
                        entries.append(warning_entry)

        return entries

    def _parse_text_log(self, file_path: Path, content: str) -> List[LogEntry]:
        """ãƒ†ã‚­ã‚¹ãƒˆãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®è§£æ"""
        entries = []

        # ä¸€èˆ¬çš„ãªãƒ­ã‚°ãƒ‘ã‚¿ãƒ¼ãƒ³
        log_patterns = [
            # [YYYY-MM-DD HH:MM:SS] LEVEL: message
            re.compile(r"^\[(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})\]\s+(\w+):\s*(.+)$"),
            # YYYY-MM-DD HH:MM:SS LEVEL message
            re.compile(r"^(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})\s+(\w+)\s+(.+)$"),
            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãªã—ï¼ˆæ—¥ä»˜æ¨å®šï¼‰
            re.compile(r"^(\w+):\s*(.+)$"),
        ]

        for line_num, line in enumerate(content.split("\n")):
            if not line.strip():
                continue

            parsed = False
            for pattern in log_patterns:
                match = pattern.match(line.strip())
                if match:
                    if len(match.groups()) == 3:
                        timestamp_str, level, message = match.groups()
                        try:
                            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æ­£è¦åŒ–
                            dt = datetime.strptime(timestamp_str, "%Y-%m-%d %H:%M:%S")
                            timestamp = dt.replace(tzinfo=timezone.utc).isoformat()
                        except ValueError:
                            timestamp = datetime.now(timezone.utc).isoformat()
                    else:
                        level, message = match.groups()
                        timestamp = datetime.now(timezone.utc).isoformat()

                    # PIIãƒã‚¹ã‚­ãƒ³ã‚°
                    masked_message, masked_data = self.pii_masker.mask_content(message)

                    entry = LogEntry(
                        timestamp=timestamp,
                        level=level.upper(),
                        session_id=self.session_id,
                        agent_id="legacy_import",
                        trace_id=str(uuid.uuid4()),
                        span_id=str(uuid.uuid4())[:8],
                        event="legacy_log_import",
                        message=masked_message,
                        metadata={"original_line": line_num + 1},
                        masked_data=masked_data if masked_data else None,
                        source_file=str(file_path),
                    )
                    entries.append(entry)
                    parsed = True
                    break

            if not parsed:
                # ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ãƒãƒƒãƒã—ãªã„è¡Œã¯æƒ…å ±ã¨ã—ã¦è¨˜éŒ²
                masked_line, masked_data = self.pii_masker.mask_content(line)
                entry = LogEntry(
                    timestamp=datetime.now(timezone.utc).isoformat(),
                    level="INFO",
                    session_id=self.session_id,
                    agent_id="legacy_import",
                    trace_id=str(uuid.uuid4()),
                    span_id=str(uuid.uuid4())[:8],
                    event="unstructured_log",
                    message=masked_line,
                    metadata={"line_number": line_num + 1, "unparsed": True},
                    masked_data=masked_data if masked_data else None,
                    source_file=str(file_path),
                )
                entries.append(entry)

        return entries

    def _json_to_log_entry(self, file_path: Path, data: Dict[str, Any]) -> LogEntry:
        """JSONè¾æ›¸ã‚’LogEntryã«å¤‰æ›"""
        # å¿…è¦ãªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æŠ½å‡ºãƒ»ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤è¨­å®š
        timestamp = data.get("timestamp", datetime.now(timezone.utc).isoformat())
        level = data.get("level", "INFO")
        message = str(data.get("message", data.get("human_message", "")))

        # PIIãƒã‚¹ã‚­ãƒ³ã‚°
        masked_message, masked_data = self.pii_masker.mask_content(message)

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ§‹ç¯‰
        metadata = {
            k: v for k, v in data.items() if k not in ["timestamp", "level", "message"]
        }

        return LogEntry(
            timestamp=timestamp,
            level=level.upper(),
            session_id=data.get("session_id", self.session_id),
            agent_id=data.get("agent_id", "unknown"),
            trace_id=data.get("trace_id", str(uuid.uuid4())),
            span_id=data.get("span_id", str(uuid.uuid4())[:8]),
            event=data.get("event", "json_import"),
            message=masked_message,
            metadata=metadata,
            masked_data=masked_data if masked_data else None,
            source_file=str(file_path),
        )

    def write_unified_log(self, entries: List[LogEntry]):
        """çµ±ä¸€ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿"""
        with open(self.unified_log_file, "a", encoding="utf-8") as f:
            for entry in entries:
                f.write(entry.to_json() + "\n")

    def migrate_all_logs(self, project_root: Path) -> Dict[str, Any]:
        """å…¨ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµ±ä¸€å½¢å¼ã«ç§»è¡Œ"""
        log_files = self.discover_log_files(project_root)

        migration_stats = {
            "start_time": datetime.now(timezone.utc).isoformat(),
            "total_files": len(log_files),
            "processed_files": 0,
            "total_entries": 0,
            "errors": [],
        }

        for file_path in log_files:
            try:
                entries = self.parse_legacy_log(file_path)
                self.write_unified_log(entries)

                migration_stats["processed_files"] += 1
                migration_stats["total_entries"] += len(entries)

                print(f"âœ… {file_path.name}: {len(entries)}ã‚¨ãƒ³ãƒˆãƒª")

            except Exception as e:
                error_msg = f"Failed to process {file_path}: {str(e)}"
                migration_stats["errors"].append(error_msg)
                print(f"âŒ {error_msg}")

        migration_stats["end_time"] = datetime.now(timezone.utc).isoformat()

        # ç§»è¡Œãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
        report_path = (
            self.output_dir
            / f"migration_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        )
        with open(report_path, "w", encoding="utf-8") as f:
            json.dump(migration_stats, f, ensure_ascii=False, indent=2)

        return migration_stats

    def setup_log_rotation(self):
        """ãƒ­ã‚°ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®š"""
        rotation_script = f"""#!/bin/bash
# çµ±ä¸€ãƒ­ã‚°ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
LOG_DIR="{self.output_dir}"
MAX_SIZE_MB=100
MAX_DAYS=30

# ã‚µã‚¤ã‚ºãƒ™ãƒ¼ã‚¹ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
find "$LOG_DIR" -name "*.jsonl" -size +${{MAX_SIZE_MB}}M -exec gzip {{}} \\;

# å¤ã„ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
find "$LOG_DIR" -name "*.jsonl.gz" -mtime +$MAX_DAYS -delete

# æ—¥æ¬¡ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
DATE=$(date '+%Y%m%d')
if [ ! -f "$LOG_DIR/unified_$DATE.jsonl" ]; then
    touch "$LOG_DIR/unified_$DATE.jsonl"
fi
"""

        rotation_script_path = self.output_dir / "rotate_logs.sh"
        with open(rotation_script_path, "w") as f:
            f.write(rotation_script)

        os.chmod(rotation_script_path, 0o750)

        return rotation_script_path


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ - ãƒ­ã‚°çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆ"""
    project_root = Path(__file__).parent.parent.parent.parent
    output_dir = project_root / "logs/unified"

    # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    log_system = UnifiedLogSystem(output_dir)

    # å…¨ãƒ­ã‚°ç§»è¡Œå®Ÿè¡Œ
    print("ğŸ”„ ãƒ­ã‚°çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹...")
    migration_stats = log_system.migrate_all_logs(project_root)

    print("\nğŸ“Š ç§»è¡Œçµæœ:")
    print(
        f"  - å‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {migration_stats['processed_files']}/{migration_stats['total_files']}"
    )
    print(f"  - çµ±ä¸€ã‚¨ãƒ³ãƒˆãƒªæ•°: {migration_stats['total_entries']}")
    print(f"  - ã‚¨ãƒ©ãƒ¼æ•°: {len(migration_stats['errors'])}")

    # ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®š
    rotation_script = log_system.setup_log_rotation()
    print(f"  - ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®š: {rotation_script}")

    print(f"\nâœ… çµ±ä¸€ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: {log_system.unified_log_file}")


if __name__ == "__main__":
    main()
