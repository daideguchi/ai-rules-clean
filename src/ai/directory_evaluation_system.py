#!/usr/bin/env python3
"""
ğŸ” Directory Evaluation System
==============================
è‡ªå¾‹æˆé•·å‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ 
ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãªã„ã‚ˆã†å …ç‰¢ãªè¨­è¨ˆ
"""

from datetime import datetime
from pathlib import Path
from typing import Dict


class DirectoryEvaluationSystem:
    def __init__(self, project_root: str = "/Users/dd/Desktop/1_dev/coding-rule2"):
        self.project_root = Path(project_root)
        self.evaluation_results = {}

    def analyze_directory_structure(self) -> Dict:
        """ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã®è©³ç´°åˆ†æ"""

        analysis = {
            "timestamp": datetime.now().isoformat(),
            "project_root": str(self.project_root),
            "structure_analysis": {},
            "file_distribution": {},
            "quality_metrics": {},
            "recommendations": [],
        }

        # ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåˆ†æ
        root_files = list(self.project_root.glob("*"))
        root_file_count = len([f for f in root_files if f.is_file()])

        analysis["structure_analysis"]["root_files"] = root_file_count
        analysis["structure_analysis"]["root_limit_exceeded"] = root_file_count > 12

        # ãƒ•ã‚¡ã‚¤ãƒ«åˆ†å¸ƒåˆ†æ
        file_types = {}
        for file_path in self.project_root.rglob("*"):
            if file_path.is_file():
                suffix = file_path.suffix.lower()
                file_types[suffix] = file_types.get(suffix, 0) + 1

        analysis["file_distribution"] = file_types

        # å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹
        total_files = sum(file_types.values())
        analysis["quality_metrics"] = {
            "total_files": total_files,
            "python_files": file_types.get(".py", 0),
            "config_files": file_types.get(".json", 0) + file_types.get(".yaml", 0),
            "documentation_files": file_types.get(".md", 0),
            "log_files": file_types.get(".log", 0),
        }

        # æ¨å¥¨äº‹é …
        recommendations = []

        if root_file_count > 12:
            recommendations.append(
                f"ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’{root_file_count}ã‹ã‚‰12ä»¥ä¸‹ã«å‰Šæ¸›"
            )

        if file_types.get(".md", 0) > 50:
            recommendations.append("Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’docs/ä»¥ä¸‹ã«æ•´ç†")

        if file_types.get(".json", 0) > 20:
            recommendations.append("è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’config/ä»¥ä¸‹ã«çµ±åˆ")

        if file_types.get(".log", 0) > 10:
            recommendations.append("ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å®šæœŸçš„ã«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—")

        analysis["recommendations"] = recommendations

        return analysis

    def generate_evaluation_report(self) -> str:
        """è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""

        analysis = self.analyze_directory_structure()

        report = f"""
# ğŸ” ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ

## åˆ†æå®Ÿè¡Œæ™‚åˆ»
{analysis["timestamp"]}

## æ§‹é€ åˆ†æçµæœ

### ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªçŠ¶æ³
- ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {analysis["structure_analysis"]["root_files"]}
- åˆ¶é™è¶…é: {"âŒ ã¯ã„" if analysis["structure_analysis"]["root_limit_exceeded"] else "âœ… ã„ã„ãˆ"}

### ãƒ•ã‚¡ã‚¤ãƒ«åˆ†å¸ƒ
"""

        for file_type, count in analysis["file_distribution"].items():
            if count > 0:
                report += f"- {file_type}: {count}å€‹\n"

        report += f"""
### å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹
- ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {analysis["quality_metrics"]["total_files"]}
- Pythonãƒ•ã‚¡ã‚¤ãƒ«: {analysis["quality_metrics"]["python_files"]}
- è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«: {analysis["quality_metrics"]["config_files"]}
- ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: {analysis["quality_metrics"]["documentation_files"]}
- ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: {analysis["quality_metrics"]["log_files"]}

### æ¨å¥¨æ”¹å–„äº‹é …
"""

        for i, recommendation in enumerate(analysis["recommendations"], 1):
            report += f"{i}. {recommendation}\n"

        # ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—
        score = 100
        if analysis["structure_analysis"]["root_limit_exceeded"]:
            score -= 20
        if analysis["quality_metrics"]["documentation_files"] > 50:
            score -= 15
        if analysis["quality_metrics"]["config_files"] > 20:
            score -= 10
        if analysis["quality_metrics"]["log_files"] > 10:
            score -= 5

        report += f"""
## ç·åˆè©•ä¾¡
**ã‚¹ã‚³ã‚¢: {score}/100**

### è©•ä¾¡åŸºæº–
- 90-100: å„ªç§€
- 70-89: è‰¯å¥½
- 50-69: è¦æ”¹å–„
- 50æœªæº€: è¦å¤§å¹…æ”¹å–„

### çµè«–
{"âœ… è‰¯å¥½ãªæ§‹é€ ã§ã™" if score >= 70 else "âš ï¸ æ”¹å–„ãŒå¿…è¦ã§ã™" if score >= 50 else "âŒ å¤§å¹…ãªæ”¹å–„ãŒå¿…è¦ã§ã™"}
"""

        return report

    def save_evaluation(self, output_path: str = None):
        """è©•ä¾¡çµæœä¿å­˜"""

        if not output_path:
            output_path = (
                self.project_root / "docs" / "analysis" / "auto_directory_evaluation.md"
            )

        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        output_path.parent.mkdir(parents=True, exist_ok=True)

        report = self.generate_evaluation_report()

        with open(output_path, "w", encoding="utf-8") as f:
            f.write(report)

        print(f"âœ… è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {output_path}")
        return output_path


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""

    print("ğŸ” ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹")
    print("=" * 50)

    evaluator = DirectoryEvaluationSystem()

    # åˆ†æå®Ÿè¡Œ
    evaluator.analyze_directory_structure()

    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆãƒ»ä¿å­˜
    report_path = evaluator.save_evaluation()

    # çµæœè¡¨ç¤º
    report = evaluator.generate_evaluation_report()
    print(report)

    print(f"\nğŸ“Š è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ: {report_path}")


if __name__ == "__main__":
    main()
