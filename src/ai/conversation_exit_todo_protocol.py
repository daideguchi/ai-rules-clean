#!/usr/bin/env python3
"""
ğŸ¯ Conversation-Exit TODO Protocol - ä¼šè©±çµ‚äº†æ™‚TODOæ˜ç¤ºãƒ—ãƒ­ãƒˆã‚³ãƒ«
================================================================
ä¼šè©±çµ‚äº†æ™‚ã«æ˜ç¢ºãªTODOã‚’æŠ½å‡ºãƒ»æ•´ç†ãƒ»æç¤ºã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ 
ç¶™ç¶šæ€§ç¢ºä¿ã¨{{mistake_count}}å›ãƒŸã‚¹é˜²æ­¢ã®ãŸã‚ã®é‡è¦ãªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
"""

import json
import re
from dataclasses import asdict, dataclass, field
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Optional


class TodoPriority(Enum):
    URGENT = "urgent"
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"


class TodoStatus(Enum):
    PENDING = "pending"
    IN_PROGRESS = "in_progress"
    COMPLETED = "completed"
    BLOCKED = "blocked"


@dataclass
class ExtractedTodo:
    """æŠ½å‡ºã•ã‚ŒãŸTODOé …ç›®"""

    id: str
    content: str
    priority: TodoPriority
    status: TodoStatus
    context: str
    source: str  # conversation, instruction, etc.
    estimated_effort: str  # minutes, hours, days
    dependencies: List[str] = field(default_factory=list)
    technical_requirements: List[str] = field(default_factory=list)
    extracted_at: str = field(default_factory=lambda: datetime.now().isoformat())
    confidence_score: float = 0.8


@dataclass
class ConversationContext:
    """ä¼šè©±ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ"""

    conversation_id: str
    start_time: str
    end_time: Optional[str]
    total_messages: int
    key_topics: List[str]
    technical_domains: List[str]
    completion_status: str
    next_session_preparation: Dict[str, Any] = field(default_factory=dict)


class ConversationExitTodoProtocol:
    """ä¼šè©±çµ‚äº†æ™‚TODOæ˜ç¤ºãƒ—ãƒ­ãƒˆã‚³ãƒ«"""

    def __init__(self, project_root: str = "/Users/dd/Desktop/1_dev/coding-rule2"):
        self.project_root = Path(project_root)
        self.todo_archive_dir = self.project_root / "runtime" / "todo_archive"
        self.active_todos_file = self.project_root / "runtime" / "active_todos.json"
        self.conversation_history_dir = (
            self.project_root / "runtime" / "conversation_logs"
        )

        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        self.todo_archive_dir.mkdir(parents=True, exist_ok=True)
        self.conversation_history_dir.mkdir(parents=True, exist_ok=True)

        # TODOæŠ½å‡ºãƒ‘ã‚¿ãƒ¼ãƒ³
        self.todo_patterns = [
            r"(æ¬¡ã«|ä»Šåº¦|å¾Œã§|å¾Œã‹ã‚‰|å°†æ¥|æ¬¡å›)\s*(.{1,100}?)(?:[ã€‚\n]|$)",
            r"(TODO|ToDo|todo|ã‚„ã‚‹ã“ã¨|ã‚¿ã‚¹ã‚¯|èª²é¡Œ)[:ï¼š]?\s*(.{1,100}?)(?:[ã€‚\n]|$)",
            r"(å®Ÿè£…|ä¿®æ­£|è¿½åŠ |æ”¹å–„|å¯¾å¿œ|æ¤œè¨|èª¿æŸ»|ç¢ºèª)\s*(ã™ã‚‹|ã™ã¹ã|ãŒå¿…è¦|ã‚’è¡Œã†)\s*(.{1,100}?)(?:[ã€‚\n]|$)",
            r"(æ®‹ã£ã¦|æœªå®Œäº†|æœªå®Ÿè£…|æœªå¯¾å¿œ|æœªè§£æ±º)\s*(.{1,100}?)(?:[ã€‚\n]|$)",
            r"(ç¶™ç¶š|å¼•ãç¶šã|ç¶šã„ã¦|ç¶šã‘ã¦)\s*(.{1,100}?)(?:[ã€‚\n]|$)",
            r"ã¾ã \s*(.{1,100}?)(?:[ã€‚\n]|$)",
            r"(å¾Œã§|ã‚ã¨ã§|ä»Šåº¦|æ¬¡å›)\s*(.{1,100}?)(?:[ã€‚\n]|$)",
        ]

        # æŠ€è¡“ãƒ‰ãƒ¡ã‚¤ãƒ³è­˜åˆ¥ãƒ‘ã‚¿ãƒ¼ãƒ³
        self.technical_domains = {
            "frontend": [
                "react",
                "vue",
                "css",
                "html",
                "javascript",
                "typescript",
                "ui",
                "ux",
            ],
            "backend": [
                "api",
                "server",
                "database",
                "sql",
                "python",
                "node",
                "express",
            ],
            "infrastructure": [
                "docker",
                "kubernetes",
                "aws",
                "cloud",
                "deployment",
                "ci/cd",
            ],
            "ai_ml": [
                "machine learning",
                "ai",
                "neural",
                "model",
                "training",
                "inference",
            ],
            "security": ["security", "auth", "encryption", "vulnerability", "audit"],
            "testing": ["test", "testing", "qa", "unit test", "integration test"],
        }

        # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–TODOã®èª­ã¿è¾¼ã¿
        self.active_todos = self._load_active_todos()

    def extract_todos_from_conversation(
        self, conversation_text: str, conversation_context: ConversationContext
    ) -> List[ExtractedTodo]:
        """ä¼šè©±ã‹ã‚‰TODOã‚’æŠ½å‡º"""
        extracted_todos = []

        # ãƒ†ã‚­ã‚¹ãƒˆã‚’æ–‡å˜ä½ã§åˆ†å‰²
        sentences = re.split(r"[ã€‚\n]", conversation_text)

        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue

            # å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ãƒãƒƒãƒãƒ³ã‚°
            for pattern in self.todo_patterns:
                matches = re.findall(pattern, sentence, re.IGNORECASE)
                for match in matches:
                    if isinstance(match, tuple):
                        # ã‚¿ãƒ—ãƒ«ã®å ´åˆã€é©åˆ‡ãªéƒ¨åˆ†ã‚’æŠ½å‡º
                        content = self._extract_content_from_match(match)
                    else:
                        content = match

                    if content and len(content.strip()) > 10:  # æœ€å°é•·ãƒã‚§ãƒƒã‚¯
                        todo = self._create_todo_from_content(
                            content, sentence, conversation_context
                        )
                        if todo:
                            extracted_todos.append(todo)

        # é‡è¤‡é™¤å»
        unique_todos = self._deduplicate_todos(extracted_todos)

        return unique_todos

    def _extract_content_from_match(self, match: tuple) -> str:
        """ãƒãƒƒãƒã—ãŸã‚¿ãƒ—ãƒ«ã‹ã‚‰ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º"""
        if len(match) >= 2:
            return match[1].strip()
        elif len(match) == 1:
            return match[0].strip()
        return ""

    def _create_todo_from_content(
        self, content: str, context: str, conversation_context: ConversationContext
    ) -> Optional[ExtractedTodo]:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰TODOã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ"""
        # ç„¡åŠ¹ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        if len(content.strip()) < 10:
            return None

        # æŠ€è¡“è¦ä»¶ã‚’æŠ½å‡º
        technical_requirements = self._extract_technical_requirements(content)

        # å„ªå…ˆåº¦ã‚’æ¨å®š
        priority = self._estimate_priority(content, context)

        # ä½œæ¥­é‡ã‚’æ¨å®š
        estimated_effort = self._estimate_effort(content)

        # ä¾å­˜é–¢ä¿‚ã‚’æ¨å®š
        dependencies = self._extract_dependencies(content)

        # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        confidence_score = self._calculate_confidence_score(content, context)

        return ExtractedTodo(
            id=f"todo_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{hash(content) % 10000}",
            content=content.strip(),
            priority=priority,
            status=TodoStatus.PENDING,
            context=context,
            source="conversation",
            estimated_effort=estimated_effort,
            dependencies=dependencies,
            technical_requirements=technical_requirements,
            confidence_score=confidence_score,
        )

    def _extract_technical_requirements(self, content: str) -> List[str]:
        """æŠ€è¡“è¦ä»¶ã‚’æŠ½å‡º"""
        requirements = []
        content_lower = content.lower()

        for domain, keywords in self.technical_domains.items():
            if any(keyword in content_lower for keyword in keywords):
                requirements.append(domain)

        return requirements

    def _estimate_priority(self, content: str, context: str) -> TodoPriority:
        """å„ªå…ˆåº¦ã‚’æ¨å®š"""
        content_lower = content.lower()
        context_lower = context.lower()

        # ç·Šæ€¥ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        urgent_keywords = [
            "æ€¥",
            "ç·Šæ€¥",
            "urgent",
            "critical",
            "asap",
            "ä»Šã™ã",
            "ã™ãã«",
        ]
        high_keywords = ["é‡è¦", "å¿…é ˆ", "important", "must", "should", "å¿…è¦"]
        low_keywords = ["ã„ãšã‚Œ", "eventually", "sometime", "maybe", "ã§ãã‚Œã°"]

        if any(
            keyword in content_lower or keyword in context_lower
            for keyword in urgent_keywords
        ):
            return TodoPriority.URGENT
        elif any(
            keyword in content_lower or keyword in context_lower
            for keyword in high_keywords
        ):
            return TodoPriority.HIGH
        elif any(
            keyword in content_lower or keyword in context_lower
            for keyword in low_keywords
        ):
            return TodoPriority.LOW
        else:
            return TodoPriority.MEDIUM

    def _estimate_effort(self, content: str) -> str:
        """ä½œæ¥­é‡ã‚’æ¨å®š"""
        content_lower = content.lower()

        # å¤§è¦æ¨¡ã‚¿ã‚¹ã‚¯ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        large_keywords = [
            "ã‚·ã‚¹ãƒ†ãƒ ",
            "system",
            "è¨­è¨ˆ",
            "design",
            "å…¨ä½“",
            "entire",
            "å®Œå…¨",
            "complete",
        ]
        small_keywords = [
            "ä¿®æ­£",
            "fix",
            "èª¿æ•´",
            "adjust",
            "å¾®èª¿æ•´",
            "tweak",
            "ãƒã‚°",
            "bug",
        ]

        if any(keyword in content_lower for keyword in large_keywords):
            return "days"
        elif any(keyword in content_lower for keyword in small_keywords):
            return "minutes"
        else:
            return "hours"

    def _extract_dependencies(self, content: str) -> List[str]:
        """ä¾å­˜é–¢ä¿‚ã‚’æŠ½å‡º"""
        dependencies = []
        content_lower = content.lower()

        # ä¾å­˜é–¢ä¿‚ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¿ãƒ¼ãƒ³
        if "å‰ã«" in content_lower or "before" in content_lower:
            dependencies.append("prerequisite_required")
        if "å¾Œã«" in content_lower or "after" in content_lower:
            dependencies.append("follow_up_task")
        if "ã¨ä¸€ç·’ã«" in content_lower or "with" in content_lower:
            dependencies.append("parallel_task")

        return dependencies

    def _calculate_confidence_score(self, content: str, context: str) -> float:
        """ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
        base_score = 0.5

        # å…·ä½“æ€§ãƒã‚§ãƒƒã‚¯
        if len(content.split()) >= 5:
            base_score += 0.2

        # æŠ€è¡“çš„å…·ä½“æ€§
        if any(
            domain in content.lower()
            for domain_keywords in self.technical_domains.values()
            for domain in domain_keywords
        ):
            base_score += 0.2

        # æ˜ç¢ºãªå‹•è©ã®å­˜åœ¨
        action_verbs = ["å®Ÿè£…", "ä¿®æ­£", "è¿½åŠ ", "å‰Šé™¤", "æ”¹å–„", "ä½œæˆ", "æ§‹ç¯‰", "è¨­è¨ˆ"]
        if any(verb in content for verb in action_verbs):
            base_score += 0.1

        return min(base_score, 1.0)

    def _deduplicate_todos(self, todos: List[ExtractedTodo]) -> List[ExtractedTodo]:
        """é‡è¤‡TODOã‚’é™¤å»"""
        unique_todos = []
        seen_contents = set()

        for todo in todos:
            # å†…å®¹ã®æ­£è¦åŒ–
            normalized_content = re.sub(r"\s+", " ", todo.content.lower().strip())

            # é¡ä¼¼åº¦ãƒã‚§ãƒƒã‚¯ï¼ˆç°¡æ˜“ç‰ˆï¼‰
            is_duplicate = False
            for seen_content in seen_contents:
                if self._calculate_similarity(normalized_content, seen_content) > 0.8:
                    is_duplicate = True
                    break

            if not is_duplicate:
                unique_todos.append(todo)
                seen_contents.add(normalized_content)

        return unique_todos

    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """ãƒ†ã‚­ã‚¹ãƒˆé¡ä¼¼åº¦ã‚’è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        words1 = set(text1.split())
        words2 = set(text2.split())

        intersection = words1.intersection(words2)
        union = words1.union(words2)

        if not union:
            return 0.0

        return len(intersection) / len(union)

    def generate_exit_todo_summary(
        self, conversation_text: str, conversation_context: ConversationContext
    ) -> Dict[str, Any]:
        """ä¼šè©±çµ‚äº†æ™‚TODOè¦ç´„ã‚’ç”Ÿæˆ"""
        # TODOã‚’æŠ½å‡º
        extracted_todos = self.extract_todos_from_conversation(
            conversation_text, conversation_context
        )

        # å„ªå…ˆåº¦åˆ¥ã«åˆ†é¡
        prioritized_todos = {"urgent": [], "high": [], "medium": [], "low": []}

        for todo in extracted_todos:
            todo_dict = asdict(todo)
            # Enumå€¤ã‚’æ–‡å­—åˆ—ã«å¤‰æ›
            todo_dict["priority"] = todo.priority.value
            todo_dict["status"] = todo.status.value
            prioritized_todos[todo.priority.value].append(todo_dict)

        # æŠ€è¡“ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ¥ã«åˆ†é¡
        domain_todos = {}
        for todo in extracted_todos:
            for domain in todo.technical_requirements:
                if domain not in domain_todos:
                    domain_todos[domain] = []
                todo_dict = asdict(todo)
                todo_dict["priority"] = todo.priority.value
                todo_dict["status"] = todo.status.value
                domain_todos[domain].append(todo_dict)

        # è¦ç´„ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        summary = {
            "conversation_id": conversation_context.conversation_id,
            "generated_at": datetime.now().isoformat(),
            "total_todos_extracted": len(extracted_todos),
            "prioritized_todos": prioritized_todos,
            "domain_todos": domain_todos,
            "next_session_recommendations": self._generate_next_session_recommendations(
                extracted_todos
            ),
            "continuity_score": self._calculate_continuity_score(extracted_todos),
            "completion_status": self._assess_completion_status(
                extracted_todos, conversation_context
            ),
            "extracted_todos": [
                {
                    **asdict(todo),
                    "priority": todo.priority.value,
                    "status": todo.status.value,
                }
                for todo in extracted_todos
            ],
        }

        return summary

    def _generate_next_session_recommendations(
        self, todos: List[ExtractedTodo]
    ) -> List[str]:
        """æ¬¡å›ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®æ¨å¥¨äº‹é …ã‚’ç”Ÿæˆ"""
        recommendations = []

        # ç·Šæ€¥ãƒ»é«˜å„ªå…ˆåº¦ã‚¿ã‚¹ã‚¯ã®æ¨å¥¨
        urgent_todos = [todo for todo in todos if todo.priority == TodoPriority.URGENT]
        high_todos = [todo for todo in todos if todo.priority == TodoPriority.HIGH]

        if urgent_todos:
            recommendations.append(f"ç·Šæ€¥ã‚¿ã‚¹ã‚¯ {len(urgent_todos)} ä»¶ã‚’æœ€å„ªå…ˆã§å¯¾å¿œ")

        if high_todos:
            recommendations.append(f"é«˜å„ªå…ˆåº¦ã‚¿ã‚¹ã‚¯ {len(high_todos)} ä»¶ã®æ—©æœŸç€æ‰‹")

        # æŠ€è¡“ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ¥æ¨å¥¨
        domain_counts = {}
        for todo in todos:
            for domain in todo.technical_requirements:
                domain_counts[domain] = domain_counts.get(domain, 0) + 1

        if domain_counts:
            top_domain = max(domain_counts, key=domain_counts.get)
            recommendations.append(
                f"{top_domain} åˆ†é‡ã®ã‚¿ã‚¹ã‚¯ãŒå¤šæ•°({domain_counts[top_domain]}ä»¶)"
            )

        return recommendations

    def _calculate_continuity_score(self, todos: List[ExtractedTodo]) -> float:
        """ç¶™ç¶šæ€§ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
        if not todos:
            return 0.0

        # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã®å¹³å‡
        avg_confidence = sum(todo.confidence_score for todo in todos) / len(todos)

        # å…·ä½“æ€§ã‚¹ã‚³ã‚¢
        concrete_todos = sum(1 for todo in todos if todo.technical_requirements)
        concrete_score = concrete_todos / len(todos)

        # å„ªå…ˆåº¦åˆ†æ•£ã‚¹ã‚³ã‚¢
        priority_counts = {}
        for todo in todos:
            priority_counts[todo.priority] = priority_counts.get(todo.priority, 0) + 1

        priority_score = 0.8 if len(priority_counts) > 1 else 0.5

        return avg_confidence * 0.4 + concrete_score * 0.4 + priority_score * 0.2

    def _assess_completion_status(
        self, todos: List[ExtractedTodo], conversation_context: ConversationContext
    ) -> str:
        """å®Œäº†çŠ¶æ³ã‚’è©•ä¾¡"""
        if not todos:
            return "å®Œäº†ãƒ»ç¶™ç¶šã‚¿ã‚¹ã‚¯ãªã—"

        urgent_count = sum(1 for todo in todos if todo.priority == TodoPriority.URGENT)
        high_count = sum(1 for todo in todos if todo.priority == TodoPriority.HIGH)

        if urgent_count > 0:
            return f"ç·Šæ€¥ã‚¿ã‚¹ã‚¯æ®‹å­˜({urgent_count}ä»¶)"
        elif high_count > 2:
            return f"é«˜å„ªå…ˆåº¦ã‚¿ã‚¹ã‚¯å¤šæ•°({high_count}ä»¶)"
        else:
            return "é©åˆ‡ãªé€²æ—ãƒ»ç¶™ç¶šæº–å‚™å®Œäº†"

    def format_exit_todo_display(self, todo_summary: Dict[str, Any]) -> str:
        """ä¼šè©±çµ‚äº†æ™‚TODOè¡¨ç¤ºã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        display_lines = []

        # ãƒ˜ãƒƒãƒ€ãƒ¼
        display_lines.append("# ğŸ¯ ä¼šè©±çµ‚äº†æ™‚TODOè¦ç´„")
        display_lines.append("")
        display_lines.append(f"**ç”Ÿæˆæ™‚åˆ»**: {todo_summary['generated_at']}")
        display_lines.append(f"**ç·TODOæ•°**: {todo_summary['total_todos_extracted']}")
        display_lines.append(
            f"**ç¶™ç¶šæ€§ã‚¹ã‚³ã‚¢**: {todo_summary['continuity_score']:.2f}"
        )
        display_lines.append(f"**å®Œäº†çŠ¶æ³**: {todo_summary['completion_status']}")
        display_lines.append("")

        # å„ªå…ˆåº¦åˆ¥TODO
        priority_labels = {
            "urgent": "ğŸš¨ ç·Šæ€¥",
            "high": "âš¡ é«˜å„ªå…ˆåº¦",
            "medium": "ğŸ“‹ ä¸­å„ªå…ˆåº¦",
            "low": "ğŸ“ ä½å„ªå…ˆåº¦",
        }

        for priority, label in priority_labels.items():
            todos = todo_summary["prioritized_todos"][priority]
            if todos:
                display_lines.append(f"## {label} ({len(todos)}ä»¶)")
                for todo in todos:
                    # ExtractedTodoã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‹è¾æ›¸ã‹ã‚’åˆ¤å®š
                    if isinstance(todo, dict):
                        content = todo["content"]
                        effort = todo["estimated_effort"]
                        tech_req = todo["technical_requirements"]
                        confidence = todo["confidence_score"]
                    else:
                        content = todo.content
                        effort = todo.estimated_effort
                        tech_req = todo.technical_requirements
                        confidence = todo.confidence_score

                    display_lines.append(f"- **{content}**")
                    display_lines.append(f"  - ä½œæ¥­é‡: {effort}")
                    if tech_req:
                        display_lines.append(f"  - æŠ€è¡“è¦ä»¶: {', '.join(tech_req)}")
                    display_lines.append(f"  - ä¿¡é ¼åº¦: {confidence:.2f}")
                    display_lines.append("")

        # æ¬¡å›ã‚»ãƒƒã‚·ãƒ§ãƒ³æ¨å¥¨
        if todo_summary["next_session_recommendations"]:
            display_lines.append("## ğŸ“… æ¬¡å›ã‚»ãƒƒã‚·ãƒ§ãƒ³æ¨å¥¨äº‹é …")
            for rec in todo_summary["next_session_recommendations"]:
                display_lines.append(f"- {rec}")
            display_lines.append("")

        # æŠ€è¡“ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ¥ã‚µãƒãƒªãƒ¼
        if todo_summary["domain_todos"]:
            display_lines.append("## ğŸ› ï¸ æŠ€è¡“ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ¥TODO")
            for domain, todos in todo_summary["domain_todos"].items():
                display_lines.append(f"- **{domain}**: {len(todos)}ä»¶")

        return "\n".join(display_lines)

    def save_todo_summary(self, todo_summary: Dict[str, Any]):
        """TODOè¦ç´„ã‚’ä¿å­˜"""
        # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–TODOã«è¿½åŠ 
        for todo_data in todo_summary["extracted_todos"]:
            self.active_todos.append(todo_data)

        # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–TODOãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°
        self._save_active_todos()

        # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã«ã‚‚ä¿å­˜
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        archive_file = self.todo_archive_dir / f"todo_summary_{timestamp}.json"

        with open(archive_file, "w", encoding="utf-8") as f:
            json.dump(todo_summary, f, ensure_ascii=False, indent=2)

    def _load_active_todos(self) -> List[Dict[str, Any]]:
        """ã‚¢ã‚¯ãƒ†ã‚£ãƒ–TODOã‚’èª­ã¿è¾¼ã¿"""
        try:
            if self.active_todos_file.exists():
                with open(self.active_todos_file, encoding="utf-8") as f:
                    return json.load(f)
        except Exception:
            pass
        return []

    def _save_active_todos(self):
        """ã‚¢ã‚¯ãƒ†ã‚£ãƒ–TODOã‚’ä¿å­˜"""
        try:
            with open(self.active_todos_file, "w", encoding="utf-8") as f:
                json.dump(self.active_todos, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"ã‚¢ã‚¯ãƒ†ã‚£ãƒ–TODOä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")


# ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é–¢æ•°
def demo_conversation_exit_todo():
    """ä¼šè©±çµ‚äº†æ™‚TODOãƒ—ãƒ­ãƒˆã‚³ãƒ«ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
    print("=== ä¼šè©±çµ‚äº†æ™‚TODOãƒ—ãƒ­ãƒˆã‚³ãƒ« ãƒ‡ãƒ¢ ===")

    protocol = ConversationExitTodoProtocol()

    # ã‚µãƒ³ãƒ—ãƒ«ä¼šè©±ãƒ†ã‚­ã‚¹ãƒˆ
    sample_conversation = """
    ãƒ¦ãƒ¼ã‚¶ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­è¨ˆã‚’æ”¹å–„ã—ã¦ãã ã•ã„
    Claude: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­è¨ˆã‚’æ”¹å–„ã—ã¾ã—ãŸã€‚æ¬¡ã«ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã®UIæ”¹ä¿®ã‚‚å¿…è¦ã§ã™ã€‚
    ãƒ¦ãƒ¼ã‚¶ãƒ¼: èªè¨¼ã‚·ã‚¹ãƒ†ãƒ ã‚‚è¿½åŠ ã—ã¦ãã ã•ã„
    Claude: èªè¨¼ã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿè£…ã—ã¾ã—ãŸã€‚å¾Œã§ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç›£æŸ»ã‚’å®Ÿæ–½ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
    ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã®è¿½åŠ ã‚‚ä»Šåº¦è¡Œã„ã¾ã™ã€‚
    """

    # ä¼šè©±ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
    context = ConversationContext(
        conversation_id="demo_conversation_001",
        start_time=datetime.now().isoformat(),
        end_time=None,
        total_messages=4,
        key_topics=["ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹", "èªè¨¼", "UI"],
        technical_domains=["backend", "frontend", "security"],
        completion_status="partial",
    )

    # TODOè¦ç´„ç”Ÿæˆ
    todo_summary = protocol.generate_exit_todo_summary(sample_conversation, context)

    # è¡¨ç¤ºãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
    formatted_display = protocol.format_exit_todo_display(todo_summary)

    print(formatted_display)

    # ä¿å­˜
    protocol.save_todo_summary(todo_summary)

    print("\nâœ… TODOè¦ç´„ãŒä¿å­˜ã•ã‚Œã¾ã—ãŸ")


if __name__ == "__main__":
    demo_conversation_exit_todo()
