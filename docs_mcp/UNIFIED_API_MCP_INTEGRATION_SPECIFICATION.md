# asagami AI Ã— Cursor Rules çµ±åˆã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆä»•æ§˜æ›¸

## ğŸ¯ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦

### ã‚·ã‚¹ãƒ†ãƒ ã®ç›®çš„
asagami AIã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã€å€‹äººãƒ»ãƒãƒ¼ãƒ ã®å¼±ç‚¹ã«åŸºã¥ã„ã¦Cursor Rulesã‚’è‡ªå‹•ç”Ÿæˆã™ã‚‹ã€Œé©å¿œå‹é–‹ç™ºç’°å¢ƒã€ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€å­¦ç¿’â†’å®Ÿè·µâ†’ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯â†’æ”¹å–„ã®ç¶™ç¶šçš„ãªã‚µã‚¤ã‚¯ãƒ«ã‚’å®Ÿç¾ã—ã€é–‹ç™ºè€…ã®èƒ½åŠ›å‘ä¸Šã¨é–‹ç™ºå“è³ªã®å‘ä¸Šã‚’åŒæ™‚ã«é”æˆã—ã¾ã™ã€‚

### æ ¸å¿ƒçš„ä¾¡å€¤ææ¡ˆ
- **å€‹äººé©å¿œå‹æ”¯æ´**: å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å€‹äººã®å¼±ç‚¹ã‚’ç‰¹å®šã—ã€å°‚ç”¨ã®Cursor Rulesã‚’ç”Ÿæˆ
- **ç¶™ç¶šçš„æ”¹å–„**: å®Ÿéš›ã®é–‹ç™ºãƒ­ã‚°ã‚’ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã—ã¦å­¦ç¿’ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æœ€é©åŒ–
- **ãƒãƒ¼ãƒ å“è³ªå‘ä¸Š**: çµ„ç¹”å…¨ä½“ã®ã‚¹ã‚­ãƒ«ãƒ¬ãƒ™ãƒ«å‡ä¸€åŒ–ã¨ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹å…±æœ‰

## ğŸ—ï¸ ã‚·ã‚¹ãƒ†ãƒ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

### å…¨ä½“æ§‹æˆå›³
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    asagami AI       â”‚    â”‚    MCP Server      â”‚    â”‚     Cursor IDE      â”‚
â”‚    (Django)         â”‚â—„â”€â”€â–ºâ”‚    (Python)        â”‚â—„â”€â”€â–ºâ”‚   (Claude Code)     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚â€¢ å­¦ç¿’ãƒãƒ¼ãƒˆç®¡ç†     â”‚    â”‚â€¢ å­¦ç¿’ãƒ‡ãƒ¼ã‚¿åˆ†æ    â”‚    â”‚â€¢ é–‹ç™ºæ”¯æ´           â”‚
â”‚â€¢ å•é¡Œè‡ªå‹•ç”Ÿæˆ       â”‚    â”‚â€¢ AIå¼±ç‚¹æ¤œå‡º        â”‚    â”‚â€¢ ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°åé›†     â”‚
â”‚â€¢ è§£ç­”çµæœåˆ†æ       â”‚    â”‚â€¢ Cursor Rulesç”Ÿæˆ  â”‚    â”‚â€¢ é©å¿œå‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ â”‚
â”‚â€¢ çµ„ç¹”ãƒ»ãƒ¦ãƒ¼ã‚¶ãƒ¼ç®¡ç† â”‚    â”‚â€¢ ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å‡¦ç†â”‚    â”‚â€¢ å®Ÿè·µãƒ‡ãƒ¼ã‚¿é€ä¿¡     â”‚
â”‚â€¢ çµ±è¨ˆãƒ»ãƒ¬ãƒãƒ¼ãƒˆ     â”‚    â”‚â€¢ ç¶™ç¶šçš„å­¦ç¿’æ”¹å–„    â”‚    â”‚â€¢ ãƒ«ãƒ¼ãƒ«é©ç”¨         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼è©³ç´°
```
1. å­¦ç¿’ãƒ•ã‚§ãƒ¼ã‚º
   asagami AI: ãƒãƒ¼ãƒˆä½œæˆ â†’ å•é¡Œç”Ÿæˆ â†’ è§£ç­” â†’ çµæœè¨˜éŒ²

2. åˆ†æãƒ•ã‚§ãƒ¼ã‚º  
   MCP Server: å­¦ç¿’ãƒ‡ãƒ¼ã‚¿å–å¾— â†’ AIåˆ†æ â†’ å¼±ç‚¹æ¤œå‡º â†’ ãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡º

3. ãƒ«ãƒ¼ãƒ«ç”Ÿæˆãƒ•ã‚§ãƒ¼ã‚º
   MCP Server: å€‹äººãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ â†’ Cursor Rulesç”Ÿæˆ â†’ é…ä¿¡

4. å®Ÿè·µãƒ•ã‚§ãƒ¼ã‚º
   Cursor: é–‹ç™ºæ”¯æ´ â†’ ã‚¨ãƒ©ãƒ¼/æˆåŠŸãƒ­ã‚°è¨˜éŒ² â†’ ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯é€ä¿¡

5. æ”¹å–„ãƒ•ã‚§ãƒ¼ã‚º
   MCP Server: ãƒ­ã‚°åˆ†æ â†’ æ–°ãŸãªå¼±ç‚¹æ¤œå‡º â†’ å­¦ç¿’ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ›´æ–°ææ¡ˆ
```

## ğŸ”§ æŠ€è¡“ä»•æ§˜

### ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯
- **Django 4.2+**: æ—¢å­˜asagami AIã‚·ã‚¹ãƒ†ãƒ ã®æ‹¡å¼µ
- **PostgreSQL 14+**: å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãƒ»åˆ†æçµæœã®æ°¸ç¶šåŒ–
- **Redis 7+**: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ»ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†ãƒ»ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€šä¿¡
- **Celery 5+**: é‡ã„åˆ†æå‡¦ç†ã®éåŒæœŸå®Ÿè¡Œ
- **OpenAI API (GPT-4)**: å­¦ç¿’ãƒ‡ãƒ¼ã‚¿åˆ†æãƒ»ãƒ«ãƒ¼ãƒ«ç”ŸæˆAI

### MCP ServeræŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯
- **Python 3.9+**: ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èª
- **mcp (Model Context Protocol)**: Claude Codeã¨ã®æ¨™æº–é€£æº
- **FastAPI 0.104+**: é«˜æ€§èƒ½APIå±¤
- **asyncio**: éåŒæœŸå‡¦ç†ã«ã‚ˆã‚‹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

### ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ï¼ˆæ‹¡å¼µï¼‰
- **React 18.2+**: æ—¢å­˜UIã®æ‹¡å¼µ
- **WebSocket**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€šçŸ¥ãƒ»æ›´æ–°
- **Chart.js**: å­¦ç¿’é€²æ—ãƒ»åˆ†æçµæœã®å¯è¦–åŒ–

## ğŸ—„ï¸ ãƒ‡ãƒ¼ã‚¿è¨­è¨ˆ

### æ–°è¦ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«

#### CursorRuleProfileï¼ˆCursor Rulesç®¡ç†ï¼‰
```python
class CursorRuleProfile(models.Model):
    user = models.ForeignKey(CustomUser, on_delete=models.CASCADE)
    rule_version = models.CharField(max_length=50)  # ãƒ«ãƒ¼ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†
    generated_at = models.DateTimeField(auto_now_add=True)
    rule_config = models.JSONField()  # ç”Ÿæˆã•ã‚ŒãŸãƒ«ãƒ¼ãƒ«å†…å®¹
    skill_focus_areas = models.JSONField()  # é‡ç‚¹ã‚¹ã‚­ãƒ«é ˜åŸŸ
    effectiveness_score = models.FloatField(default=0.0)  # ãƒ«ãƒ¼ãƒ«åŠ¹æœæ¸¬å®š
    is_active = models.BooleanField(default=True)
```

#### PracticeLogï¼ˆå®Ÿè·µãƒ­ã‚°ï¼‰
```python
class PracticeLog(models.Model):
    user = models.ForeignKey(CustomUser, on_delete=models.CASCADE)
    session_id = models.CharField(max_length=100, unique=True)
    start_time = models.DateTimeField()
    end_time = models.DateTimeField()
    
    # é–‹ç™ºã‚»ãƒƒã‚·ãƒ§ãƒ³è©³ç´°
    files_modified = models.IntegerField(default=0)
    lines_of_code = models.IntegerField(default=0)
    commits_made = models.IntegerField(default=0)
    
    # ã‚¨ãƒ©ãƒ¼ãƒ»å®Œäº†ãƒ‡ãƒ¼ã‚¿
    error_data = models.JSONField(default=list)  # ã‚¨ãƒ©ãƒ¼è©³ç´°
    completion_data = models.JSONField(default=list)  # ã‚³ãƒ¼ãƒ‰è£œå®Œãƒ‡ãƒ¼ã‚¿
    rule_trigger_data = models.JSONField(default=list)  # ãƒ«ãƒ¼ãƒ«ç™ºç«ãƒ‡ãƒ¼ã‚¿
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    productivity_score = models.FloatField(default=0.0)
    satisfaction_rating = models.IntegerField(null=True, blank=True)  # 1-5è©•ä¾¡
```

#### LearningAnalysisï¼ˆå­¦ç¿’åˆ†æçµæœï¼‰
```python
class LearningAnalysis(models.Model):
    user = models.ForeignKey(CustomUser, on_delete=models.CASCADE)
    analysis_date = models.DateField(auto_now_add=True)
    analysis_period_days = models.IntegerField(default=30)
    
    # åˆ†æçµæœ
    weak_points = models.JSONField()  # å¼±ç‚¹è©³ç´°ãƒªã‚¹ãƒˆ
    strong_points = models.JSONField()  # å¼·ã¿è©³ç´°ãƒªã‚¹ãƒˆ
    skill_level = models.CharField(max_length=20)  # beginner/intermediate/advanced
    improvement_suggestions = models.JSONField()  # æ”¹å–„ææ¡ˆãƒªã‚¹ãƒˆ
    
    # ä¿¡é ¼åº¦ãƒ»ç²¾åº¦æŒ‡æ¨™
    confidence_score = models.FloatField()  # åˆ†æä¿¡é ¼åº¦ï¼ˆ0.0-1.0ï¼‰
    data_sufficiency = models.BooleanField()  # åˆ†æååˆ†æ€§ãƒ•ãƒ©ã‚°
```

### ãƒ‡ãƒ¼ã‚¿æ§‹é€ ä¾‹

#### å­¦ç¿’åˆ†æãƒ‡ãƒ¼ã‚¿æ§‹é€ 
```json
{
  "user_id": 123,
  "analysis_date": "2025-07-13",
  "analysis_period": 30,
  "summary": {
    "total_notes": 45,
    "total_questions": 180,
    "average_score": 78.5,
    "study_hours": 67.2,
    "skill_level": "intermediate"
  },
  "weak_points": [
    {
      "topic": "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­è¨ˆ",
      "subject_id": 5,
      "average_score": 62,
      "question_count": 15,
      "error_frequency": 8,
      "error_patterns": [
        "æ­£è¦åŒ–ç†è«–ã®ç†è§£ä¸è¶³",
        "ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¨­è¨ˆã®åˆ¤æ–­ãƒŸã‚¹",
        "å¤–éƒ¨ã‚­ãƒ¼åˆ¶ç´„ã®å®Ÿè£…ã‚¨ãƒ©ãƒ¼"
      ],
      "improvement_priority": "high",
      "estimated_study_time": "8-12æ™‚é–“",
      "recommended_actions": [
        "æ­£è¦åŒ–ç†è«–ã®åŸºç¤å¾©ç¿’ï¼ˆç¬¬1-3æ­£è¦å½¢ï¼‰",
        "å®Ÿéš›ã®DBã§ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŠ¹æœã‚’æ¸¬å®š",
        "å¤–éƒ¨ã‚­ãƒ¼åˆ¶ç´„ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹å­¦ç¿’"
      ]
    }
  ],
  "strong_points": [
    {
      "topic": "REST APIè¨­è¨ˆ",
      "subject_id": 3,
      "average_score": 94,
      "mastery_level": "advanced",
      "expertise_areas": ["RESTfulè¨­è¨ˆåŸå‰‡", "HTTP ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰", "èªè¨¼å®Ÿè£…"]
    }
  ],
  "learning_patterns": {
    "preferred_study_time": "09:00-11:00",
    "average_session_duration": 45,
    "retention_rate": 0.82,
    "difficulty_preference": "intermediate_to_advanced",
    "most_effective_question_types": ["scenario_based", "hands_on_coding"]
  }
}
```

#### Cursor Rulesç”Ÿæˆçµæœ
```json
{
  "rule_id": "cr_123_20250713_v1",
  "generated_at": "2025-07-13T10:30:00Z",
  "user_profile": {
    "user_id": 123,
    "skill_level": "intermediate",
    "specializations": ["web_development", "database_design"],
    "learning_goals": ["improve_db_design", "enhance_security_awareness"]
  },
  "cursor_rules": {
    "database": {
      "normalization": {
        "enabled": true,
        "severity": "error",
        "message": "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ­£è¦åŒ–ãƒã‚§ãƒƒã‚¯: {table_name}ã®è¨­è¨ˆã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚ç¬¬3æ­£è¦å½¢ã¾ã§é©ç”¨ã•ã‚Œã¦ã„ã¾ã™ã‹ï¼Ÿ",
        "trigger_patterns": ["CREATE TABLE", "ALTER TABLE"],
        "auto_suggestions": true,
        "reference_links": [
          "https://asagami.ai/notes/normalization-guide",
          "https://asagami.ai/practice/db-design-checker"
        ]
      },
      "index_optimization": {
        "enabled": true,
        "severity": "warning", 
        "message": "ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æœ€é©åŒ–: ã“ã®ã‚¯ã‚¨ãƒªã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒå¿…è¦ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚EXPLAIN PLANã§ç¢ºèªã—ã¦ãã ã•ã„ã€‚",
        "trigger_patterns": ["SELECT .* WHERE", "JOIN"],
        "code_templates": ["templates/index_example.sql"]
      }
    },
    "security": {
      "sql_injection": {
        "enabled": true,
        "severity": "error",
        "message": "SQLã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³å¯¾ç­–: ãƒ—ãƒªãƒšã‚¢ãƒ‰ã‚¹ãƒ†ãƒ¼ãƒˆãƒ¡ãƒ³ãƒˆã¾ãŸã¯ORMã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„",
        "trigger_patterns": ["SELECT.*\\+", "INSERT.*\\+", "UPDATE.*\\+"],
        "auto_fix": true,
        "fix_template": "prepared_statement_template.py"
      }
    },
    "performance": {
      "n_plus_one": {
        "enabled": true,
        "severity": "warning",
        "message": "N+1ã‚¯ã‚¨ãƒªå•é¡Œã®å¯èƒ½æ€§: ãƒãƒƒãƒãƒ­ãƒ¼ãƒ‰å‡¦ç†ã‚’æ¤œè¨ã—ã¦ãã ã•ã„",
        "trigger_patterns": ["for.*in.*:", "while.*:"],
        "optimization_suggestions": ["eager_loading", "bulk_operations"]
      }
    }
  },
  "code_templates": [
    {
      "name": "secure_database_connection",
      "file_path": "templates/secure_db_conn.py",
      "description": "ã‚»ã‚­ãƒ¥ã‚¢ãªãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼ˆæ¥ç¶šãƒ—ãƒ¼ãƒ«ã€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å«ã‚€ï¼‰",
      "usage_context": ["database_connection", "orm_setup"]
    },
    {
      "name": "index_performance_check",
      "file_path": "templates/index_check.sql", 
      "description": "ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŠ¹æœæ¸¬å®šç”¨SQLãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ",
      "usage_context": ["performance_tuning", "query_optimization"]
    }
  ],
  "personalized_suggestions": [
    "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­è¨ˆã®å®Ÿè·µçš„èª²é¡Œã«å–ã‚Šçµ„ã‚€ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™",
    "æ­£è¦åŒ–ç†è«–ã®ç¢ºèªãƒ†ã‚¹ãƒˆã‚’é€±1å›å®Ÿæ–½ã—ã¦ã¿ã¦ãã ã•ã„",
    "å®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§EXPLAIN PLANã‚’ä½¿ã£ãŸæœ€é©åŒ–ã‚’è©¦ã—ã¦ã¿ã¦ãã ã•ã„"
  ],
  "effectiveness_tracking": {
    "previous_error_count": 12,
    "target_error_reduction": 0.5,
    "measurement_period": 30,
    "success_metrics": ["error_reduction", "code_quality_score", "development_velocity"]
  }
}
```

## ğŸŒ APIä»•æ§˜

### RESTful API ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ

#### 1. å­¦ç¿’ãƒ‡ãƒ¼ã‚¿åˆ†æAPI

**å€‹äººå­¦ç¿’ãƒ‡ãƒ¼ã‚¿å–å¾—**
```
GET /api/cursor-integration/learning-data/{user_id}
Parameters:
  - user_id (required): ãƒ¦ãƒ¼ã‚¶ãƒ¼ID
  - period (optional): åˆ†ææœŸé–“ï¼ˆæ—¥æ•°ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ30ï¼‰
  - include_team_context (optional): ãƒãƒ¼ãƒ æ¯”è¼ƒãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€ã‹
  - detail_level (optional): è©³ç´°ãƒ¬ãƒ™ãƒ«ï¼ˆsummary/detailed/comprehensiveï¼‰

Response: ä¸Šè¨˜ã®å­¦ç¿’åˆ†æãƒ‡ãƒ¼ã‚¿æ§‹é€ 
```

**ãƒãƒ¼ãƒ å­¦ç¿’åˆ†æ**
```
GET /api/cursor-integration/team-analytics/{department_id}
Parameters:
  - department_id (required): éƒ¨é–€ID
  - comparison_period (optional): æ¯”è¼ƒæœŸé–“è¨­å®š
  - skill_breakdown (optional): ã‚¹ã‚­ãƒ«åˆ¥è©³ç´°åˆ†æ

Response:
{
  "department_id": 10,
  "team_size": 12,
  "analysis_date": "2025-07-13",
  "skill_distribution": {
    "database_design": {"beginner": 3, "intermediate": 7, "advanced": 2},
    "security": {"beginner": 5, "intermediate": 5, "advanced": 2},
    "api_development": {"beginner": 2, "intermediate": 8, "advanced": 2}
  },
  "common_weak_points": [
    {
      "topic": "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å®Ÿè£…",
      "affected_members": 8,
      "average_score": 65,
      "priority": "high",
      "team_impact": "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“ã®ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒªã‚¹ã‚¯"
    }
  ],
  "improvement_roadmap": [
    {
      "phase": "immediate",
      "actions": ["ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£åŸºç¤ã®ãƒãƒ¼ãƒ å‹‰å¼·ä¼š"],
      "timeline": "2é€±é–“",
      "expected_improvement": 15
    }
  ]
}
```

#### 2. Cursor Rulesç”ŸæˆAPI

**å€‹äººç”¨ãƒ«ãƒ¼ãƒ«ç”Ÿæˆ**
```
POST /api/cursor-integration/generate-rules

Request Body:
{
  "user_id": 123,
  "analysis_data": { /* å­¦ç¿’åˆ†æçµæœ */ },
  "rule_config": {
    "strictness_level": "intermediate",  // strict/intermediate/lenient
    "focus_areas": ["security", "performance", "maintainability"],
    "include_templates": true,
    "auto_suggestions": true,
    "development_context": "web_application"  // context for better rules
  },
  "integration_preferences": {
    "notification_level": "moderate",  // high/moderate/low
    "auto_fix_enabled": true,
    "learning_mode": true  // ã‚ˆã‚Šè©³ç´°ãªèª¬æ˜ã¨ãƒªãƒ³ã‚¯ã‚’æä¾›
  }
}

Response: ä¸Šè¨˜ã®Cursor Rulesç”Ÿæˆçµæœæ§‹é€ 
```

**ãƒãƒ¼ãƒ æ¨™æº–ãƒ«ãƒ¼ãƒ«ç”Ÿæˆ**
```
POST /api/cursor-integration/generate-team-rules

Request Body:
{
  "department_id": 10,
  "team_analytics": { /* ãƒãƒ¼ãƒ åˆ†æçµæœ */ },
  "organizational_standards": {
    "compliance_requirements": ["GDPR", "PCI_DSS", "SOX"],
    "coding_standards": "google_style_guide",
    "security_level": "enterprise",
    "performance_requirements": "high_traffic"
  },
  "customization_level": "organization"  // organization/team/individual
}
```

#### 3. å®Ÿè·µãƒ­ã‚°åé›†API

**é–‹ç™ºã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ­ã‚°é€ä¿¡**
```
POST /api/cursor-integration/practice-logs

Request Body:
{
  "user_id": 123,
  "session_id": "sess_20250713_001",
  "session_metadata": {
    "start_time": "2025-07-13T09:00:00Z",
    "end_time": "2025-07-13T10:30:00Z",
    "project_type": "web_application",
    "development_environment": "local",
    "cursor_version": "0.42.0"
  },
  "development_activity": {
    "files_modified": [
      {
        "file_path": "src/auth/login.py",
        "language": "python",
        "modification_type": "enhancement",
        "lines_added": 23,
        "lines_deleted": 5
      }
    ],
    "commits": [
      {
        "commit_hash": "a1b2c3d",
        "message": "Add secure login validation",
        "timestamp": "2025-07-13T10:15:00Z"
      }
    ]
  },
  "error_events": [
    {
      "error_type": "runtime_error",
      "error_category": "authentication",
      "error_message": "Invalid token format",
      "file_path": "src/auth/token_validator.py",
      "line_number": 45,
      "resolution_time_seconds": 180,
      "resolution_method": "cursor_suggestion",
      "user_satisfaction": 4,  // 1-5 scale
      "learning_value": "high"  // high/medium/low
    }
  ],
  "rule_interactions": [
    {
      "rule_id": "security.sql_injection",
      "triggered_at": "2025-07-13T09:45:00Z",
      "trigger_context": "UPDATE users SET password = ? WHERE id = ?",
      "user_action": "accepted",  // accepted/dismissed/modified
      "effectiveness_rating": 5,
      "user_feedback": "Very helpful reminder"
    }
  ],
  "productivity_metrics": {
    "code_completion_usage": 67,  // percentage
    "auto_fix_acceptance_rate": 0.78,
    "average_time_to_resolve_error": 156,  // seconds
    "focus_time_percentage": 0.85,
    "context_switch_count": 3
  }
}

Response:
{
  "log_id": "log_123_20250713_001",
  "processing_status": "accepted",
  "analysis_scheduled": true,
  "immediate_insights": [
    "authentication é–¢é€£ã®ã‚¨ãƒ©ãƒ¼ãŒå¤šãç™ºç”Ÿã—ã¦ã„ã¾ã™",
    "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ«ãƒ¼ãƒ«ã®åŠ¹æœãŒç¢ºèªã•ã‚Œã¾ã—ãŸ"
  ],
  "next_analysis_eta": "2025-07-13T11:00:00Z"
}
```

#### 4. ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åˆ†æAPI

**ç¶™ç¶šçš„æ”¹å–„ãƒ¬ãƒãƒ¼ãƒˆ**
```
GET /api/cursor-integration/improvement-report/{user_id}

Parameters:
  - report_period: æœŸé–“ï¼ˆweekly/monthly/quarterlyï¼‰
  - include_projections: å°†æ¥äºˆæ¸¬ã‚’å«ã‚€ã‹
  - detail_level: è©³ç´°ãƒ¬ãƒ™ãƒ«

Response:
{
  "user_id": 123,
  "report_period": "2025-06-13 to 2025-07-13",
  "overall_progress": {
    "skill_score_change": "+12.5",
    "skill_level_progression": "intermediate â†’ upper-intermediate",
    "confidence_improvement": 0.23,
    "error_reduction_percentage": 0.45
  },
  "learning_effectiveness": {
    "asagami_study_contribution": 0.65,
    "cursor_practice_contribution": 0.72,
    "synergy_effect": 0.89,  // å˜ä½“ä½¿ç”¨æ™‚ã¨æ¯”è¼ƒã—ãŸç›¸ä¹—åŠ¹æœ
    "optimal_study_practice_ratio": "40:60"
  },
  "skill_improvements": [
    {
      "skill": "database_design",
      "before_score": 62,
      "current_score": 78,
      "improvement_velocity": "+2.1 points/week",
      "plateau_risk": "low",
      "next_milestone": "advanced_level"
    }
  ],
  "cursor_rules_effectiveness": [
    {
      "rule_category": "security",
      "error_prevention_count": 15,
      "false_positive_rate": 0.12,
      "user_satisfaction": 4.3,
      "adjustment_recommendation": "increase_strictness"
    }
  ],
  "future_projections": {
    "expected_skill_level": "advanced",
    "eta_to_next_level": "6-8 weeks",
    "recommended_focus_areas": ["microservices", "cloud_architecture"],
    "optimal_learning_path": [
      "å®Œäº†: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­è¨ˆåŸºç¤",
      "é€²è¡Œä¸­: ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å®Ÿè£…",
      "æ¬¡æœŸ: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–",
      "å°†æ¥: ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆ"
    ]
  }
}
```

## ğŸ¤– MCP (Model Context Protocol) å®Ÿè£…

### MCPã‚µãƒ¼ãƒãƒ¼ä»•æ§˜

**ã‚µãƒ¼ãƒãƒ¼åŸºæœ¬æƒ…å ±**
- ã‚µãƒ¼ãƒãƒ¼å: `asagami-mcp-server`
- ãƒãƒ¼ã‚¸ãƒ§ãƒ³: `1.0.0`
- ãƒ—ãƒ­ãƒˆã‚³ãƒ«: MCP v1.0
- ãƒãƒ¼ãƒˆ: 8001

### æä¾›ãƒ„ãƒ¼ãƒ«è©³ç´°

#### 1. analyze_learning_data
```python
@mcp_server.tool()
async def analyze_learning_data(
    user_id: int, 
    analysis_period: int = 30,
    analysis_type: str = "comprehensive"
) -> dict:
    """
    å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ç·åˆçš„ã«åˆ†æã—ã€å€‹äººã®å­¦ç¿’ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨å¼±ç‚¹ã‚’ç‰¹å®š
    
    Args:
        user_id: ãƒ¦ãƒ¼ã‚¶ãƒ¼ID
        analysis_period: åˆ†ææœŸé–“ï¼ˆæ—¥æ•°ï¼‰
        analysis_type: åˆ†æã‚¿ã‚¤ãƒ—ï¼ˆcomprehensive/focused/quickï¼‰
    
    Returns:
        è©³ç´°ãªå­¦ç¿’åˆ†æçµæœï¼ˆå¼±ç‚¹ã€å¼·ã¿ã€å­¦ç¿’ãƒ‘ã‚¿ãƒ¼ãƒ³ã€æ”¹å–„ææ¡ˆï¼‰
    """
    # Django APIã‹ã‚‰ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    learning_data = await fetch_user_learning_data(user_id, analysis_period)
    
    # OpenAI GPT-4ã«ã‚ˆã‚‹è©³ç´°åˆ†æ
    analysis_prompt = create_analysis_prompt(learning_data, analysis_type)
    ai_analysis = await openai_client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": analysis_prompt}],
        response_format={"type": "json_object"}
    )
    
    # åˆ†æçµæœã®å¾Œå‡¦ç†ã¨æ¤œè¨¼
    processed_result = validate_and_enhance_analysis(ai_analysis)
    
    return processed_result
```

#### 2. generate_cursor_rules
```python
@mcp_server.tool()
async def generate_cursor_rules(
    analysis_data: dict,
    customization_level: str = "personalized",
    rule_strictness: str = "intermediate"
) -> dict:
    """
    å­¦ç¿’åˆ†æçµæœã‹ã‚‰å€‹äººã«æœ€é©åŒ–ã•ã‚ŒãŸCursor Rulesã‚’ç”Ÿæˆ
    
    Args:
        analysis_data: analyze_learning_dataã®å‡ºåŠ›çµæœ
        customization_level: ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«ï¼ˆbasic/personalized/expertï¼‰
        rule_strictness: ãƒ«ãƒ¼ãƒ«ã®å³æ ¼åº¦ï¼ˆlenient/intermediate/strictï¼‰
    
    Returns:
        å®Œå…¨ã«ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã•ã‚ŒãŸCursor Rulesè¨­å®š
    """
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã®æ§‹ç¯‰
    user_profile = build_user_profile(analysis_data)
    
    # å¼±ç‚¹ã«åŸºã¥ããƒ«ãƒ¼ãƒ«ç”Ÿæˆæˆ¦ç•¥ã®æ±ºå®š
    rule_strategy = determine_rule_strategy(
        user_profile.weak_points,
        user_profile.skill_level,
        rule_strictness
    )
    
    # ãƒ«ãƒ¼ãƒ«ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ãƒ­ãƒ¼ãƒ‰ã¨ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º
    base_rules = load_rule_templates(user_profile.focus_areas)
    customized_rules = customize_rules_for_user(
        base_rules, 
        user_profile, 
        rule_strategy
    )
    
    # ã‚³ãƒ¼ãƒ‰ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã¨ã‚µãƒ³ãƒ—ãƒ«ã®ç”Ÿæˆ
    code_templates = generate_code_templates(user_profile.weak_points)
    
    # æœ€çµ‚ãƒ«ãƒ¼ãƒ«æ§‹é€ ã®æ§‹ç¯‰
    final_rules = {
        "metadata": {
            "generated_for": user_profile.user_id,
            "generation_timestamp": datetime.utcnow().isoformat(),
            "customization_level": customization_level,
            "expected_effectiveness": calculate_expected_effectiveness(user_profile)
        },
        "rules": customized_rules,
        "templates": code_templates,
        "learning_resources": generate_learning_links(user_profile.weak_points)
    }
    
    return final_rules
```

#### 3. process_practice_feedback
```python
@mcp_server.tool()
async def process_practice_feedback(
    practice_logs: list,
    feedback_type: str = "comprehensive"
) -> dict:
    """
    å®Ÿè·µãƒ­ã‚°ã‚’åˆ†æã—ã¦ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’ç”Ÿæˆã—ã€å­¦ç¿’è¨ˆç”»ã‚’æ›´æ–°
    
    Args:
        practice_logs: å®Ÿè·µã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿
        feedback_type: ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚¿ã‚¤ãƒ—ï¼ˆquick/comprehensive/predictiveï¼‰
    
    Returns:
        ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åˆ†æçµæœã¨æ”¹å–„ææ¡ˆ
    """
    # ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã¨çµ±åˆ
    consolidated_logs = consolidate_practice_logs(practice_logs)
    
    # ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ©Ÿæ¢°å­¦ç¿’åˆ†æ
    error_patterns = analyze_error_patterns(consolidated_logs.errors)
    skill_gaps = detect_skill_gaps(consolidated_logs, error_patterns)
    
    # ãƒ«ãƒ¼ãƒ«åŠ¹æœã®æ¸¬å®š
    rule_effectiveness = measure_rule_effectiveness(
        consolidated_logs.rule_interactions
    )
    
    # æ–°ã—ã„å­¦ç¿’ç›®æ¨™ã®ææ¡ˆ
    learning_recommendations = generate_learning_recommendations(
        skill_gaps, 
        error_patterns,
        rule_effectiveness
    )
    
    # asagami AIã¸ã®å­¦ç¿’ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ›´æ–°ææ¡ˆ
    content_updates = propose_content_updates(learning_recommendations)
    
    return {
        "analysis_summary": {
            "total_sessions": len(practice_logs),
            "error_trend": calculate_error_trend(error_patterns),
            "skill_improvement_rate": calculate_improvement_rate(consolidated_logs),
            "rule_adoption_rate": calculate_rule_adoption(rule_effectiveness)
        },
        "detected_patterns": error_patterns,
        "skill_gap_analysis": skill_gaps,
        "rule_effectiveness": rule_effectiveness,
        "learning_recommendations": learning_recommendations,
        "content_update_proposals": content_updates
    }
```

### MCPã‚µãƒ¼ãƒãƒ¼å®Ÿè£…ä¾‹

```python
# mcp_server/main.py
import asyncio
from mcp import McpServer
from mcp.types import Tool, TextContent
import openai
import aiohttp
import json
from datetime import datetime
from typing import Dict, List, Optional

class AsagamiMcpServer(McpServer):
    def __init__(self, django_api_base: str, openai_api_key: str):
        super().__init__("asagami-mcp-server", "1.0.0")
        self.django_api = django_api_base
        self.openai_client = openai.AsyncOpenAI(api_key=openai_api_key)
        self.setup_tools()
    
    async def fetch_user_learning_data(self, user_id: int, period: int) -> Dict:
        """Django APIã‹ã‚‰å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        async with aiohttp.ClientSession() as session:
            url = f"{self.django_api}/api/cursor-integration/learning-data/{user_id}"
            params = {"period": period, "detail_level": "comprehensive"}
            async with session.get(url, params=params) as response:
                return await response.json()
    
    def create_analysis_prompt(self, learning_data: Dict, analysis_type: str) -> str:
        """AIåˆ†æç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ"""
        base_prompt = f"""
        ä»¥ä¸‹ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’è©³ç´°ã«åˆ†æã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å­¦ç¿’ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨æ”¹å–„ç‚¹ã‚’ç‰¹å®šã—ã¦ãã ã•ã„ï¼š

        å­¦ç¿’çµ±è¨ˆ:
        - ç·å­¦ç¿’æ™‚é–“: {learning_data.get('summary', {}).get('study_hours', 0)}æ™‚é–“
        - ç·å•é¡Œæ•°: {learning_data.get('summary', {}).get('total_questions', 0)}
        - å¹³å‡æ­£ç­”ç‡: {learning_data.get('summary', {}).get('average_score', 0)}%

        ç§‘ç›®åˆ¥æˆç¸¾:
        {json.dumps(learning_data.get('subject_scores', {}), indent=2)}

        æœ€è¿‘ã®ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³:
        {json.dumps(learning_data.get('recent_errors', []), indent=2)}

        åˆ†æè¦æ±‚ãƒ¬ãƒ™ãƒ«: {analysis_type}

        ä»¥ä¸‹ã®JSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§å›ç­”ã—ã¦ãã ã•ã„ï¼š
        {{
            "weak_points": [
                {{
                    "topic": "å…·ä½“çš„ãªãƒˆãƒ”ãƒƒã‚¯å",
                    "severity": "high/medium/low",
                    "evidence": ["å…·ä½“çš„ãªæ ¹æ‹ 1", "å…·ä½“çš„ãªæ ¹æ‹ 2"],
                    "improvement_priority": 1-10,
                    "estimated_improvement_time": "æ™‚é–“ã®è¦‹ç©ã‚‚ã‚Š",
                    "specific_actions": ["å…·ä½“çš„ãªè¡Œå‹•1", "å…·ä½“çš„ãªè¡Œå‹•2"]
                }}
            ],
            "strong_points": [
                {{
                    "topic": "å¾—æ„åˆ†é‡",
                    "mastery_level": "advanced/expert",
                    "can_mentor_others": true/false
                }}
            ],
            "learning_patterns": {{
                "optimal_study_time": "æ¨å¥¨å­¦ç¿’æ™‚é–“å¸¯",
                "effective_question_types": ["åŠ¹æœçš„ãªå•é¡Œã‚¿ã‚¤ãƒ—"],
                "learning_velocity": "å­¦ç¿’é€Ÿåº¦ã®ç‰¹å¾´",
                "retention_characteristics": "è¨˜æ†¶ä¿æŒã®ç‰¹å¾´"
            }},
            "personalized_strategies": [
                "å€‹äººã«ç‰¹åŒ–ã—ãŸå­¦ç¿’æˆ¦ç•¥1",
                "å€‹äººã«ç‰¹åŒ–ã—ãŸå­¦ç¿’æˆ¦ç•¥2"
            ]
        }}
        """
        return base_prompt
    
    async def run_server(self):
        """MCPã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•"""
        await self.serve()

if __name__ == "__main__":
    server = AsagamiMcpServer(
        django_api_base="http://localhost:8000",
        openai_api_key="your-openai-api-key"
    )
    asyncio.run(server.run_server())
```

## ğŸš€ å®Ÿè£…ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—

### Phase 1: åŸºç›¤æ§‹ç¯‰ (4é€±é–“)
**ç›®æ¨™**: MVPã®å®Œæˆã¨åŸºæœ¬çš„ãªå­¦ç¿’â†’ãƒ«ãƒ¼ãƒ«ç”Ÿæˆãƒ•ãƒ­ãƒ¼ã®ç¢ºç«‹

#### Week 1-2: Djangoæ‹¡å¼µ

**Step 1: æ–°è¦ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ä½œæˆ**
```bash
# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§å®Ÿè¡Œ
cd /path/to/asagami-project
python manage.py startapp cursor_integration
```

**Step 2: settings.pyã®æ›´æ–°**
```python
# mysite/settings.py
INSTALLED_APPS = [
    'django.contrib.admin',
    'django.contrib.auth',
    'django.contrib.contenttypes',
    'django.contrib.sessions',
    'django.contrib.messages',
    'django.contrib.staticfiles',
    'rest_framework',
    'corsheaders',
    'app',
    'cursor_integration',  # æ–°ã—ãè¿½åŠ 
]

# Cursorçµ±åˆç”¨ã®è¨­å®š
CURSOR_INTEGRATION = {
    'MCP_SERVER_URL': 'http://localhost:8001',
    'OPENAI_API_KEY': os.environ.get('OPENAI_API_KEY'),
    'ANALYSIS_CACHE_TIMEOUT': 3600,  # 1æ™‚é–“
    'RULE_GENERATION_TIMEOUT': 300,  # 5åˆ†
}

# Celeryè¨­å®šï¼ˆéåŒæœŸå‡¦ç†ç”¨ï¼‰
CELERY_BROKER_URL = 'redis://localhost:6379'
CELERY_RESULT_BACKEND = 'redis://localhost:6379'
```

**Step 3: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®å®Ÿè£…**
```python
# cursor_integration/models.py
from django.db import models
from django.contrib.auth import get_user_model
from django.utils import timezone
import uuid

User = get_user_model()

class LearningAnalysis(models.Model):
    """å­¦ç¿’ãƒ‡ãƒ¼ã‚¿åˆ†æçµæœã‚’ä¿å­˜"""
    
    ANALYSIS_STATUS_CHOICES = [
        ('pending', 'Pending'),
        ('processing', 'Processing'),
        ('completed', 'Completed'),
        ('failed', 'Failed'),
    ]
    
    id = models.UUIDField(primary_key=True, default=uuid.uuid4, editable=False)
    user = models.ForeignKey(User, on_delete=models.CASCADE, related_name='learning_analyses')
    analysis_date = models.DateTimeField(auto_now_add=True)
    analysis_period_days = models.IntegerField(default=30)
    status = models.CharField(max_length=20, choices=ANALYSIS_STATUS_CHOICES, default='pending')
    
    # åˆ†æçµæœãƒ‡ãƒ¼ã‚¿
    weak_points = models.JSONField(default=list, blank=True)
    strong_points = models.JSONField(default=list, blank=True)
    skill_level = models.CharField(max_length=20, blank=True)
    learning_patterns = models.JSONField(default=dict, blank=True)
    improvement_suggestions = models.JSONField(default=list, blank=True)
    
    # åˆ†æãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    confidence_score = models.FloatField(null=True, blank=True)
    data_sufficiency = models.BooleanField(default=False)
    analysis_version = models.CharField(max_length=10, default='1.0')
    
    # å‡¦ç†æ™‚é–“è¿½è·¡
    processing_started_at = models.DateTimeField(null=True, blank=True)
    processing_completed_at = models.DateTimeField(null=True, blank=True)
    
    class Meta:
        ordering = ['-analysis_date']
        indexes = [
            models.Index(fields=['user', '-analysis_date']),
            models.Index(fields=['status']),
        ]
    
    def __str__(self):
        return f"Analysis for {self.user.username} on {self.analysis_date.date()}"
    
    @property
    def processing_duration(self):
        if self.processing_started_at and self.processing_completed_at:
            return (self.processing_completed_at - self.processing_started_at).total_seconds()
        return None

class CursorRuleProfile(models.Model):
    """ç”Ÿæˆã•ã‚ŒãŸCursor Rulesãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«"""
    
    STRICTNESS_CHOICES = [
        ('lenient', 'Lenient'),
        ('intermediate', 'Intermediate'),
        ('strict', 'Strict'),
    ]
    
    id = models.UUIDField(primary_key=True, default=uuid.uuid4, editable=False)
    user = models.ForeignKey(User, on_delete=models.CASCADE, related_name='cursor_profiles')
    analysis = models.ForeignKey(LearningAnalysis, on_delete=models.CASCADE, related_name='rule_profiles')
    
    # ãƒ«ãƒ¼ãƒ«è¨­å®š
    rule_version = models.CharField(max_length=50)
    generated_at = models.DateTimeField(auto_now_add=True)
    rule_config = models.JSONField()
    code_templates = models.JSONField(default=list)
    learning_resources = models.JSONField(default=list)
    
    # ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºè¨­å®š
    strictness_level = models.CharField(max_length=20, choices=STRICTNESS_CHOICES, default='intermediate')
    focus_areas = models.JSONField(default=list)
    auto_suggestions_enabled = models.BooleanField(default=True)
    notification_level = models.CharField(max_length=20, default='moderate')
    
    # åŠ¹æœæ¸¬å®š
    effectiveness_score = models.FloatField(default=0.0)
    user_satisfaction_rating = models.IntegerField(null=True, blank=True)  # 1-5
    usage_count = models.IntegerField(default=0)
    
    # çŠ¶æ…‹ç®¡ç†
    is_active = models.BooleanField(default=True)
    deployed_at = models.DateTimeField(null=True, blank=True)
    last_updated_at = models.DateTimeField(auto_now=True)
    
    class Meta:
        ordering = ['-generated_at']
        indexes = [
            models.Index(fields=['user', '-generated_at']),
            models.Index(fields=['is_active']),
        ]
    
    def __str__(self):
        return f"Rules v{self.rule_version} for {self.user.username}"

class PracticeLog(models.Model):
    """Cursorã§ã®é–‹ç™ºå®Ÿè·µãƒ­ã‚°"""
    
    SESSION_STATUS_CHOICES = [
        ('active', 'Active'),
        ('completed', 'Completed'),
        ('interrupted', 'Interrupted'),
    ]
    
    id = models.UUIDField(primary_key=True, default=uuid.uuid4, editable=False)
    user = models.ForeignKey(User, on_delete=models.CASCADE, related_name='practice_logs')
    cursor_profile = models.ForeignKey(CursorRuleProfile, on_delete=models.SET_NULL, null=True, blank=True)
    
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³åŸºæœ¬æƒ…å ±
    session_id = models.CharField(max_length=100, unique=True)
    start_time = models.DateTimeField()
    end_time = models.DateTimeField(null=True, blank=True)
    status = models.CharField(max_length=20, choices=SESSION_STATUS_CHOICES, default='active')
    
    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæƒ…å ±
    project_name = models.CharField(max_length=200, blank=True)
    project_type = models.CharField(max_length=100, blank=True)
    development_environment = models.CharField(max_length=100, blank=True)
    cursor_version = models.CharField(max_length=50, blank=True)
    
    # é–‹ç™ºæ´»å‹•ãƒ‡ãƒ¼ã‚¿
    files_modified = models.JSONField(default=list)
    lines_added = models.IntegerField(default=0)
    lines_deleted = models.IntegerField(default=0)
    commits_made = models.IntegerField(default=0)
    
    # ã‚¨ãƒ©ãƒ¼ãƒ»å®Œäº†ãƒ‡ãƒ¼ã‚¿
    error_events = models.JSONField(default=list)
    completion_events = models.JSONField(default=list)
    rule_interactions = models.JSONField(default=list)
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    code_completion_usage = models.IntegerField(default=0)  # percentage
    auto_fix_acceptance_rate = models.FloatField(default=0.0)
    average_error_resolution_time = models.IntegerField(default=0)  # seconds
    focus_time_percentage = models.FloatField(default=0.0)
    context_switch_count = models.IntegerField(default=0)
    
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯
    session_satisfaction = models.IntegerField(null=True, blank=True)  # 1-5
    rule_helpfulness = models.IntegerField(null=True, blank=True)  # 1-5
    feedback_comments = models.TextField(blank=True)
    
    class Meta:
        ordering = ['-start_time']
        indexes = [
            models.Index(fields=['user', '-start_time']),
            models.Index(fields=['session_id']),
            models.Index(fields=['status']),
        ]
    
    def __str__(self):
        return f"Session {self.session_id} for {self.user.username}"
    
    @property
    def duration_minutes(self):
        if self.start_time and self.end_time:
            return (self.end_time - self.start_time).total_seconds() / 60
        return None

class FeedbackAnalysis(models.Model):
    """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åˆ†æçµæœ"""
    
    id = models.UUIDField(primary_key=True, default=uuid.uuid4, editable=False)
    user = models.ForeignKey(User, on_delete=models.CASCADE, related_name='feedback_analyses')
    practice_logs = models.ManyToManyField(PracticeLog, related_name='feedback_analyses')
    
    analysis_date = models.DateTimeField(auto_now_add=True)
    analysis_period_days = models.IntegerField(default=7)
    
    # åˆ†æçµæœ
    new_weak_points = models.JSONField(default=list)
    improved_areas = models.JSONField(default=list)
    error_pattern_analysis = models.JSONField(default=dict)
    rule_effectiveness_metrics = models.JSONField(default=dict)
    
    # æ”¹å–„ææ¡ˆ
    learning_recommendations = models.JSONField(default=list)
    content_update_proposals = models.JSONField(default=list)
    rule_adjustment_suggestions = models.JSONField(default=list)
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    confidence_score = models.FloatField(default=0.0)
    processed_log_count = models.IntegerField(default=0)
    
    class Meta:
        ordering = ['-analysis_date']
    
    def __str__(self):
        return f"Feedback Analysis for {self.user.username} on {self.analysis_date.date()}"
```

**Step 4: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³**
```bash
# ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
python manage.py makemigrations cursor_integration

# ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ
python manage.py migrate

# ç®¡ç†ç”»é¢ã§ã®ç¢ºèªç”¨
python manage.py createsuperuser
```

**Step 5: ç®¡ç†ç”»é¢è¨­å®š**
```python
# cursor_integration/admin.py
from django.contrib import admin
from .models import LearningAnalysis, CursorRuleProfile, PracticeLog, FeedbackAnalysis

@admin.register(LearningAnalysis)
class LearningAnalysisAdmin(admin.ModelAdmin):
    list_display = ['user', 'analysis_date', 'status', 'confidence_score', 'data_sufficiency']
    list_filter = ['status', 'analysis_date', 'data_sufficiency']
    search_fields = ['user__username', 'user__email']
    readonly_fields = ['id', 'analysis_date', 'processing_duration']
    
    fieldsets = (
        ('åŸºæœ¬æƒ…å ±', {
            'fields': ('user', 'analysis_period_days', 'status')
        }),
        ('åˆ†æçµæœ', {
            'fields': ('weak_points', 'strong_points', 'skill_level', 'learning_patterns'),
            'classes': ('collapse',)
        }),
        ('ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿', {
            'fields': ('confidence_score', 'data_sufficiency', 'analysis_version'),
        }),
        ('å‡¦ç†æ™‚é–“', {
            'fields': ('processing_started_at', 'processing_completed_at'),
            'classes': ('collapse',)
        })
    )

@admin.register(CursorRuleProfile)
class CursorRuleProfileAdmin(admin.ModelAdmin):
    list_display = ['user', 'rule_version', 'generated_at', 'is_active', 'effectiveness_score']
    list_filter = ['is_active', 'strictness_level', 'generated_at']
    search_fields = ['user__username', 'rule_version']
    
    fieldsets = (
        ('åŸºæœ¬æƒ…å ±', {
            'fields': ('user', 'analysis', 'rule_version', 'is_active')
        }),
        ('ãƒ«ãƒ¼ãƒ«è¨­å®š', {
            'fields': ('rule_config', 'strictness_level', 'focus_areas'),
            'classes': ('collapse',)
        }),
        ('åŠ¹æœæ¸¬å®š', {
            'fields': ('effectiveness_score', 'user_satisfaction_rating', 'usage_count')
        })
    )

@admin.register(PracticeLog)
class PracticeLogAdmin(admin.ModelAdmin):
    list_display = ['user', 'session_id', 'start_time', 'status', 'duration_minutes']
    list_filter = ['status', 'start_time', 'project_type']
    search_fields = ['user__username', 'session_id', 'project_name']
    readonly_fields = ['duration_minutes']

@admin.register(FeedbackAnalysis)
class FeedbackAnalysisAdmin(admin.ModelAdmin):
    list_display = ['user', 'analysis_date', 'confidence_score', 'processed_log_count']
    list_filter = ['analysis_date']
    search_fields = ['user__username']
```

#### Week 3-4: MCP ã‚µãƒ¼ãƒãƒ¼åŸºç›¤

**Step 1: MCP ã‚µãƒ¼ãƒãƒ¼ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹æˆ**
```bash
mkdir mcp_server
cd mcp_server

# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã‚’ä½œæˆ
mkdir -p {src,tests,config,templates,logs}
touch {src/__init__.py,tests/__init__.py,main.py,requirements.txt}
```

**Step 2: ä¾å­˜é–¢ä¿‚ã®è¨­å®š**
```txt
# mcp_server/requirements.txt
mcp>=0.1.0
fastapi>=0.104.0
uvicorn>=0.24.0
openai>=1.0.0
aiohttp>=3.9.0
pydantic>=2.0.0
python-multipart>=0.0.6
python-dotenv>=1.0.0
structlog>=23.0.0
prometheus-client>=0.18.0
```

**Step 3: ç’°å¢ƒè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«**
```bash
# mcp_server/.env
OPENAI_API_KEY=your_openai_api_key_here
DJANGO_API_BASE_URL=http://localhost:8000
MCP_SERVER_HOST=0.0.0.0
MCP_SERVER_PORT=8001
LOG_LEVEL=INFO
PROMETHEUS_PORT=9090

# Redisè¨­å®š
REDIS_URL=redis://localhost:6379

# åˆ†æè¨­å®š
DEFAULT_ANALYSIS_PERIOD=30
MAX_ANALYSIS_PERIOD=90
ANALYSIS_CACHE_TTL=3600
```

**Step 4: MCPã‚µãƒ¼ãƒãƒ¼ãƒ¡ã‚¤ãƒ³å®Ÿè£…**
```python
# mcp_server/main.py
import asyncio
import os
import json
import structlog
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
from dataclasses import dataclass

import aiohttp
import openai
from mcp import McpServer
from mcp.types import Tool, TextContent
from dotenv import load_dotenv

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

# ãƒ­ã‚°è¨­å®š
structlog.configure(
    processors=[
        structlog.stdlib.filter_by_level,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.stdlib.PositionalArgumentsFormatter(),
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
        structlog.processors.UnicodeDecoder(),
        structlog.processors.JSONRenderer()
    ],
    context_class=dict,
    logger_factory=structlog.stdlib.LoggerFactory(),
    wrapper_class=structlog.stdlib.BoundLogger,
    cache_logger_on_first_use=True,
)

logger = structlog.get_logger()

@dataclass
class AnalysisConfig:
    """åˆ†æè¨­å®š"""
    period_days: int = 30
    detail_level: str = "comprehensive"
    include_team_context: bool = False
    confidence_threshold: float = 0.7

@dataclass
class RuleGenerationConfig:
    """ãƒ«ãƒ¼ãƒ«ç”Ÿæˆè¨­å®š"""
    strictness_level: str = "intermediate"
    customization_level: str = "personalized"
    include_templates: bool = True
    auto_suggestions: bool = True
    focus_areas: List[str] = None

class AsagamiMcpServer(McpServer):
    """asagami AI MCP ã‚µãƒ¼ãƒãƒ¼"""
    
    def __init__(self):
        super().__init__(
            name="asagami-mcp-server",
            version="1.0.0"
        )
        
        # è¨­å®šã®åˆæœŸåŒ–
        self.django_api_base = os.getenv("DJANGO_API_BASE_URL", "http://localhost:8000")
        self.openai_client = openai.AsyncOpenAI(
            api_key=os.getenv("OPENAI_API_KEY")
        )
        
        # HTTPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        self.http_session = None
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆæœ¬ç•ªç’°å¢ƒã§ã¯Redisã‚’ä½¿ç”¨ï¼‰
        self.analysis_cache = {}
        
        logger.info("MCP Server initialized", 
                   server_name=self.name, 
                   version=self.version)
        
        # ãƒ„ãƒ¼ãƒ«ã®è¨­å®š
        self.setup_tools()
    
    async def __aenter__(self):
        """éåŒæœŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼é–‹å§‹"""
        self.http_session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=30)
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """éåŒæœŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼çµ‚äº†"""
        if self.http_session:
            await self.http_session.close()
    
    def setup_tools(self):
        """MCPãƒ„ãƒ¼ãƒ«ã®è¨­å®š"""
        
        @self.tool()
        async def analyze_learning_data(
            user_id: int,
            period_days: int = 30,
            detail_level: str = "comprehensive",
            include_team_context: bool = False
        ) -> str:
            """
            å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¼±ç‚¹ã¨å¼·ã¿ã‚’ç‰¹å®š
            
            Args:
                user_id: ãƒ¦ãƒ¼ã‚¶ãƒ¼ID
                period_days: åˆ†ææœŸé–“ï¼ˆæ—¥æ•°ï¼‰
                detail_level: è©³ç´°ãƒ¬ãƒ™ãƒ«ï¼ˆsummary/detailed/comprehensiveï¼‰
                include_team_context: ãƒãƒ¼ãƒ æ¯”è¼ƒã‚’å«ã‚€ã‹
            
            Returns:
                JSONå½¢å¼ã®åˆ†æçµæœ
            """
            try:
                logger.info("Starting learning data analysis",
                           user_id=user_id, 
                           period_days=period_days,
                           detail_level=detail_level)
                
                config = AnalysisConfig(
                    period_days=period_days,
                    detail_level=detail_level,
                    include_team_context=include_team_context
                )
                
                # Django APIã‹ã‚‰å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                learning_data = await self.fetch_learning_data(user_id, config)
                
                # AIåˆ†æã®å®Ÿè¡Œ
                analysis_result = await self.perform_ai_analysis(learning_data, config)
                
                # çµæœã®æ¤œè¨¼ã¨å¾Œå‡¦ç†
                validated_result = self.validate_analysis_result(analysis_result)
                
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
                cache_key = f"analysis_{user_id}_{period_days}_{detail_level}"
                self.analysis_cache[cache_key] = {
                    "result": validated_result,
                    "timestamp": datetime.utcnow(),
                    "expires_at": datetime.utcnow() + timedelta(hours=1)
                }
                
                logger.info("Learning data analysis completed",
                           user_id=user_id,
                           confidence_score=validated_result.get("confidence_score", 0),
                           weak_points_count=len(validated_result.get("weak_points", [])))
                
                return json.dumps(validated_result, ensure_ascii=False, indent=2)
                
            except Exception as e:
                logger.error("Learning data analysis failed",
                           user_id=user_id,
                           error=str(e),
                           exc_info=True)
                return json.dumps({
                    "error": "åˆ†æã«å¤±æ•—ã—ã¾ã—ãŸ",
                    "details": str(e),
                    "timestamp": datetime.utcnow().isoformat()
                })
        
        @self.tool()
        async def generate_cursor_rules(
            analysis_data: str,
            strictness_level: str = "intermediate",
            focus_areas: str = "",
            customization_level: str = "personalized"
        ) -> str:
            """
            å­¦ç¿’åˆ†æçµæœã‹ã‚‰Cursor Rulesã‚’ç”Ÿæˆ
            
            Args:
                analysis_data: analyze_learning_dataã®å‡ºåŠ›çµæœï¼ˆJSONæ–‡å­—åˆ—ï¼‰
                strictness_level: ãƒ«ãƒ¼ãƒ«ã®å³æ ¼åº¦ï¼ˆlenient/intermediate/strictï¼‰
                focus_areas: é‡ç‚¹ã‚¨ãƒªã‚¢ï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼‰
                customization_level: ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«ï¼ˆbasic/personalized/expertï¼‰
            
            Returns:
                JSONå½¢å¼ã®Cursor Rulesè¨­å®š
            """
            try:
                logger.info("Starting Cursor Rules generation",
                           strictness_level=strictness_level,
                           customization_level=customization_level)
                
                # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®è§£æ
                analysis_result = json.loads(analysis_data)
                focus_list = [area.strip() for area in focus_areas.split(",") if area.strip()]
                
                config = RuleGenerationConfig(
                    strictness_level=strictness_level,
                    customization_level=customization_level,
                    focus_areas=focus_list
                )
                
                # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã®æ§‹ç¯‰
                user_profile = self.build_user_profile(analysis_result)
                
                # ãƒ«ãƒ¼ãƒ«ç”Ÿæˆæˆ¦ç•¥ã®æ±ºå®š
                rule_strategy = self.determine_rule_strategy(user_profile, config)
                
                # åŸºæœ¬ãƒ«ãƒ¼ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã¨ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º
                cursor_rules = await self.generate_personalized_rules(
                    user_profile, rule_strategy, config
                )
                
                # ã‚³ãƒ¼ãƒ‰ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ç”Ÿæˆ
                code_templates = await self.generate_code_templates(user_profile)
                
                # æœ€çµ‚ãƒ«ãƒ¼ãƒ«æ§‹é€ ã®æ§‹ç¯‰
                final_rules = {
                    "metadata": {
                        "generated_at": datetime.utcnow().isoformat(),
                        "user_profile": user_profile,
                        "customization_level": customization_level,
                        "strictness_level": strictness_level,
                        "focus_areas": focus_list,
                        "rule_version": "1.0"
                    },
                    "cursor_rules": cursor_rules,
                    "code_templates": code_templates,
                    "learning_resources": self.generate_learning_resources(user_profile),
                    "effectiveness_tracking": {
                        "metrics_to_track": ["error_reduction", "code_quality", "productivity"],
                        "measurement_period": 30,
                        "success_thresholds": {
                            "error_reduction": 0.3,
                            "code_quality_improvement": 0.2,
                            "productivity_increase": 0.15
                        }
                    }
                }
                
                logger.info("Cursor Rules generation completed",
                           rule_count=len(cursor_rules),
                           template_count=len(code_templates))
                
                return json.dumps(final_rules, ensure_ascii=False, indent=2)
                
            except json.JSONDecodeError as e:
                logger.error("Invalid analysis data format", error=str(e))
                return json.dumps({
                    "error": "åˆ†æãƒ‡ãƒ¼ã‚¿ã®å½¢å¼ãŒä¸æ­£ã§ã™",
                    "details": str(e)
                })
            except Exception as e:
                logger.error("Cursor Rules generation failed", error=str(e), exc_info=True)
                return json.dumps({
                    "error": "ãƒ«ãƒ¼ãƒ«ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ",
                    "details": str(e)
                })
        
        @self.tool()
        async def process_practice_feedback(
            practice_logs_json: str,
            analysis_type: str = "comprehensive"
        ) -> str:
            """
            å®Ÿè·µãƒ­ã‚°ã‚’åˆ†æã—ã¦ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’ç”Ÿæˆ
            
            Args:
                practice_logs_json: å®Ÿè·µãƒ­ã‚°ã®JSONé…åˆ—
                analysis_type: åˆ†æã‚¿ã‚¤ãƒ—ï¼ˆquick/comprehensive/predictiveï¼‰
            
            Returns:
                JSONå½¢å¼ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åˆ†æçµæœ
            """
            try:
                logger.info("Starting practice feedback processing",
                           analysis_type=analysis_type)
                
                practice_logs = json.loads(practice_logs_json)
                
                # ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿ã®çµ±åˆã¨å‰å‡¦ç†
                consolidated_data = self.consolidate_practice_logs(practice_logs)
                
                # ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ
                error_patterns = await self.analyze_error_patterns(consolidated_data)
                
                # ã‚¹ã‚­ãƒ«ã‚®ãƒ£ãƒƒãƒ—ã®æ¤œå‡º
                skill_gaps = self.detect_skill_gaps(consolidated_data, error_patterns)
                
                # ãƒ«ãƒ¼ãƒ«åŠ¹æœã®æ¸¬å®š
                rule_effectiveness = self.measure_rule_effectiveness(consolidated_data)
                
                # å­¦ç¿’æ”¹å–„ææ¡ˆã®ç”Ÿæˆ
                learning_recommendations = await self.generate_learning_recommendations(
                    skill_gaps, error_patterns
                )
                
                feedback_result = {
                    "analysis_summary": {
                        "total_sessions": len(practice_logs),
                        "analysis_period": self.calculate_analysis_period(practice_logs),
                        "overall_trend": self.calculate_overall_trend(consolidated_data),
                        "key_insights": error_patterns.get("key_insights", [])
                    },
                    "error_pattern_analysis": error_patterns,
                    "skill_gap_analysis": skill_gaps,
                    "rule_effectiveness": rule_effectiveness,
                    "learning_recommendations": learning_recommendations,
                    "next_actions": self.generate_next_actions(skill_gaps, learning_recommendations)
                }
                
                logger.info("Practice feedback processing completed",
                           sessions_analyzed=len(practice_logs),
                           new_skill_gaps=len(skill_gaps))
                
                return json.dumps(feedback_result, ensure_ascii=False, indent=2)
                
            except Exception as e:
                logger.error("Practice feedback processing failed", error=str(e), exc_info=True)
                return json.dumps({
                    "error": "ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ",
                    "details": str(e)
                })
    
    async def fetch_learning_data(self, user_id: int, config: AnalysisConfig) -> Dict:
        """Django APIã‹ã‚‰å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        
        url = f"{self.django_api_base}/api/cursor-integration/learning-data/{user_id}"
        params = {
            "period": config.period_days,
            "detail_level": config.detail_level,
            "include_team_context": config.include_team_context
        }
        
        headers = {
            "Content-Type": "application/json",
            "X-Requested-With": "XMLHttpRequest"
        }
        
        async with self.http_session.get(url, params=params, headers=headers) as response:
            if response.status == 200:
                return await response.json()
            else:
                error_text = await response.text()
                raise Exception(f"API request failed: {response.status} - {error_text}")
    
    async def perform_ai_analysis(self, learning_data: Dict, config: AnalysisConfig) -> Dict:
        """OpenAI APIã‚’ä½¿ç”¨ã—ãŸå­¦ç¿’ãƒ‡ãƒ¼ã‚¿åˆ†æ"""
        
        analysis_prompt = self.create_detailed_analysis_prompt(learning_data, config)
        
        try:
            response = await self.openai_client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {
                        "role": "system",
                        "content": self.get_analysis_system_prompt()
                    },
                    {
                        "role": "user",
                        "content": analysis_prompt
                    }
                ],
                response_format={"type": "json_object"},
                temperature=0.3,
                max_tokens=4000
            )
            
            result = json.loads(response.choices[0].message.content)
            result["ai_model_used"] = "gpt-4"
            result["analysis_timestamp"] = datetime.utcnow().isoformat()
            
            return result
            
        except Exception as e:
            logger.error("OpenAI API call failed", error=str(e))
            raise Exception(f"AIåˆ†æã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")
    
    def get_analysis_system_prompt(self) -> str:
        """AIåˆ†æç”¨ã®ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"""
        return """
        ã‚ãªãŸã¯å­¦ç¿’ãƒ‡ãƒ¼ã‚¿åˆ†æã®å°‚é–€å®¶ã§ã™ã€‚ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°å­¦ç¿’è€…ã®å­¦ç¿’ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æã—ã€
        å…·ä½“çš„ã§å®Ÿè¡Œå¯èƒ½ãªæ”¹å–„ææ¡ˆã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚
        
        åˆ†æã®è¦³ç‚¹ï¼š
        1. å­¦ç¿’åŠ¹ç‡ã®è©•ä¾¡
        2. çŸ¥è­˜ã®ã‚®ãƒ£ãƒƒãƒ—ç‰¹å®š
        3. ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†é¡
        4. å­¦ç¿’ã‚¹ã‚¿ã‚¤ãƒ«ã®ç‰¹å®š
        5. æ”¹å–„ã®å„ªå…ˆé †ä½ä»˜ã‘
        
        å›ç­”ã¯å¿…ãšJSONå½¢å¼ã§ã€ä»¥ä¸‹ã®æ§‹é€ ã«å¾“ã£ã¦ãã ã•ã„ï¼š
        - weak_points: å¼±ç‚¹ã®è©³ç´°åˆ†æ
        - strong_points: å¼·ã¿ã®ç‰¹å®š
        - learning_patterns: å­¦ç¿’ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ
        - improvement_suggestions: å…·ä½“çš„ãªæ”¹å–„ææ¡ˆ
        - confidence_score: åˆ†æã®ä¿¡é ¼åº¦ï¼ˆ0.0-1.0ï¼‰
        """
    
    def create_detailed_analysis_prompt(self, learning_data: Dict, config: AnalysisConfig) -> str:
        """è©³ç´°ãªåˆ†æãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ"""
        
        summary = learning_data.get('summary', {})
        
        prompt = f"""
        ä»¥ä¸‹ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã¦ãã ã•ã„ï¼š

        ã€åŸºæœ¬çµ±è¨ˆã€‘
        - å­¦ç¿’æœŸé–“: {config.period_days}æ—¥é–“
        - ç·å­¦ç¿’æ™‚é–“: {summary.get('study_hours', 0)}æ™‚é–“
        - ä½œæˆãƒãƒ¼ãƒˆæ•°: {summary.get('total_notes', 0)}
        - è§£ç­”å•é¡Œæ•°: {summary.get('total_questions', 0)}
        - å¹³å‡æ­£ç­”ç‡: {summary.get('average_score', 0)}%
        - ç¾åœ¨ã®ã‚¹ã‚­ãƒ«ãƒ¬ãƒ™ãƒ«: {summary.get('skill_level', 'unknown')}

        ã€ç§‘ç›®åˆ¥æˆç¸¾ã€‘
        {json.dumps(learning_data.get('subject_scores', {}), ensure_ascii=False, indent=2)}

        ã€æœ€è¿‘ã®ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã€‘
        {json.dumps(learning_data.get('recent_errors', []), ensure_ascii=False, indent=2)}

        ã€å­¦ç¿’ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±ã€‘
        {json.dumps(learning_data.get('session_patterns', {}), ensure_ascii=False, indent=2)}

        ã€è©³ç´°ãƒ¬ãƒ™ãƒ«ã€‘: {config.detail_level}
        
        ã“ã®åˆ†æçµæœã‹ã‚‰ã€ä»¥ä¸‹ã®å½¢å¼ã§JSONã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ï¼š
        
        {{
            "weak_points": [
                {{
                    "topic": "å…·ä½“çš„ãªãƒˆãƒ”ãƒƒã‚¯å",
                    "severity": "high/medium/low",
                    "current_score": æ•°å€¤,
                    "evidence": ["å…·ä½“çš„ãªæ ¹æ‹ 1", "å…·ä½“çš„ãªæ ¹æ‹ 2"],
                    "error_frequency": æ•°å€¤,
                    "improvement_priority": 1-10,
                    "estimated_improvement_time": "æ™‚é–“ã®è¦‹ç©ã‚‚ã‚Š",
                    "specific_actions": ["å…·ä½“çš„ãªè¡Œå‹•1", "å…·ä½“çš„ãªè¡Œå‹•2"],
                    "blocking_factors": ["é˜»å®³è¦å› 1", "é˜»å®³è¦å› 2"]
                }}
            ],
            "strong_points": [
                {{
                    "topic": "å¾—æ„åˆ†é‡",
                    "mastery_level": "intermediate/advanced/expert",
                    "score": æ•°å€¤,
                    "expertise_areas": ["å°‚é–€é ˜åŸŸ1", "å°‚é–€é ˜åŸŸ2"],
                    "can_mentor_others": true/false,
                    "application_opportunities": ["æ´»ç”¨æ©Ÿä¼š1", "æ´»ç”¨æ©Ÿä¼š2"]
                }}
            ],
            "learning_patterns": {{
                "optimal_study_time": "æ¨å¥¨å­¦ç¿’æ™‚é–“å¸¯",
                "effective_session_duration": æ•°å€¤ï¼ˆåˆ†ï¼‰,
                "preferred_difficulty": "beginner/intermediate/advanced",
                "effective_question_types": ["åŠ¹æœçš„ãªå•é¡Œã‚¿ã‚¤ãƒ—"],
                "learning_velocity": "fast/normal/slow",
                "retention_characteristics": "ç†è§£åº¦ä¿æŒã®ç‰¹å¾´",
                "motivation_patterns": "ãƒ¢ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³"
            }},
            "improvement_strategies": [
                {{
                    "strategy": "æˆ¦ç•¥å",
                    "target_skills": ["å¯¾è±¡ã‚¹ã‚­ãƒ«"],
                    "implementation_steps": ["ã‚¹ãƒ†ãƒƒãƒ—1", "ã‚¹ãƒ†ãƒƒãƒ—2"],
                    "timeline": "æœŸé–“",
                    "success_metrics": ["æˆåŠŸæŒ‡æ¨™"]
                }}
            ],
            "confidence_score": 0.0-1.0ã®æ•°å€¤,
            "analysis_reliability": {{
                "data_sufficiency": true/false,
                "pattern_consistency": true/false,
                "recommendation_confidence": 0.0-1.0
            }}
        }}
        """
        
        return prompt
    
    def validate_analysis_result(self, result: Dict) -> Dict:
        """åˆ†æçµæœã®æ¤œè¨¼ã¨å“è³ªä¿è¨¼"""
        
        # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®æ¤œè¨¼
        required_fields = ["weak_points", "strong_points", "learning_patterns", "confidence_score"]
        for field in required_fields:
            if field not in result:
                result[field] = []
        
        # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã®æ¤œè¨¼
        confidence = result.get("confidence_score", 0.0)
        if not 0.0 <= confidence <= 1.0:
            result["confidence_score"] = 0.5
        
        # å¼±ç‚¹ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼ã¨è£œå®Œ
        validated_weak_points = []
        for weak_point in result.get("weak_points", []):
            if "topic" in weak_point and "severity" in weak_point:
                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã®è¨­å®š
                weak_point.setdefault("improvement_priority", 5)
                weak_point.setdefault("estimated_improvement_time", "è¦è©•ä¾¡")
                weak_point.setdefault("specific_actions", [])
                validated_weak_points.append(weak_point)
        
        result["weak_points"] = validated_weak_points
        
        # æ¤œè¨¼ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®è¿½åŠ 
        result["validation_info"] = {
            "validated_at": datetime.utcnow().isoformat(),
            "validation_version": "1.0",
            "data_quality_score": self.calculate_data_quality_score(result)
        }
        
        return result
    
    def calculate_data_quality_score(self, result: Dict) -> float:
        """ãƒ‡ãƒ¼ã‚¿å“è³ªã‚¹ã‚³ã‚¢ã®è¨ˆç®—"""
        
        score = 0.0
        max_score = 10.0
        
        # å¼±ç‚¹ãƒ‡ãƒ¼ã‚¿ã®å“è³ª
        weak_points = result.get("weak_points", [])
        if weak_points:
            score += 3.0
            if all("evidence" in wp for wp in weak_points):
                score += 1.0
            if all("specific_actions" in wp for wp in weak_points):
                score += 1.0
        
        # å¼·ã¿ãƒ‡ãƒ¼ã‚¿ã®å“è³ª
        strong_points = result.get("strong_points", [])
        if strong_points:
            score += 2.0
        
        # å­¦ç¿’ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å“è³ª
        learning_patterns = result.get("learning_patterns", {})
        if learning_patterns:
            score += 2.0
        
        # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢
        confidence = result.get("confidence_score", 0.0)
        if confidence >= 0.7:
            score += 1.0
        
        return min(score / max_score, 1.0)

    # ãã®ä»–ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰ã®å®Ÿè£…...
    # (build_user_profile, determine_rule_strategy, generate_personalized_rules ãªã©)

async def main():
    """ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
    
    # ãƒ­ã‚°ã®åˆæœŸåŒ–
    logger.info("Starting asagami MCP Server")
    
    async with AsagamiMcpServer() as server:
        # ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
        @server.app.get("/health")
        async def health_check():
            return {"status": "healthy", "timestamp": datetime.utcnow().isoformat()}
        
        # MCPã‚µãƒ¼ãƒãƒ¼ã®èµ·å‹•
        host = os.getenv("MCP_SERVER_HOST", "0.0.0.0")
        port = int(os.getenv("MCP_SERVER_PORT", 8001))
        
        logger.info("MCP Server starting", host=host, port=port)
        
        await server.serve(host=host, port=port)

if __name__ == "__main__":
    asyncio.run(main())
```

**Step 5: ã‚µãƒ¼ãƒãƒ¼èµ·å‹•ã‚¹ã‚¯ãƒªãƒ—ãƒˆ**
```bash
#!/bin/bash
# mcp_server/start_server.sh

# ä»®æƒ³ç’°å¢ƒã®ä½œæˆï¼ˆåˆå›ã®ã¿ï¼‰
if [ ! -d "venv" ]; then
    python -m venv venv
fi

# ä»®æƒ³ç’°å¢ƒã®æœ‰åŠ¹åŒ–
source venv/bin/activate

# ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install -r requirements.txt

# ç’°å¢ƒå¤‰æ•°ã®ç¢ºèª
if [ ! -f ".env" ]; then
    echo "Error: .env file not found. Please create it with required configurations."
    exit 1
fi

# ã‚µãƒ¼ãƒãƒ¼ã®èµ·å‹•
echo "Starting asagami MCP Server..."
python main.py
```

**Step 6: Django APIå®Ÿè£…ï¼ˆå­¦ç¿’ãƒ‡ãƒ¼ã‚¿åˆ†æï¼‰**
```python
# cursor_integration/views.py
from django.http import JsonResponse
from django.views.decorators.csrf import csrf_exempt
from django.contrib.auth.decorators import login_required
from django.utils.decorators import method_decorator
from django.views import View
from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework.authentication import TokenAuthentication
from rest_framework.permissions import IsAuthenticated
import json
from datetime import datetime, timedelta
from .models import LearningAnalysis, CursorRuleProfile, PracticeLog
from app.models import Note, CustomUser  # æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

class LearningDataAPIView(APIView):
    """å­¦ç¿’ãƒ‡ãƒ¼ã‚¿å–å¾—API"""
    authentication_classes = [TokenAuthentication]
    permission_classes = [IsAuthenticated]
    
    def get(self, request, user_id):
        """
        æŒ‡å®šãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        """
        try:
            # ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ¨©é™ãƒã‚§ãƒƒã‚¯
            if request.user.id != user_id and not request.user.is_staff:
                return Response({'error': 'Permission denied'}, status=403)
            
            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å–å¾—
            period_days = int(request.GET.get('period', 30))
            detail_level = request.GET.get('detail_level', 'comprehensive')
            include_team_context = request.GET.get('include_team_context', 'false').lower() == 'true'
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å­˜åœ¨ç¢ºèª
            try:
                user = CustomUser.objects.get(id=user_id)
            except CustomUser.DoesNotExist:
                return Response({'error': 'User not found'}, status=404)
            
            # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®åé›†æœŸé–“ã‚’è¨­å®š
            end_date = datetime.now()
            start_date = end_date - timedelta(days=period_days)
            
            # ãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã®å–å¾—
            notes = Note.objects.filter(
                user=user,
                created_at__gte=start_date,
                created_at__lte=end_date
            ).order_by('-created_at')
            
            # å•é¡Œè§£ç­”ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ï¼ˆæ—¢å­˜ã®asagami AIãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ï¼‰
            question_results = self.get_question_results(user, start_date, end_date)
            
            # ç§‘ç›®åˆ¥ã‚¹ã‚³ã‚¢ã®è¨ˆç®—
            subject_scores = self.calculate_subject_scores(user, start_date, end_date)
            
            # ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ
            recent_errors = self.analyze_recent_errors(user, start_date, end_date)
            
            # å­¦ç¿’ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ
            session_patterns = self.analyze_session_patterns(user, start_date, end_date)
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã®æ§‹ç¯‰
            response_data = {
                'user_id': user_id,
                'analysis_period': period_days,
                'data_collection_date': datetime.now().isoformat(),
                'summary': {
                    'total_notes': notes.count(),
                    'total_questions': question_results.get('total_count', 0),
                    'average_score': question_results.get('average_score', 0),
                    'study_hours': self.calculate_study_hours(user, start_date, end_date),
                    'skill_level': self.determine_skill_level(user),
                    'learning_streak': self.calculate_learning_streak(user)
                },
                'subject_scores': subject_scores,
                'recent_errors': recent_errors,
                'session_patterns': session_patterns,
                'note_analytics': self.analyze_note_content(notes),
                'improvement_trends': self.calculate_improvement_trends(user, period_days)
            }
            
            # ãƒãƒ¼ãƒ æ¯”è¼ƒãƒ‡ãƒ¼ã‚¿ã®è¿½åŠ 
            if include_team_context and user.department_id:
                team_data = self.get_team_comparison_data(user, start_date, end_date)
                response_data['team_context'] = team_data
            
            return Response(response_data)
            
        except Exception as e:
            return Response({
                'error': 'ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ',
                'details': str(e)
            }, status=500)
    
    def get_question_results(self, user, start_date, end_date):
        """å•é¡Œè§£ç­”çµæœã®å–å¾—ã¨åˆ†æ"""
        # æ—¢å­˜ã®asagami AIã®å•é¡Œè§£ç­”ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å–å¾—
        # ã“ã“ã§ã¯ä¾‹ã¨ã—ã¦åŸºæœ¬æ§‹é€ ã‚’ç¤ºã™
        return {
            'total_count': 150,
            'average_score': 78.5,
            'correct_answers': 118,
            'incorrect_answers': 32,
            'completion_rate': 0.89,
            'topics_covered': ['ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹', 'ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£', 'APIè¨­è¨ˆ']
        }
    
    def calculate_subject_scores(self, user, start_date, end_date):
        """ç§‘ç›®åˆ¥ã‚¹ã‚³ã‚¢ã®è¨ˆç®—"""
        # æ—¢å­˜ã®Subjectãƒ¢ãƒ‡ãƒ«ã¨ã®é€£æº
        return {
            'ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­è¨ˆ': {'score': 65, 'question_count': 20, 'improvement': -5},
            'ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£': {'score': 72, 'question_count': 15, 'improvement': +8},
            'APIé–‹ç™º': {'score': 88, 'question_count': 25, 'improvement': +12},
            'ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰': {'score': 81, 'question_count': 18, 'improvement': +3}
        }
    
    def analyze_recent_errors(self, user, start_date, end_date):
        """æœ€è¿‘ã®ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ"""
        return [
            {
                'error_type': 'concept_misunderstanding',
                'topic': 'ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ­£è¦åŒ–',
                'frequency': 8,
                'last_occurrence': '2025-07-10',
                'pattern': 'ç¬¬3æ­£è¦å½¢ã®ç†è§£ãŒä¸ååˆ†'
            },
            {
                'error_type': 'implementation_error',
                'topic': 'SQLæ–‡æ³•',
                'frequency': 5,
                'last_occurrence': '2025-07-12',
                'pattern': 'JOINå¥ã®è¨˜è¿°ãƒŸã‚¹'
            }
        ]
    
    def analyze_session_patterns(self, user, start_date, end_date):
        """å­¦ç¿’ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ"""
        return {
            'preferred_time_slots': ['09:00-11:00', '14:00-16:00'],
            'average_session_duration': 45,  # åˆ†
            'weekly_frequency': 4.2,
            'productivity_peaks': ['ç«æ›œæ—¥åˆå‰', 'æœ¨æ›œæ—¥åˆå¾Œ'],
            'attention_span': {
                'average': 35,  # åˆ†
                'declining_after': 40
            }
        }

class CursorRulesGenerationAPIView(APIView):
    """Cursor Rulesç”ŸæˆAPI"""
    authentication_classes = [TokenAuthentication]
    permission_classes = [IsAuthenticated]
    
    def post(self, request):
        """
        å­¦ç¿’åˆ†æçµæœã‹ã‚‰Cursor Rulesã‚’ç”Ÿæˆ
        """
        try:
            data = json.loads(request.body)
            
            # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼
            required_fields = ['user_id', 'analysis_data']
            for field in required_fields:
                if field not in data:
                    return Response({'error': f'{field} is required'}, status=400)
            
            user_id = data['user_id']
            analysis_data = data['analysis_data']
            
            # æ¨©é™ãƒã‚§ãƒƒã‚¯
            if request.user.id != user_id and not request.user.is_staff:
                return Response({'error': 'Permission denied'}, status=403)
            
            # ãƒ«ãƒ¼ãƒ«ç”Ÿæˆè¨­å®šã®å–å¾—
            rule_config = data.get('rule_config', {})
            strictness_level = rule_config.get('strictness_level', 'intermediate')
            focus_areas = rule_config.get('focus_areas', [])
            include_templates = rule_config.get('include_templates', True)
            
            # MCP ã‚µãƒ¼ãƒãƒ¼ã¨ã®é€£æºã§ãƒ«ãƒ¼ãƒ«ç”Ÿæˆ
            mcp_client = self.get_mcp_client()
            rules_result = await mcp_client.generate_cursor_rules(
                analysis_data=json.dumps(analysis_data),
                strictness_level=strictness_level,
                focus_areas=','.join(focus_areas),
                customization_level='personalized'
            )
            
            rules_data = json.loads(rules_result)
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
            cursor_profile = CursorRuleProfile.objects.create(
                user_id=user_id,
                rule_version=f"v{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                rule_config=rules_data['cursor_rules'],
                code_templates=rules_data.get('code_templates', []),
                learning_resources=rules_data.get('learning_resources', []),
                strictness_level=strictness_level,
                focus_areas=focus_areas,
                auto_suggestions_enabled=rule_config.get('auto_suggestions', True)
            )
            
            response_data = {
                'rule_id': str(cursor_profile.id),
                'generated_at': cursor_profile.generated_at.isoformat(),
                'cursor_rules': rules_data,
                'deployment_instructions': self.generate_deployment_instructions(rules_data),
                'expected_impact': self.calculate_expected_impact(analysis_data),
                'monitoring_setup': self.generate_monitoring_setup(user_id)
            }
            
            return Response(response_data)
            
        except Exception as e:
            return Response({
                'error': 'ãƒ«ãƒ¼ãƒ«ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ',
                'details': str(e)
            }, status=500)
    
    def generate_deployment_instructions(self, rules_data):
        """Cursorã¸ã®ãƒ‡ãƒ—ãƒ­ã‚¤æ‰‹é †ã‚’ç”Ÿæˆ"""
        return {
            'step_1': {
                'action': 'Create .cursor directory',
                'command': 'mkdir -p .cursor',
                'description': 'ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã«.cursorãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ'
            },
            'step_2': {
                'action': 'Create rules.json',
                'command': 'Create .cursor/rules.json with generated rules',
                'content': rules_data['cursor_rules'],
                'description': 'ç”Ÿæˆã•ã‚ŒãŸãƒ«ãƒ¼ãƒ«ã‚’.cursor/rules.jsonã«ä¿å­˜'
            },
            'step_3': {
                'action': 'Restart Cursor',
                'command': 'Restart your Cursor IDE',
                'description': 'ãƒ«ãƒ¼ãƒ«ã‚’é©ç”¨ã™ã‚‹ãŸã‚Cursorã‚’å†èµ·å‹•'
            },
            'verification': {
                'action': 'Test rules',
                'description': 'ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ã§ãƒ«ãƒ¼ãƒ«ãŒé©ç”¨ã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª'
            }
        }

@csrf_exempt
@login_required
def practice_logs_api(request):
    """å®Ÿè·µãƒ­ã‚°åé›†API"""
    if request.method == 'POST':
        try:
            data = json.loads(request.body)
            
            # ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼ã¨ä¿å­˜
            practice_log = PracticeLog.objects.create(
                user=request.user,
                session_id=data['session_id'],
                start_time=datetime.fromisoformat(data['session_metadata']['start_time'].replace('Z', '+00:00')),
                end_time=datetime.fromisoformat(data['session_metadata']['end_time'].replace('Z', '+00:00')),
                project_name=data['session_metadata'].get('project_name', ''),
                project_type=data['session_metadata'].get('project_type', ''),
                development_environment=data['session_metadata'].get('development_environment', ''),
                cursor_version=data['session_metadata'].get('cursor_version', ''),
                files_modified=data['development_activity'].get('files_modified', []),
                lines_added=sum(f.get('lines_added', 0) for f in data['development_activity'].get('files_modified', [])),
                lines_deleted=sum(f.get('lines_deleted', 0) for f in data['development_activity'].get('files_modified', [])),
                commits_made=len(data['development_activity'].get('commits', [])),
                error_events=data.get('error_events', []),
                completion_events=data.get('completion_events', []),
                rule_interactions=data.get('rule_interactions', []),
                code_completion_usage=data['productivity_metrics'].get('code_completion_usage', 0),
                auto_fix_acceptance_rate=data['productivity_metrics'].get('auto_fix_acceptance_rate', 0.0),
                average_error_resolution_time=data['productivity_metrics'].get('average_time_to_resolve_error', 0),
                focus_time_percentage=data['productivity_metrics'].get('focus_time_percentage', 0.0),
                context_switch_count=data['productivity_metrics'].get('context_switch_count', 0)
            )
            
            # éåŒæœŸã§ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åˆ†æã‚’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«
            from .tasks import analyze_practice_feedback
            analyze_practice_feedback.delay(practice_log.id)
            
            return JsonResponse({
                'status': 'success',
                'log_id': str(practice_log.id),
                'processing_scheduled': True,
                'next_analysis_eta': (datetime.now() + timedelta(minutes=5)).isoformat()
            })
            
        except Exception as e:
            return JsonResponse({
                'status': 'error',
                'error': str(e)
            }, status=500)
    
    return JsonResponse({'error': 'Method not allowed'}, status=405)
```

**Step 7: Celeryã‚¿ã‚¹ã‚¯ã®å®Ÿè£…**
```python
# cursor_integration/tasks.py
from celery import shared_task
from django.utils import timezone
from .models import PracticeLog, FeedbackAnalysis, LearningAnalysis
import json
import logging

logger = logging.getLogger(__name__)

@shared_task(bind=True)
def analyze_practice_feedback(self, practice_log_id):
    """
    å®Ÿè·µãƒ­ã‚°ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åˆ†æã‚¿ã‚¹ã‚¯
    """
    try:
        practice_log = PracticeLog.objects.get(id=practice_log_id)
        
        logger.info(f"Starting feedback analysis for log {practice_log_id}")
        
        # ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ
        error_analysis = analyze_error_patterns(practice_log.error_events)
        
        # ãƒ«ãƒ¼ãƒ«åŠ¹æœã®æ¸¬å®š
        rule_effectiveness = measure_rule_effectiveness(practice_log.rule_interactions)
        
        # ã‚¹ã‚­ãƒ«ã‚®ãƒ£ãƒƒãƒ—ã®æ¤œå‡º
        skill_gaps = detect_skill_gaps_from_errors(practice_log.error_events)
        
        # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åˆ†æãƒ¬ã‚³ãƒ¼ãƒ‰ã®ä½œæˆ
        feedback_analysis = FeedbackAnalysis.objects.create(
            user=practice_log.user,
            analysis_period_days=7,
            error_pattern_analysis=error_analysis,
            rule_effectiveness_metrics=rule_effectiveness,
            new_weak_points=skill_gaps,
            confidence_score=calculate_analysis_confidence(error_analysis, rule_effectiveness),
            processed_log_count=1
        )
        
        feedback_analysis.practice_logs.add(practice_log)
        
        # æ–°ã—ã„å­¦ç¿’ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ææ¡ˆãŒã‚ã‚Œã°é€šçŸ¥
        if skill_gaps:
            generate_learning_content_suggestions.delay(practice_log.user.id, skill_gaps)
        
        logger.info(f"Feedback analysis completed for log {practice_log_id}")
        
        return {
            'analysis_id': str(feedback_analysis.id),
            'new_weak_points_count': len(skill_gaps),
            'rule_effectiveness_score': rule_effectiveness.get('overall_score', 0)
        }
        
    except PracticeLog.DoesNotExist:
        logger.error(f"Practice log {practice_log_id} not found")
        raise
    except Exception as e:
        logger.error(f"Feedback analysis failed for log {practice_log_id}: {str(e)}")
        raise

@shared_task
def generate_learning_content_suggestions(user_id, skill_gaps):
    """
    æ–°ã—ã„å­¦ç¿’ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ææ¡ˆç”Ÿæˆ
    """
    try:
        # asagami AIã®æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã¨ã®é€£æº
        # æ–°ã—ã„å•é¡Œã‚„ãƒãƒ¼ãƒˆã®ç”Ÿæˆææ¡ˆ
        pass
    except Exception as e:
        logger.error(f"Learning content suggestion failed for user {user_id}: {str(e)}")

def analyze_error_patterns(error_events):
    """ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ"""
    patterns = {}
    
    for error in error_events:
        category = error.get('error_category', 'unknown')
        if category not in patterns:
            patterns[category] = {
                'frequency': 0,
                'avg_resolution_time': 0,
                'common_triggers': []
            }
        
        patterns[category]['frequency'] += 1
        patterns[category]['avg_resolution_time'] += error.get('resolution_time_seconds', 0)
    
    # å¹³å‡å€¤ã®è¨ˆç®—
    for category, data in patterns.items():
        if data['frequency'] > 0:
            data['avg_resolution_time'] /= data['frequency']
    
    return patterns

def measure_rule_effectiveness(rule_interactions):
    """ãƒ«ãƒ¼ãƒ«åŠ¹æœã®æ¸¬å®š"""
    if not rule_interactions:
        return {'overall_score': 0, 'interactions': 0}
    
    total_interactions = len(rule_interactions)
    accepted_count = sum(1 for r in rule_interactions if r.get('user_action') == 'accepted')
    avg_effectiveness = sum(r.get('effectiveness_rating', 0) for r in rule_interactions) / total_interactions
    
    return {
        'overall_score': avg_effectiveness,
        'interactions': total_interactions,
        'acceptance_rate': accepted_count / total_interactions if total_interactions > 0 else 0,
        'most_effective_rules': get_most_effective_rules(rule_interactions)
    }

def get_most_effective_rules(rule_interactions):
    """æœ€ã‚‚åŠ¹æœçš„ãªãƒ«ãƒ¼ãƒ«ã®ç‰¹å®š"""
    rule_scores = {}
    
    for interaction in rule_interactions:
        rule_id = interaction.get('rule_id')
        if rule_id:
            if rule_id not in rule_scores:
                rule_scores[rule_id] = []
            rule_scores[rule_id].append(interaction.get('effectiveness_rating', 0))
    
    # å¹³å‡ã‚¹ã‚³ã‚¢ã®è¨ˆç®—ã¨ä¸Šä½3ã¤ã‚’è¿”ã™
    avg_scores = {rule_id: sum(scores)/len(scores) for rule_id, scores in rule_scores.items()}
    return sorted(avg_scores.items(), key=lambda x: x[1], reverse=True)[:3]
```

**Step 8: URLãƒ‘ã‚¿ãƒ¼ãƒ³ã®è¨­å®š**
```python
# cursor_integration/urls.py
from django.urls import path
from . import views

app_name = 'cursor_integration'

urlpatterns = [
    # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿åˆ†æAPI
    path('learning-data/<int:user_id>/', views.LearningDataAPIView.as_view(), name='learning_data'),
    
    # Cursor Rulesç”ŸæˆAPI
    path('generate-rules/', views.CursorRulesGenerationAPIView.as_view(), name='generate_rules'),
    
    # å®Ÿè·µãƒ­ã‚°åé›†API
    path('practice-logs/', views.practice_logs_api, name='practice_logs'),
    
    # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åˆ†æAPI
    path('feedback-analysis/<int:user_id>/', views.FeedbackAnalysisAPIView.as_view(), name='feedback_analysis'),
    
    # çµ±è¨ˆãƒ»ãƒ¬ãƒãƒ¼ãƒˆAPI
    path('analytics/improvement-report/<int:user_id>/', views.ImprovementReportAPIView.as_view(), name='improvement_report'),
    path('analytics/team-stats/<int:department_id>/', views.TeamStatsAPIView.as_view(), name='team_stats'),
    
    # ãƒ«ãƒ¼ãƒ«ç®¡ç†API
    path('rules/<uuid:rule_id>/', views.RuleManagementAPIView.as_view(), name='rule_management'),
    path('rules/<uuid:rule_id>/effectiveness/', views.RuleEffectivenessAPIView.as_view(), name='rule_effectiveness'),
]
```

**Step 9: ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆURLã®æ›´æ–°**
```python
# mysite/urls.py ã«è¿½åŠ 
path('api/cursor-integration/', include('cursor_integration.urls')),
```

### Phase 2: AIåˆ†æã‚¨ãƒ³ã‚¸ãƒ³å¼·åŒ– (3é€±é–“)
**ç›®æ¨™**: é«˜ç²¾åº¦ãªå­¦ç¿’åˆ†æã¨ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸãƒ«ãƒ¼ãƒ«ç”Ÿæˆ

#### Week 5-6: åˆ†æç²¾åº¦å‘ä¸Š

**Step 1: æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®å®Ÿè£…**
```python
# mcp_server/ml_models.py
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import joblib
import json

class WeakPointDetector:
    """æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹å¼±ç‚¹æ¤œå‡ºãƒ¢ãƒ‡ãƒ«"""
    
    def __init__(self):
        self.classifier = RandomForestClassifier(n_estimators=100, random_state=42)
        self.scaler = StandardScaler()
        self.is_trained = False
    
    def prepare_features(self, learning_data):
        """å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç‰¹å¾´é‡ã‚’æº–å‚™"""
        features = []
        
        # åŸºæœ¬çµ±è¨ˆç‰¹å¾´é‡
        summary = learning_data.get('summary', {})
        features.extend([
            summary.get('study_hours', 0),
            summary.get('total_questions', 0),
            summary.get('average_score', 0),
            summary.get('learning_streak', 0)
        ])
        
        # ç§‘ç›®åˆ¥ã‚¹ã‚³ã‚¢ç‰¹å¾´é‡
        subject_scores = learning_data.get('subject_scores', {})
        for subject, data in subject_scores.items():
            features.extend([
                data.get('score', 0),
                data.get('question_count', 0),
                data.get('improvement', 0)
            ])
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³ç‰¹å¾´é‡
        session_patterns = learning_data.get('session_patterns', {})
        features.extend([
            session_patterns.get('average_session_duration', 0),
            session_patterns.get('weekly_frequency', 0),
            session_patterns.get('attention_span', {}).get('average', 0)
        ])
        
        # ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ç‰¹å¾´é‡
        recent_errors = learning_data.get('recent_errors', [])
        error_features = self.extract_error_features(recent_errors)
        features.extend(error_features)
        
        return np.array(features).reshape(1, -1)
    
    def extract_error_features(self, errors):
        """ã‚¨ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç‰¹å¾´é‡ã‚’æŠ½å‡º"""
        if not errors:
            return [0] * 10  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆç‰¹å¾´é‡
        
        # ã‚¨ãƒ©ãƒ¼é »åº¦ã€ã‚¿ã‚¤ãƒ—åˆ¥é›†è¨ˆãªã©
        total_errors = len(errors)
        concept_errors = sum(1 for e in errors if e.get('error_type') == 'concept_misunderstanding')
        implementation_errors = sum(1 for e in errors if e.get('error_type') == 'implementation_error')
        
        avg_frequency = sum(e.get('frequency', 0) for e in errors) / total_errors
        
        return [
            total_errors,
            concept_errors,
            implementation_errors,
            avg_frequency,
            concept_errors / total_errors if total_errors > 0 else 0,
            implementation_errors / total_errors if total_errors > 0 else 0,
            0, 0, 0, 0  # è¿½åŠ ç‰¹å¾´é‡ç”¨
        ]
    
    def predict_weak_points(self, learning_data):
        """å¼±ç‚¹ã‚’äºˆæ¸¬"""
        if not self.is_trained:
            # åˆæœŸãƒ¢ãƒ‡ãƒ«ã¾ãŸã¯ãƒ—ãƒªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
            return self.fallback_weak_point_detection(learning_data)
        
        features = self.prepare_features(learning_data)
        features_scaled = self.scaler.transform(features)
        
        # å¼±ç‚¹ã®äºˆæ¸¬
        weak_point_probs = self.classifier.predict_proba(features_scaled)
        
        return self.format_predictions(weak_point_probs)
    
    def fallback_weak_point_detection(self, learning_data):
        """ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¼±ç‚¹æ¤œå‡º"""
        weak_points = []
        
        # ã‚¹ã‚³ã‚¢ãƒ™ãƒ¼ã‚¹ã®å¼±ç‚¹æ¤œå‡º
        subject_scores = learning_data.get('subject_scores', {})
        for subject, data in subject_scores.items():
            score = data.get('score', 0)
            if score < 70:  # é–¾å€¤
                severity = 'high' if score < 50 else 'medium'
                weak_points.append({
                    'topic': subject,
                    'severity': severity,
                    'current_score': score,
                    'improvement_priority': 10 - (score // 10),
                    'detection_method': 'score_based'
                })
        
        # ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ™ãƒ¼ã‚¹ã®æ¤œå‡º
        recent_errors = learning_data.get('recent_errors', [])
        for error in recent_errors:
            if error.get('frequency', 0) >= 5:
                weak_points.append({
                    'topic': error.get('topic', 'Unknown'),
                    'severity': 'high',
                    'error_frequency': error.get('frequency'),
                    'improvement_priority': 8,
                    'detection_method': 'error_pattern_based'
                })
        
        return weak_points

class LearningEffectivenessPredictor:
    """å­¦ç¿’åŠ¹æœäºˆæ¸¬ãƒ¢ãƒ‡ãƒ«"""
    
    def __init__(self):
        self.regressor = GradientBoostingRegressor(n_estimators=100, random_state=42)
        self.scaler = StandardScaler()
    
    def predict_learning_outcome(self, user_profile, study_plan):
        """å­¦ç¿’è¨ˆç”»ã®åŠ¹æœã‚’äºˆæ¸¬"""
        # ç‰¹å¾´é‡ã®æº–å‚™
        features = self.prepare_prediction_features(user_profile, study_plan)
        
        # äºˆæ¸¬å®Ÿè¡Œ
        predicted_improvement = self.regressor.predict(features)
        
        return {
            'expected_score_improvement': float(predicted_improvement[0]),
            'confidence_interval': self.calculate_confidence_interval(features),
            'timeline_estimate': self.estimate_timeline(user_profile, predicted_improvement[0]),
            'success_probability': self.calculate_success_probability(features)
        }

class UserClusteringEngine:
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self):
        self.kmeans = KMeans(n_clusters=5, random_state=42)
        self.scaler = StandardScaler()
    
    def cluster_users(self, users_data):
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’ã‚¹ã‚­ãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°"""
        features_matrix = []
        
        for user_data in users_data:
            features = self.extract_user_features(user_data)
            features_matrix.append(features)
        
        features_scaled = self.scaler.fit_transform(features_matrix)
        clusters = self.kmeans.fit_predict(features_scaled)
        
        return {
            'cluster_assignments': clusters.tolist(),
            'cluster_centers': self.kmeans.cluster_centers_.tolist(),
            'cluster_characteristics': self.analyze_clusters(features_scaled, clusters)
        }

# ä½¿ç”¨ä¾‹
def enhanced_analysis_with_ml(learning_data):
    """æ©Ÿæ¢°å­¦ç¿’ã‚’ä½¿ç”¨ã—ãŸé«˜åº¦ãªåˆ†æ"""
    
    # å¼±ç‚¹æ¤œå‡º
    detector = WeakPointDetector()
    ml_weak_points = detector.predict_weak_points(learning_data)
    
    # å­¦ç¿’åŠ¹æœäºˆæ¸¬
    predictor = LearningEffectivenessPredictor()
    # effectiveness_prediction = predictor.predict_learning_outcome(user_profile, study_plan)
    
    return {
        'ml_weak_points': ml_weak_points,
        'confidence_score': 0.85,  # ML ãƒ¢ãƒ‡ãƒ«ã®ä¿¡é ¼åº¦
        'analysis_method': 'machine_learning_enhanced'
    }
```

**Step 2: A/Bãƒ†ã‚¹ãƒˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯**
```python
# mcp_server/ab_testing.py
import random
import json
from datetime import datetime, timedelta
from typing import Dict, List, Optional

class ABTestFramework:
    """A/Bãƒ†ã‚¹ãƒˆå®Ÿè¡Œãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯"""
    
    def __init__(self):
        self.active_tests = {}
        self.test_results = {}
    
    def create_test(self, test_config: Dict) -> str:
        """
        æ–°ã—ã„A/Bãƒ†ã‚¹ãƒˆã‚’ä½œæˆ
        
        Args:
            test_config: ãƒ†ã‚¹ãƒˆè¨­å®š
            {
                'name': 'rule_strictness_test',
                'description': 'ãƒ«ãƒ¼ãƒ«å³æ ¼åº¦ã®åŠ¹æœæ¸¬å®š',
                'variants': [
                    {'name': 'lenient', 'config': {'strictness': 'lenient'}},
                    {'name': 'strict', 'config': {'strictness': 'strict'}}
                ],
                'traffic_allocation': [0.5, 0.5],
                'duration_days': 14,
                'success_metrics': ['error_reduction', 'user_satisfaction'],
                'minimum_sample_size': 100
            }
        
        Returns:
            test_id: ãƒ†ã‚¹ãƒˆID
        """
        test_id = f"test_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        self.active_tests[test_id] = {
            **test_config,
            'test_id': test_id,
            'start_date': datetime.now(),
            'end_date': datetime.now() + timedelta(days=test_config['duration_days']),
            'participants': {},
            'status': 'active'
        }
        
        return test_id
    
    def assign_user_to_variant(self, test_id: str, user_id: int) -> Optional[Dict]:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’ãƒ†ã‚¹ãƒˆãƒãƒªã‚¢ãƒ³ãƒˆã«å‰²ã‚Šå½“ã¦"""
        
        if test_id not in self.active_tests:
            return None
        
        test = self.active_tests[test_id]
        
        # æ—¢ã«å‰²ã‚Šå½“ã¦æ¸ˆã¿ã®å ´åˆã¯æ—¢å­˜ã®ãƒãƒªã‚¢ãƒ³ãƒˆã‚’è¿”ã™
        if user_id in test['participants']:
            return test['participants'][user_id]
        
        # ãƒ©ãƒ³ãƒ€ãƒ å‰²ã‚Šå½“ã¦
        variants = test['variants']
        allocation = test['traffic_allocation']
        
        random_value = random.random()
        cumulative_prob = 0
        
        for i, prob in enumerate(allocation):
            cumulative_prob += prob
            if random_value <= cumulative_prob:
                selected_variant = variants[i]
                break
        else:
            selected_variant = variants[-1]  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        
        # å‚åŠ è€…æƒ…å ±ã‚’è¨˜éŒ²
        participant_info = {
            'variant': selected_variant,
            'assigned_at': datetime.now(),
            'user_id': user_id
        }
        
        test['participants'][user_id] = participant_info
        
        return participant_info
    
    def record_metric(self, test_id: str, user_id: int, metric_name: str, value: float):
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹å€¤ã‚’è¨˜éŒ²"""
        
        if test_id not in self.active_tests:
            return False
        
        test = self.active_tests[test_id]
        
        if user_id not in test['participants']:
            return False
        
        participant = test['participants'][user_id]
        
        if 'metrics' not in participant:
            participant['metrics'] = {}
        
        if metric_name not in participant['metrics']:
            participant['metrics'][metric_name] = []
        
        participant['metrics'][metric_name].append({
            'value': value,
            'timestamp': datetime.now()
        })
        
        return True
    
    def analyze_test_results(self, test_id: str) -> Dict:
        """ãƒ†ã‚¹ãƒˆçµæœã®çµ±è¨ˆåˆ†æ"""
        
        if test_id not in self.active_tests:
            return {'error': 'Test not found'}
        
        test = self.active_tests[test_id]
        participants = test['participants']
        
        # ãƒãƒªã‚¢ãƒ³ãƒˆåˆ¥ã®çµæœé›†è¨ˆ
        variant_results = {}
        
        for user_id, participant in participants.items():
            variant_name = participant['variant']['name']
            
            if variant_name not in variant_results:
                variant_results[variant_name] = {
                    'participants': 0,
                    'metrics': {}
                }
            
            variant_results[variant_name]['participants'] += 1
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®é›†è¨ˆ
            for metric_name, values in participant.get('metrics', {}).items():
                if metric_name not in variant_results[variant_name]['metrics']:
                    variant_results[variant_name]['metrics'][metric_name] = []
                
                # æœ€æ–°å€¤ã‚’ä½¿ç”¨
                if values:
                    latest_value = values[-1]['value']
                    variant_results[variant_name]['metrics'][metric_name].append(latest_value)
        
        # çµ±è¨ˆçš„æœ‰æ„æ€§ã®æ¤œå®š
        statistical_results = self.perform_statistical_tests(variant_results)
        
        return {
            'test_id': test_id,
            'test_name': test['name'],
            'status': test['status'],
            'duration': (datetime.now() - test['start_date']).days,
            'variant_results': variant_results,
            'statistical_analysis': statistical_results,
            'recommendations': self.generate_recommendations(variant_results, statistical_results)
        }
    
    def perform_statistical_tests(self, variant_results: Dict) -> Dict:
        """çµ±è¨ˆçš„æœ‰æ„æ€§ã®æ¤œå®š"""
        # ç°¡æ˜“ç‰ˆã®å®Ÿè£…ï¼ˆæœ¬æ ¼çš„ã«ã¯scipy.statsã‚’ä½¿ç”¨ï¼‰
        
        results = {}
        variants = list(variant_results.keys())
        
        if len(variants) != 2:
            return {'error': 'Only supports two-variant tests currently'}
        
        variant_a, variant_b = variants
        
        for metric_name in variant_results[variant_a].get('metrics', {}):
            if metric_name in variant_results[variant_b].get('metrics', {}):
                
                values_a = variant_results[variant_a]['metrics'][metric_name]
                values_b = variant_results[variant_b]['metrics'][metric_name]
                
                if len(values_a) >= 10 and len(values_b) >= 10:  # æœ€å°ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º
                    
                    mean_a = sum(values_a) / len(values_a)
                    mean_b = sum(values_b) / len(values_b)
                    
                    improvement = ((mean_b - mean_a) / mean_a * 100) if mean_a != 0 else 0
                    
                    results[metric_name] = {
                        'variant_a_mean': mean_a,
                        'variant_b_mean': mean_b,
                        'improvement_percentage': improvement,
                        'sample_size_a': len(values_a),
                        'sample_size_b': len(values_b),
                        'statistical_significance': abs(improvement) > 5  # ç°¡æ˜“åˆ¤å®š
                    }
        
        return results

# A/Bãƒ†ã‚¹ãƒˆã®ä½¿ç”¨ä¾‹
def setup_rule_effectiveness_test():
    """ãƒ«ãƒ¼ãƒ«åŠ¹æœã®A/Bãƒ†ã‚¹ãƒˆè¨­å®š"""
    
    ab_framework = ABTestFramework()
    
    test_config = {
        'name': 'cursor_rule_strictness_test',
        'description': 'Cursor Rulesã®å³æ ¼åº¦ã«ã‚ˆã‚‹åŠ¹æœã®é•ã„ã‚’æ¸¬å®š',
        'variants': [
            {
                'name': 'lenient_rules',
                'config': {
                    'strictness_level': 'lenient',
                    'auto_suggestions': True,
                    'notification_level': 'low'
                }
            },
            {
                'name': 'strict_rules',
                'config': {
                    'strictness_level': 'strict',
                    'auto_suggestions': True,
                    'notification_level': 'high'
                }
            }
        ],
        'traffic_allocation': [0.5, 0.5],
        'duration_days': 21,
        'success_metrics': [
            'error_reduction_rate',
            'code_quality_score',
            'user_satisfaction',
            'learning_velocity'
        ],
        'minimum_sample_size': 50
    }
    
    test_id = ab_framework.create_test(test_config)
    return test_id, ab_framework
```

#### Week 7: ãƒ«ãƒ¼ãƒ«ç”Ÿæˆã®é«˜åº¦åŒ–

**Step 1: å‹•çš„ãƒ«ãƒ¼ãƒ«èª¿æ•´æ©Ÿèƒ½**
```python
# mcp_server/dynamic_rules.py
from typing import Dict, List
import json
from datetime import datetime, timedelta

class DynamicRuleAdjuster:
    """å‹•çš„ãƒ«ãƒ¼ãƒ«èª¿æ•´ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self):
        self.adjustment_strategies = {
            'performance_based': self.performance_based_adjustment,
            'error_pattern_based': self.error_pattern_adjustment,
            'user_feedback_based': self.feedback_based_adjustment,
            'learning_progress_based': self.progress_based_adjustment
        }
    
    def adjust_rules(self, user_id: int, current_rules: Dict, 
                    performance_data: Dict, adjustment_strategy: str = 'performance_based') -> Dict:
        """
        ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦ãƒ«ãƒ¼ãƒ«ã‚’å‹•çš„èª¿æ•´
        
        Args:
            user_id: ãƒ¦ãƒ¼ã‚¶ãƒ¼ID
            current_rules: ç¾åœ¨ã®ãƒ«ãƒ¼ãƒ«è¨­å®š
            performance_data: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿
            adjustment_strategy: èª¿æ•´æˆ¦ç•¥
        
        Returns:
            èª¿æ•´ã•ã‚ŒãŸãƒ«ãƒ¼ãƒ«è¨­å®š
        """
        
        if adjustment_strategy in self.adjustment_strategies:
            adjuster = self.adjustment_strategies[adjustment_strategy]
            adjusted_rules = adjuster(current_rules, performance_data)
        else:
            adjusted_rules = current_rules
        
        # èª¿æ•´å±¥æ­´ã®è¨˜éŒ²
        adjustment_log = {
            'user_id': user_id,
            'timestamp': datetime.now().isoformat(),
            'strategy': adjustment_strategy,
            'original_rules': current_rules,
            'adjusted_rules': adjusted_rules,
            'performance_trigger': performance_data
        }
        
        self.log_adjustment(adjustment_log)
        
        return adjusted_rules
    
    def performance_based_adjustment(self, rules: Dict, performance_data: Dict) -> Dict:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ¼ã‚¹ã®èª¿æ•´"""
        
        adjusted_rules = rules.copy()
        
        # ã‚¨ãƒ©ãƒ¼ç‡ãŒé«˜ã„å ´åˆã¯ãƒ«ãƒ¼ãƒ«ã‚’å³æ ¼åŒ–
        error_rate = performance_data.get('error_rate', 0)
        if error_rate > 0.3:  # 30%ä»¥ä¸Šã®ã‚¨ãƒ©ãƒ¼ç‡
            adjusted_rules = self.increase_rule_strictness(adjusted_rules)
        
        # ã‚¨ãƒ©ãƒ¼ç‡ãŒä½ã„å ´åˆã¯æ®µéšçš„ã«ç·©å’Œ
        elif error_rate < 0.1:  # 10%æœªæº€ã®ã‚¨ãƒ©ãƒ¼ç‡
            adjusted_rules = self.decrease_rule_strictness(adjusted_rules)
        
        # ç‰¹å®šã®ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—ãŒå¤šã„å ´åˆã®ç‰¹åˆ¥å¯¾å¿œ
        frequent_errors = performance_data.get('frequent_error_types', [])
        for error_type in frequent_errors:
            adjusted_rules = self.strengthen_rules_for_error_type(adjusted_rules, error_type)
        
        return adjusted_rules
    
    def error_pattern_adjustment(self, rules: Dict, performance_data: Dict) -> Dict:
        """ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ™ãƒ¼ã‚¹ã®èª¿æ•´"""
        
        adjusted_rules = rules.copy()
        error_patterns = performance_data.get('error_patterns', {})
        
        for pattern, frequency in error_patterns.items():
            if frequency > 5:  # é »ç¹ã«ç™ºç”Ÿã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³
                # è©²å½“ã™ã‚‹ãƒ«ãƒ¼ãƒ«ã‚’å¼·åŒ–
                adjusted_rules = self.add_specific_rule_for_pattern(adjusted_rules, pattern)
        
        return adjusted_rules
    
    def feedback_based_adjustment(self, rules: Dict, performance_data: Dict) -> Dict:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ™ãƒ¼ã‚¹ã®èª¿æ•´"""
        
        adjusted_rules = rules.copy()
        user_feedback = performance_data.get('user_feedback', {})
        
        # æº€è¶³åº¦ãŒä½ã„ãƒ«ãƒ¼ãƒ«ã‚’èª¿æ•´
        low_satisfaction_rules = user_feedback.get('low_satisfaction_rules', [])
        for rule_id in low_satisfaction_rules:
            adjusted_rules = self.adjust_specific_rule(adjusted_rules, rule_id, 'reduce_severity')
        
        # é«˜è©•ä¾¡ã®ãƒ«ãƒ¼ãƒ«ã‚’ç¶­æŒãƒ»å¼·åŒ–
        high_satisfaction_rules = user_feedback.get('high_satisfaction_rules', [])
        for rule_id in high_satisfaction_rules:
            adjusted_rules = self.adjust_specific_rule(adjusted_rules, rule_id, 'maintain_or_enhance')
        
        return adjusted_rules
    
    def increase_rule_strictness(self, rules: Dict) -> Dict:
        """ãƒ«ãƒ¼ãƒ«ã®å³æ ¼åº¦ã‚’ä¸Šã’ã‚‹"""
        
        adjusted = rules.copy()
        
        for category, category_rules in adjusted.items():
            if isinstance(category_rules, dict):
                for rule_name, rule_config in category_rules.items():
                    if isinstance(rule_config, dict):
                        # severity ã‚’ä¸Šã’ã‚‹
                        if rule_config.get('severity') == 'warning':
                            rule_config['severity'] = 'error'
                        elif rule_config.get('severity') == 'info':
                            rule_config['severity'] = 'warning'
                        
                        # auto_fix ã‚’æœ‰åŠ¹åŒ–
                        rule_config['auto_fix'] = True
        
        return adjusted
    
    def add_specific_rule_for_pattern(self, rules: Dict, error_pattern: str) -> Dict:
        """ç‰¹å®šã®ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¯¾ã™ã‚‹ãƒ«ãƒ¼ãƒ«ã‚’è¿½åŠ """
        
        adjusted = rules.copy()
        
        # ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¿œã˜ãŸãƒ«ãƒ¼ãƒ«ç”Ÿæˆ
        pattern_rules = {
            'sql_injection': {
                'enabled': True,
                'severity': 'error',
                'message': 'SQLã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³å¯¾ç­–: ãƒ—ãƒªãƒšã‚¢ãƒ‰ã‚¹ãƒ†ãƒ¼ãƒˆãƒ¡ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„',
                'auto_fix': True,
                'trigger_patterns': ['SELECT.*\\+', 'INSERT.*\\+', 'UPDATE.*\\+']
            },
            'xss_vulnerability': {
                'enabled': True,
                'severity': 'error',
                'message': 'XSSå¯¾ç­–: å…¥åŠ›å€¤ã‚’ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã—ã¦ãã ã•ã„',
                'auto_fix': True,
                'trigger_patterns': ['innerHTML.*\\+', 'document.write']
            },
            'memory_leak': {
                'enabled': True,
                'severity': 'warning',
                'message': 'ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯å¯¾ç­–: ãƒªã‚½ãƒ¼ã‚¹ã®é©åˆ‡ãªè§£æ”¾ã‚’ç¢ºèªã—ã¦ãã ã•ã„',
                'auto_fix': False,
                'trigger_patterns': ['addEventListener', 'setInterval']
            }
        }
        
        if error_pattern in pattern_rules:
            if 'security' not in adjusted:
                adjusted['security'] = {}
            
            adjusted['security'][error_pattern] = pattern_rules[error_pattern]
        
        return adjusted

class ContextAwareRuleGenerator:
    """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä¾å­˜ãƒ«ãƒ¼ãƒ«ç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.context_templates = self.load_context_templates()
    
    def generate_context_rules(self, user_profile: Dict, project_context: Dict) -> Dict:
        """
        ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«å¿œã˜ãŸãƒ«ãƒ¼ãƒ«ç”Ÿæˆ
        
        Args:
            user_profile: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
            project_context: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
            {
                'project_type': 'web_application',
                'frameworks': ['django', 'react'],
                'languages': ['python', 'javascript'],
                'team_size': 5,
                'compliance_requirements': ['GDPR', 'PCI_DSS'],
                'performance_requirements': 'high',
                'security_level': 'enterprise'
            }
        
        Returns:
            ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç‰¹åŒ–ãƒ«ãƒ¼ãƒ«
        """
        
        context_rules = {}
        
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚¿ã‚¤ãƒ—ç‰¹åŒ–ãƒ«ãƒ¼ãƒ«
        project_type = project_context.get('project_type')
        if project_type in self.context_templates:
            context_rules.update(self.context_templates[project_type])
        
        # ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ç‰¹åŒ–ãƒ«ãƒ¼ãƒ«
        frameworks = project_context.get('frameworks', [])
        for framework in frameworks:
            if framework in self.context_templates.get('frameworks', {}):
                framework_rules = self.context_templates['frameworks'][framework]
                context_rules = self.merge_rules(context_rules, framework_rules)
        
        # ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹è¦ä»¶ãƒ«ãƒ¼ãƒ«
        compliance_reqs = project_context.get('compliance_requirements', [])
        for req in compliance_reqs:
            if req in self.context_templates.get('compliance', {}):
                compliance_rules = self.context_templates['compliance'][req]
                context_rules = self.merge_rules(context_rules, compliance_rules)
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¦ä»¶ãƒ«ãƒ¼ãƒ«
        perf_req = project_context.get('performance_requirements')
        if perf_req == 'high':
            perf_rules = self.context_templates.get('performance', {}).get('high', {})
            context_rules = self.merge_rules(context_rules, perf_rules)
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚¹ã‚­ãƒ«ãƒ¬ãƒ™ãƒ«ã«å¿œã˜ãŸèª¿æ•´
        skill_level = user_profile.get('skill_level', 'intermediate')
        context_rules = self.adjust_for_skill_level(context_rules, skill_level)
        
        return context_rules
    
    def load_context_templates(self) -> Dict:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã¿"""
        
        return {
            'web_application': {
                'security': {
                    'csrf_protection': {
                        'enabled': True,
                        'severity': 'error',
                        'message': 'CSRFä¿è­·ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„'
                    },
                    'https_enforcement': {
                        'enabled': True,
                        'severity': 'warning',
                        'message': 'HTTPSé€šä¿¡ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„'
                    }
                },
                'performance': {
                    'image_optimization': {
                        'enabled': True,
                        'severity': 'info',
                        'message': 'ç”»åƒã®æœ€é©åŒ–ã‚’æ¤œè¨ã—ã¦ãã ã•ã„'
                    }
                }
            },
            'frameworks': {
                'django': {
                    'django_security': {
                        'sql_injection_protection': {
                            'enabled': True,
                            'severity': 'error',
                            'message': 'Django ORMã‚’ä½¿ç”¨ã—ã¦SQLã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’é˜²ã„ã§ãã ã•ã„'
                        },
                        'template_escaping': {
                            'enabled': True,
                            'severity': 'error',
                            'message': 'ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã§ã®è‡ªå‹•ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã‚’ç¢ºèªã—ã¦ãã ã•ã„'
                        }
                    }
                },
                'react': {
                    'react_security': {
                        'jsx_xss_prevention': {
                            'enabled': True,
                            'severity': 'error',
                            'message': 'dangerouslySetInnerHTMLã®ä½¿ç”¨ã¯é¿ã‘ã¦ãã ã•ã„'
                        }
                    },
                    'react_performance': {
                        'unnecessary_renders': {
                            'enabled': True,
                            'severity': 'warning',
                            'message': 'ä¸è¦ãªå†ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã‚’é¿ã‘ã‚‹ãŸã‚ãƒ¡ãƒ¢åŒ–ã‚’æ¤œè¨ã—ã¦ãã ã•ã„'
                        }
                    }
                }
            },
            'compliance': {
                'GDPR': {
                    'data_protection': {
                        'consent_management': {
                            'enabled': True,
                            'severity': 'error',
                            'message': 'GDPRæº–æ‹ : ãƒ¦ãƒ¼ã‚¶ãƒ¼åŒæ„ã®ç®¡ç†ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„'
                        },
                        'data_minimization': {
                            'enabled': True,
                            'severity': 'warning',
                            'message': 'GDPRæº–æ‹ : å¿…è¦æœ€å°é™ã®ãƒ‡ãƒ¼ã‚¿åé›†ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„'
                        }
                    }
                },
                'PCI_DSS': {
                    'payment_security': {
                        'card_data_encryption': {
                            'enabled': True,
                            'severity': 'error',
                            'message': 'PCI DSSæº–æ‹ : ã‚«ãƒ¼ãƒ‰æƒ…å ±ã®æš—å·åŒ–ãŒå¿…è¦ã§ã™'
                        }
                    }
                }
            },
            'performance': {
                'high': {
                    'optimization': {
                        'database_queries': {
                            'enabled': True,
                            'severity': 'warning',
                            'message': 'N+1ã‚¯ã‚¨ãƒªå•é¡Œã«æ³¨æ„ã—ã¦ãã ã•ã„'
                        },
                        'caching_strategy': {
                            'enabled': True,
                            'severity': 'info',
                            'message': 'ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥ã‚’æ¤œè¨ã—ã¦ãã ã•ã„'
                        }
                    }
                }
            }
        }
```

### Phase 3: ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—å®Œæˆ (3é€±é–“)
**ç›®æ¨™**: ç¶™ç¶šçš„æ”¹å–„ã‚µã‚¤ã‚¯ãƒ«ã®è‡ªå‹•åŒ–

#### Week 8-9: å®Ÿè·µãƒ­ã‚°çµ±åˆ

**Step 1: Cursorå´ãƒ­ã‚°åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ**
```typescript
// cursor_log_collector.ts - Cursorã‚¨ã‚¯ã‚¹ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å†…ã§å®Ÿè¡Œ
import * as vscode from 'vscode';
import axios from 'axios';

interface CursorLogData {
    session_id: string;
    user_id: number;
    session_metadata: SessionMetadata;
    development_activity: DevelopmentActivity;
    error_events: ErrorEvent[];
    completion_events: CompletionEvent[];
    rule_interactions: RuleInteraction[];
    productivity_metrics: ProductivityMetrics;
}

class CursorLogCollector {
    private sessionId: string;
    private userId: number;
    private sessionStartTime: Date;
    private errorEvents: ErrorEvent[] = [];
    private completionEvents: CompletionEvent[] = [];
    private ruleInteractions: RuleInteraction[] = [];
    private filesModified: Set<string> = new Set();
    private linesAdded: number = 0;
    private linesDeleted: number = 0;
    private codeCompletionUsage: number = 0;
    private contextSwitchCount: number = 0;
    
    constructor(userId: number) {
        this.userId = userId;
        this.sessionId = `cursor_${Date.now()}_${userId}`;
        this.sessionStartTime = new Date();
        this.setupEventListeners();
    }
    
    private setupEventListeners() {
        // ã‚¨ãƒ©ãƒ¼ã‚¤ãƒ™ãƒ³ãƒˆã®ç›£è¦–
        vscode.languages.onDidChangeDiagnostics((e) => {
            this.handleDiagnosticsChange(e);
        });
        
        // ãƒ•ã‚¡ã‚¤ãƒ«ç·¨é›†ã®ç›£è¦–
        vscode.workspace.onDidChangeTextDocument((e) => {
            this.handleTextDocumentChange(e);
        });
        
        // ã‚³ãƒ¼ãƒ‰è£œå®Œã®ç›£è¦–
        vscode.languages.registerCompletionItemProvider('*', {
            provideCompletionItems: (document, position) => {
                this.recordCompletionUsage();
                return [];
            }
        });
        
        // Cursor Rulesç™ºç«ã®ç›£è¦–
        this.setupRuleInteractionListeners();
    }
    
    private handleDiagnosticsChange(e: vscode.DiagnosticChangeEvent) {
        e.uris.forEach(uri => {
            const diagnostics = vscode.languages.getDiagnostics(uri);
            diagnostics.forEach(diagnostic => {
                if (diagnostic.severity === vscode.DiagnosticSeverity.Error) {
                    this.recordErrorEvent({
                        error_type: this.categorizeError(diagnostic.message),
                        error_category: this.getErrorCategory(diagnostic.source),
                        error_message: diagnostic.message,
                        file_path: uri.fsPath,
                        line_number: diagnostic.range.start.line + 1,
                        timestamp: new Date(),
                        resolution_time_seconds: 0, // å¾Œã§æ›´æ–°
                        resolution_method: 'unknown',
                        user_satisfaction: null,
                        learning_value: 'medium'
                    });
                }
            });
        });
    }
    
    private handleTextDocumentChange(e: vscode.TextDocumentChangeEvent) {
        this.filesModified.add(e.document.fileName);
        
        e.contentChanges.forEach(change => {
            const changedLines = change.text.split('\n').length - 1;
            const deletedLines = change.rangeLength > 0 ? 
                e.document.getText(change.range).split('\n').length - 1 : 0;
            
            this.linesAdded += Math.max(0, changedLines - deletedLines);
            this.linesDeleted += Math.max(0, deletedLines - changedLines);
        });
    }
    
    private recordErrorEvent(errorEvent: ErrorEvent) {
        this.errorEvents.push(errorEvent);
    }
    
    private recordCompletionUsage() {
        this.codeCompletionUsage++;
    }
    
    private recordRuleInteraction(ruleId: string, triggered: boolean, userAction: string) {
        this.ruleInteractions.push({
            rule_id: ruleId,
            triggered_at: new Date(),
            trigger_context: this.getCurrentContext(),
            user_action: userAction,
            effectiveness_rating: this.getUserEffectivenessRating(),
            user_feedback: null
        });
    }
    
    private categorizeError(message: string): string {
        // ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰ã‚¿ã‚¤ãƒ—ã‚’åˆ†é¡
        if (message.includes('TypeError')) return 'type_error';
        if (message.includes('SyntaxError')) return 'syntax_error';
        if (message.includes('ReferenceError')) return 'reference_error';
        if (message.includes('SecurityError')) return 'security_error';
        return 'unknown_error';
    }
    
    private getErrorCategory(source: string | undefined): string {
        // ã‚¨ãƒ©ãƒ¼ã‚½ãƒ¼ã‚¹ã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªã‚’æ±ºå®š
        if (!source) return 'unknown';
        if (source.includes('typescript')) return 'language_server';
        if (source.includes('eslint')) return 'linting';
        if (source.includes('cursor')) return 'cursor_rules';
        return 'other';
    }
    
    private getCurrentContext(): string {
        const activeEditor = vscode.window.activeTextEditor;
        if (!activeEditor) return '';
        
        const position = activeEditor.selection.active;
        const line = activeEditor.document.lineAt(position.line);
        return line.text.trim();
    }
    
    private getUserEffectivenessRating(): number {
        // ç°¡æ˜“çš„ãªåŠ¹æœè©•ä¾¡ï¼ˆå®Ÿéš›ã«ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‚’æ±‚ã‚ã‚‹ï¼‰
        return Math.floor(Math.random() * 5) + 1; // 1-5ã®ç¯„å›²
    }
    
    public async endSession() {
        const sessionEndTime = new Date();
        const sessionDuration = sessionEndTime.getTime() - this.sessionStartTime.getTime();
        
        const logData: CursorLogData = {
            session_id: this.sessionId,
            user_id: this.userId,
            session_metadata: {
                start_time: this.sessionStartTime.toISOString(),
                end_time: sessionEndTime.toISOString(),
                project_name: this.getProjectName(),
                project_type: this.detectProjectType(),
                development_environment: 'cursor',
                cursor_version: this.getCursorVersion()
            },
            development_activity: {
                files_modified: Array.from(this.filesModified).map(path => ({
                    file_path: path,
                    language: this.detectLanguage(path),
                    modification_type: 'unknown',
                    lines_added: 0, // ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ã®è©³ç´°ã¯çœç•¥
                    lines_deleted: 0
                })),
                commits: [], // Gitçµ±åˆã§å–å¾—
                total_lines_added: this.linesAdded,
                total_lines_deleted: this.linesDeleted
            },
            error_events: this.errorEvents,
            completion_events: this.completionEvents,
            rule_interactions: this.ruleInteractions,
            productivity_metrics: {
                code_completion_usage: this.codeCompletionUsage,
                auto_fix_acceptance_rate: this.calculateAutoFixAcceptanceRate(),
                average_time_to_resolve_error: this.calculateAvgErrorResolutionTime(),
                focus_time_percentage: this.calculateFocusTime(sessionDuration),
                context_switch_count: this.contextSwitchCount
            }
        };
        
        // asagami AI APIã«ãƒ‡ãƒ¼ã‚¿ã‚’é€ä¿¡
        await this.sendLogData(logData);
    }
    
    private async sendLogData(logData: CursorLogData) {
        try {
            const response = await axios.post(
                'http://localhost:8000/api/cursor-integration/practice-logs/',
                logData,
                {
                    headers: {
                        'Content-Type': 'application/json',
                        'Authorization': `Bearer ${this.getAuthToken()}`
                    }
                }
            );
            
            console.log('Log data sent successfully:', response.data);
        } catch (error) {
            console.error('Failed to send log data:', error);
            // ãƒ­ãƒ¼ã‚«ãƒ«ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã«ä¿å­˜ã—ã¦ãƒªãƒˆãƒ©ã‚¤
            this.saveLogDataLocally(logData);
        }
    }
    
    private getAuthToken(): string {
        // ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®èªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—
        return vscode.workspace.getConfiguration('asagami').get('authToken', '');
    }
    
    private saveLogDataLocally(logData: CursorLogData) {
        // å¾Œã§ãƒªãƒˆãƒ©ã‚¤ã™ã‚‹ãŸã‚ã®ãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜
        const savedLogs = vscode.workspace.getConfiguration('asagami').get('pendingLogs', []);
        savedLogs.push(logData);
        vscode.workspace.getConfiguration('asagami').update('pendingLogs', savedLogs);
    }
}

// ä½¿ç”¨ä¾‹
export function activate(context: vscode.ExtensionContext) {
    const userId = getUserId(); // ãƒ¦ãƒ¼ã‚¶ãƒ¼IDã‚’å–å¾—
    const logCollector = new CursorLogCollector(userId);
    
    // ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ‚äº†æ™‚ã®å‡¦ç†
    vscode.window.onDidChangeWindowState((e) => {
        if (!e.focused) {
            logCollector.endSession();
        }
    });
}
```

**Step 2: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†æã‚¨ãƒ³ã‚¸ãƒ³**
```python
# mcp_server/realtime_analyzer.py
import asyncio
import json
import websockets
from datetime import datetime, timedelta
from typing import Dict, List, Optional
import redis
import logging

logger = logging.getLogger(__name__)

class RealtimeAnalysisEngine:
    """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†æã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self, redis_url: str = "redis://localhost:6379"):
        self.redis_client = redis.from_url(redis_url)
        self.analysis_tasks = {}
        self.active_sessions = {}
        
    async def start_session_monitoring(self, user_id: int, session_id: str):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ç›£è¦–ã®é–‹å§‹"""
        
        session_key = f"session:{user_id}:{session_id}"
        self.active_sessions[session_key] = {
            'user_id': user_id,
            'session_id': session_id,
            'start_time': datetime.now(),
            'error_count': 0,
            'completion_count': 0,
            'rule_triggers': 0,
            'last_activity': datetime.now()
        }
        
        # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†æã‚¿ã‚¹ã‚¯ã‚’é–‹å§‹
        task = asyncio.create_task(
            self.monitor_session(user_id, session_id)
        )
        self.analysis_tasks[session_key] = task
        
        logger.info(f"Started monitoring session {session_id} for user {user_id}")
    
    async def monitor_session(self, user_id: int, session_id: str):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ç¶™ç¶šç›£è¦–"""
        
        session_key = f"session:{user_id}:{session_id}"
        
        while session_key in self.active_sessions:
            try:
                # ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã®æ›´æ–°ãƒã‚§ãƒƒã‚¯
                await self.check_session_updates(user_id, session_id)
                
                # ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º
                await self.detect_anomalies(user_id, session_id)
                
                # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¨å¥¨ã®ç”Ÿæˆ
                await self.generate_realtime_suggestions(user_id, session_id)
                
                # 5ç§’é–“éš”ã§ç›£è¦–
                await asyncio.sleep(5)
                
            except Exception as e:
                logger.error(f"Error monitoring session {session_id}: {e}")
                break
    
    async def process_realtime_event(self, event_data: Dict):
        """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¤ãƒ™ãƒ³ãƒˆã®å‡¦ç†"""
        
        user_id = event_data.get('user_id')
        session_id = event_data.get('session_id')
        event_type = event_data.get('event_type')
        
        session_key = f"session:{user_id}:{session_id}"
        
        if session_key not in self.active_sessions:
            await self.start_session_monitoring(user_id, session_id)
        
        session = self.active_sessions[session_key]
        session['last_activity'] = datetime.now()
        
        # ã‚¤ãƒ™ãƒ³ãƒˆã‚¿ã‚¤ãƒ—åˆ¥ã®å‡¦ç†
        if event_type == 'error_occurred':
            await self.handle_error_event(session, event_data)
        elif event_type == 'completion_used':
            await self.handle_completion_event(session, event_data)
        elif event_type == 'rule_triggered':
            await self.handle_rule_event(session, event_data)
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’Redisã«ä¿å­˜
        await self.save_session_data(session_key, session)
    
    async def handle_error_event(self, session: Dict, event_data: Dict):
        """ã‚¨ãƒ©ãƒ¼ã‚¤ãƒ™ãƒ³ãƒˆã®å‡¦ç†"""
        
        session['error_count'] += 1
        error_category = event_data.get('error_category', 'unknown')
        
        # ã‚¨ãƒ©ãƒ¼é »åº¦ãŒé«˜ã„å ´åˆã®ã‚¢ãƒ©ãƒ¼ãƒˆ
        if session['error_count'] > 10:
            await self.trigger_intervention_alert(
                session['user_id'],
                'high_error_frequency',
                f"çŸ­æ™‚é–“ã§{session['error_count']}å€‹ã®ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã„ã¾ã™"
            )
        
        # ç‰¹å®šã®ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º
        if error_category == 'security_error':
            await self.trigger_security_alert(
                session['user_id'],
                event_data.get('error_message', '')
            )
    
    async def handle_completion_event(self, session: Dict, event_data: Dict):
        """ã‚³ãƒ¼ãƒ‰è£œå®Œã‚¤ãƒ™ãƒ³ãƒˆã®å‡¦ç†"""
        
        session['completion_count'] += 1
        completion_type = event_data.get('completion_type', 'unknown')
        
        # è£œå®Œä½¿ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ
        if completion_type == 'security_template':
            await self.record_positive_behavior(
                session['user_id'],
                'security_awareness',
                'ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ä½¿ç”¨'
            )
    
    async def handle_rule_event(self, session: Dict, event_data: Dict):
        """ãƒ«ãƒ¼ãƒ«ç™ºç«ã‚¤ãƒ™ãƒ³ãƒˆã®å‡¦ç†"""
        
        session['rule_triggers'] += 1
        rule_id = event_data.get('rule_id')
        user_action = event_data.get('user_action')
        
        # ãƒ«ãƒ¼ãƒ«åŠ¹æœã®å³åº§ã®è©•ä¾¡
        if user_action == 'accepted':
            await self.record_rule_effectiveness(
                session['user_id'],
                rule_id,
                'positive'
            )
        elif user_action == 'dismissed':
            await self.record_rule_effectiveness(
                session['user_id'],
                rule_id,
                'negative'
            )
    
    async def detect_anomalies(self, user_id: int, session_id: str):
        """ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º"""
        
        session_key = f"session:{user_id}:{session_id}"
        session = self.active_sessions.get(session_key)
        
        if not session:
            return
        
        current_time = datetime.now()
        session_duration = (current_time - session['start_time']).total_seconds() / 60
        
        # é•·æ™‚é–“ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®æ¤œå‡º
        if session_duration > 120:  # 2æ™‚é–“ä»¥ä¸Š
            await self.suggest_break(user_id, session_duration)
        
        # é«˜ã‚¨ãƒ©ãƒ¼ç‡ã®æ¤œå‡º
        if session_duration > 30:  # 30åˆ†ä»¥ä¸Šã®ã‚»ãƒƒã‚·ãƒ§ãƒ³
            error_rate = session['error_count'] / session_duration
            if error_rate > 0.5:  # 1åˆ†ã‚ãŸã‚Š0.5å€‹ä»¥ä¸Šã®ã‚¨ãƒ©ãƒ¼
                await self.suggest_learning_content(
                    user_id,
                    'high_error_rate',
                    session['error_count']
                )
    
    async def generate_realtime_suggestions(self, user_id: int, session_id: str):
        """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¨å¥¨ã®ç”Ÿæˆ"""
        
        session_key = f"session:{user_id}:{session_id}"
        session = self.active_sessions.get(session_key)
        
        if not session:
            return
        
        # æœ€è¿‘ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æ
        recent_data = await self.get_recent_performance_data(user_id)
        
        if recent_data:
            suggestions = await self.analyze_and_suggest(user_id, recent_data)
            
            if suggestions:
                await self.send_realtime_suggestions(user_id, suggestions)
    
    async def trigger_intervention_alert(self, user_id: int, alert_type: str, message: str):
        """ä»‹å…¥ã‚¢ãƒ©ãƒ¼ãƒˆã®ç™ºç«"""
        
        alert_data = {
            'user_id': user_id,
            'alert_type': alert_type,
            'message': message,
            'timestamp': datetime.now().isoformat(),
            'suggested_actions': self.get_intervention_actions(alert_type)
        }
        
        # WebSocketçµŒç”±ã§ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€šçŸ¥
        await self.send_websocket_notification(user_id, alert_data)
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ãƒ­ã‚°è¨˜éŒ²
        await self.log_intervention_alert(alert_data)
    
    def get_intervention_actions(self, alert_type: str) -> List[str]:
        """ä»‹å…¥ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®å–å¾—"""
        
        actions = {
            'high_error_frequency': [
                '5åˆ†é–“ã®ä¼‘æ†©ã‚’å–ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™',
                'åŸºæœ¬æ¦‚å¿µã®å¾©ç¿’ã‚’ã—ã¦ã¿ã¦ãã ã•ã„',
                'ã‚ˆã‚Šç°¡å˜ãªå•é¡Œã‹ã‚‰å§‹ã‚ã‚‹ã“ã¨ã‚’æ¤œè¨ã—ã¦ãã ã•ã„'
            ],
            'long_session': [
                'ä¼‘æ†©ã‚’å–ã£ã¦é›†ä¸­åŠ›ã‚’å›å¾©ã—ã¦ãã ã•ã„',
                'æ˜æ—¥ç¶šãã‚’è¡Œã†ã“ã¨ã‚’æ¤œè¨ã—ã¦ãã ã•ã„',
                'é©åº¦ãªé‹å‹•ã§é ­ã‚’ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ã—ã¦ãã ã•ã„'
            ],
            'security_concern': [
                'ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„',
                'ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£é–¢é€£ã®å­¦ç¿’ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ¨å¥¨ã—ã¾ã™',
                'ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’ä¾é ¼ã™ã‚‹ã“ã¨ã‚’æ¤œè¨ã—ã¦ãã ã•ã„'
            ]
        }
        
        return actions.get(alert_type, ['å°‚é–€å®¶ã«ç›¸è«‡ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™'])

class WebSocketManager:
    """WebSocketæ¥ç¶šç®¡ç†"""
    
    def __init__(self):
        self.connections = {}  # user_id -> websocket
    
    async def register_connection(self, user_id: int, websocket):
        """WebSocketæ¥ç¶šã®ç™»éŒ²"""
        self.connections[user_id] = websocket
        logger.info(f"WebSocket registered for user {user_id}")
    
    async def unregister_connection(self, user_id: int):
        """WebSocketæ¥ç¶šã®å‰Šé™¤"""
        if user_id in self.connections:
            del self.connections[user_id]
            logger.info(f"WebSocket unregistered for user {user_id}")
    
    async def send_notification(self, user_id: int, notification: Dict):
        """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€šçŸ¥ã®é€ä¿¡"""
        
        if user_id in self.connections:
            websocket = self.connections[user_id]
            try:
                await websocket.send(json.dumps(notification))
            except Exception as e:
                logger.error(f"Failed to send notification to user {user_id}: {e}")
                await self.unregister_connection(user_id)

# WebSocketã‚µãƒ¼ãƒãƒ¼ã®èµ·å‹•
async def websocket_handler(websocket, path):
    """WebSocketãƒãƒ³ãƒ‰ãƒ©ãƒ¼"""
    
    try:
        # èªè¨¼
        auth_message = await websocket.recv()
        auth_data = json.loads(auth_message)
        user_id = auth_data.get('user_id')
        
        if not user_id:
            await websocket.close(code=4001, reason="Invalid authentication")
            return
        
        # æ¥ç¶šã‚’ç™»éŒ²
        ws_manager = WebSocketManager()
        await ws_manager.register_connection(user_id, websocket)
        
        # æ¥ç¶šç¶­æŒ
        async for message in websocket:
            # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¤ãƒ™ãƒ³ãƒˆã®å‡¦ç†
            event_data = json.loads(message)
            await realtime_analyzer.process_realtime_event(event_data)
            
    except websockets.exceptions.ConnectionClosed:
        logger.info(f"WebSocket connection closed for user {user_id}")
    except Exception as e:
        logger.error(f"WebSocket error: {e}")
    finally:
        if 'user_id' in locals():
            await ws_manager.unregister_connection(user_id)

# ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†æã‚¨ãƒ³ã‚¸ãƒ³ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
realtime_analyzer = RealtimeAnalysisEngine()

# WebSocketã‚µãƒ¼ãƒãƒ¼ã®èµ·å‹•
async def start_websocket_server():
    """WebSocketã‚µãƒ¼ãƒãƒ¼ã®èµ·å‹•"""
    
    logger.info("Starting WebSocket server on port 8765")
    await websockets.serve(websocket_handler, "localhost", 8765)
```

**Step 3: è‡ªå‹•ãƒ«ãƒ¼ãƒ«æ›´æ–°ã‚·ã‚¹ãƒ†ãƒ **
```python
# mcp_server/auto_rule_updater.py
from typing import Dict, List
import asyncio
from datetime import datetime, timedelta
import json
import logging

logger = logging.getLogger(__name__)

class AutomaticRuleUpdater:
    """è‡ªå‹•ãƒ«ãƒ¼ãƒ«æ›´æ–°ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, update_threshold: float = 0.3):
        self.update_threshold = update_threshold  # æ›´æ–°ã®é–¾å€¤
        self.pending_updates = {}
        self.update_history = {}
    
    async def evaluate_rule_updates(self, user_id: int):
        """ãƒ«ãƒ¼ãƒ«æ›´æ–°ã®è©•ä¾¡"""
        
        # æœ€è¿‘ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        performance_data = await self.get_recent_performance(user_id)
        current_rules = await self.get_current_rules(user_id)
        
        if not performance_data or not current_rules:
            return
        
        # æ›´æ–°å€™è£œã®è©•ä¾¡
        update_candidates = await self.identify_update_candidates(
            user_id, performance_data, current_rules
        )
        
        for candidate in update_candidates:
            await self.process_update_candidate(user_id, candidate)
    
    async def identify_update_candidates(self, user_id: int, 
                                       performance_data: Dict, 
                                       current_rules: Dict) -> List[Dict]:
        """æ›´æ–°å€™è£œã®ç‰¹å®š"""
        
        candidates = []
        
        # 1. åŠ¹æœãŒä½ã„ãƒ«ãƒ¼ãƒ«ã®ç‰¹å®š
        ineffective_rules = self.find_ineffective_rules(
            performance_data.get('rule_effectiveness', {})
        )
        
        for rule_id in ineffective_rules:
            candidates.append({
                'type': 'effectiveness_improvement',
                'rule_id': rule_id,
                'current_effectiveness': ineffective_rules[rule_id],
                'suggested_action': 'adjust_or_disable'
            })
        
        # 2. æ–°ã—ã„ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¯¾ã™ã‚‹ãƒ«ãƒ¼ãƒ«è¿½åŠ 
        new_error_patterns = self.detect_new_error_patterns(
            performance_data.get('error_patterns', {})
        )
        
        for pattern in new_error_patterns:
            candidates.append({
                'type': 'new_rule_addition',
                'error_pattern': pattern,
                'frequency': new_error_patterns[pattern],
                'suggested_action': 'add_rule'
            })
        
        # 3. ã‚¹ã‚­ãƒ«å‘ä¸Šã«ä¼´ã†ãƒ«ãƒ¼ãƒ«èª¿æ•´
        skill_improvements = self.detect_skill_improvements(
            performance_data.get('skill_progress', {})
        )
        
        for skill, improvement in skill_improvements.items():
            if improvement > self.update_threshold:
                candidates.append({
                    'type': 'skill_based_adjustment',
                    'skill': skill,
                    'improvement': improvement,
                    'suggested_action': 'reduce_assistance'
                })
        
        return candidates
    
    def find_ineffective_rules(self, rule_effectiveness: Dict) -> Dict:
        """åŠ¹æœãŒä½ã„ãƒ«ãƒ¼ãƒ«ã‚’ç‰¹å®š"""
        
        ineffective = {}
        
        for rule_id, effectiveness in rule_effectiveness.items():
            avg_score = effectiveness.get('overall_score', 0)
            acceptance_rate = effectiveness.get('acceptance_rate', 0)
            
            # åŠ¹æœã‚¹ã‚³ã‚¢ãŒä½ã„ã€ã¾ãŸã¯å—ã‘å…¥ã‚Œç‡ãŒä½ã„
            if avg_score < 2.0 or acceptance_rate < 0.3:
                ineffective[rule_id] = {
                    'score': avg_score,
                    'acceptance_rate': acceptance_rate,
                    'interactions': effectiveness.get('interactions', 0)
                }
        
        return ineffective
    
    def detect_new_error_patterns(self, error_patterns: Dict) -> Dict:
        """æ–°ã—ã„ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º"""
        
        new_patterns = {}
        
        for pattern, data in error_patterns.items():
            frequency = data.get('frequency', 0)
            
            # é »åº¦ãŒé«˜ãã€ã¾ã ãƒ«ãƒ¼ãƒ«ãŒå­˜åœ¨ã—ãªã„ãƒ‘ã‚¿ãƒ¼ãƒ³
            if frequency >= 5 and not self.rule_exists_for_pattern(pattern):
                new_patterns[pattern] = frequency
        
        return new_patterns
    
    def detect_skill_improvements(self, skill_progress: Dict) -> Dict:
        """ã‚¹ã‚­ãƒ«å‘ä¸Šã‚’æ¤œå‡º"""
        
        improvements = {}
        
        for skill, progress in skill_progress.items():
            current_score = progress.get('current_score', 0)
            previous_score = progress.get('previous_score', 0)
            
            if previous_score > 0:
                improvement_rate = (current_score - previous_score) / previous_score
                if improvement_rate > 0.2:  # 20%ä»¥ä¸Šã®å‘ä¸Š
                    improvements[skill] = improvement_rate
        
        return improvements
    
    async def process_update_candidate(self, user_id: int, candidate: Dict):
        """æ›´æ–°å€™è£œã®å‡¦ç†"""
        
        candidate_type = candidate['type']
        
        if candidate_type == 'effectiveness_improvement':
            await self.handle_effectiveness_improvement(user_id, candidate)
        elif candidate_type == 'new_rule_addition':
            await self.handle_new_rule_addition(user_id, candidate)
        elif candidate_type == 'skill_based_adjustment':
            await self.handle_skill_based_adjustment(user_id, candidate)
    
    async def handle_effectiveness_improvement(self, user_id: int, candidate: Dict):
        """åŠ¹æœæ”¹å–„ã®å‡¦ç†"""
        
        rule_id = candidate['rule_id']
        current_effectiveness = candidate['current_effectiveness']
        
        # A/Bãƒ†ã‚¹ãƒˆã§æ”¹å–„æ¡ˆã‚’ãƒ†ã‚¹ãƒˆ
        improvement_variants = await self.generate_improvement_variants(
            rule_id, current_effectiveness
        )
        
        if improvement_variants:
            await self.schedule_ab_test(
                user_id,
                rule_id,
                improvement_variants,
                'effectiveness_improvement'
            )
    
    async def handle_new_rule_addition(self, user_id: int, candidate: Dict):
        """æ–°è¦ãƒ«ãƒ¼ãƒ«è¿½åŠ ã®å‡¦ç†"""
        
        error_pattern = candidate['error_pattern']
        frequency = candidate['frequency']
        
        # æ–°ã—ã„ãƒ«ãƒ¼ãƒ«ã‚’ç”Ÿæˆ
        new_rule = await self.generate_rule_for_pattern(error_pattern, frequency)
        
        if new_rule:
            # æ®µéšçš„å°å…¥ã§ãƒ†ã‚¹ãƒˆ
            await self.schedule_gradual_rollout(
                user_id,
                new_rule,
                'new_rule_addition'
            )
    
    async def handle_skill_based_adjustment(self, user_id: int, candidate: Dict):
        """ã‚¹ã‚­ãƒ«ãƒ™ãƒ¼ã‚¹èª¿æ•´ã®å‡¦ç†"""
        
        skill = candidate['skill']
        improvement = candidate['improvement']
        
        # ã‚¹ã‚­ãƒ«å‘ä¸Šã«å¿œã˜ãŸãƒ«ãƒ¼ãƒ«èª¿æ•´
        adjusted_rules = await self.adjust_rules_for_skill_improvement(
            user_id, skill, improvement
        )
        
        if adjusted_rules:
            await self.apply_rule_adjustments(user_id, adjusted_rules)
    
    async def generate_improvement_variants(self, rule_id: str, 
                                         current_effectiveness: Dict) -> List[Dict]:
        """æ”¹å–„ãƒãƒªã‚¢ãƒ³ãƒˆã®ç”Ÿæˆ"""
        
        variants = []
        
        # ç¾åœ¨ã®å•é¡Œã‚’åˆ†æ
        low_score = current_effectiveness.get('score', 0) < 2.0
        low_acceptance = current_effectiveness.get('acceptance_rate', 0) < 0.3
        
        if low_score:
            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®æ”¹å–„
            variants.append({
                'type': 'message_improvement',
                'changes': {
                    'message_tone': 'more_helpful',
                    'include_examples': True,
                    'add_learning_links': True
                }
            })
        
        if low_acceptance:
            # å³æ ¼åº¦ã®èª¿æ•´
            variants.append({
                'type': 'strictness_adjustment',
                'changes': {
                    'severity': 'reduce',
                    'auto_fix': 'enable',
                    'notification_frequency': 'reduce'
                }
            })
        
        return variants
    
    async def generate_rule_for_pattern(self, error_pattern: str, 
                                      frequency: int) -> Optional[Dict]:
        """ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¯¾ã™ã‚‹ãƒ«ãƒ¼ãƒ«ç”Ÿæˆ"""
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥ã®ãƒ«ãƒ¼ãƒ«ç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯
        rule_templates = {
            'undefined_variable': {
                'severity': 'error',
                'message': 'æœªå®šç¾©ã®å¤‰æ•°ãŒä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚å¤‰æ•°ã®å®£è¨€ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚',
                'auto_fix': True,
                'trigger_patterns': ['ReferenceError.*is not defined']
            },
            'async_await_misuse': {
                'severity': 'warning',
                'message': 'async/awaitã®ä½¿ç”¨æ–¹æ³•ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚',
                'auto_fix': False,
                'trigger_patterns': ['await.*non-async', 'async.*without-await']
            },
            'memory_leak_risk': {
                'severity': 'info',
                'message': 'ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ãƒªã‚½ãƒ¼ã‚¹ã®è§£æ”¾ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚',
                'auto_fix': False,
                'trigger_patterns': ['addEventListener.*without.*removeEventListener']
            }
        }
        
        if error_pattern in rule_templates:
            rule = rule_templates[error_pattern].copy()
            rule['pattern'] = error_pattern
            rule['frequency_based_priority'] = min(frequency / 10, 1.0)
            return rule
        
        return None
    
    async def schedule_ab_test(self, user_id: int, rule_id: str, 
                             variants: List[Dict], test_type: str):
        """A/Bãƒ†ã‚¹ãƒˆã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«"""
        
        test_config = {
            'name': f'{test_type}_{rule_id}_{datetime.now().strftime("%Y%m%d")}',
            'description': f'Rule effectiveness improvement test for {rule_id}',
            'variants': [
                {'name': 'current', 'config': {'rule_id': rule_id, 'changes': {}}},
                *[{'name': f'variant_{i}', 'config': variant} for i, variant in enumerate(variants)]
            ],
            'traffic_allocation': [0.5] + [0.5 / len(variants)] * len(variants),
            'duration_days': 7,
            'success_metrics': ['effectiveness_score', 'acceptance_rate', 'error_reduction'],
            'user_filter': {'user_ids': [user_id]}
        }
        
        from .ab_testing import ABTestFramework
        ab_framework = ABTestFramework()
        test_id = ab_framework.create_test(test_config)
        
        logger.info(f"Scheduled A/B test {test_id} for rule {rule_id}")
        
        return test_id
    
    async def apply_rule_adjustments(self, user_id: int, adjusted_rules: Dict):
        """ãƒ«ãƒ¼ãƒ«èª¿æ•´ã®é©ç”¨"""
        
        # ç¾åœ¨ã®ãƒ«ãƒ¼ãƒ«ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
        current_profile = await self.get_current_rule_profile(user_id)
        
        if not current_profile:
            return
        
        # ãƒ«ãƒ¼ãƒ«ã‚’æ›´æ–°
        updated_rules = current_profile['rule_config'].copy()
        updated_rules.update(adjusted_rules)
        
        # æ–°ã—ã„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ä½œæˆ
        new_version = f"v{datetime.now().strftime('%Y%m%d_%H%M%S')}_auto"
        
        await self.create_new_rule_version(
            user_id,
            new_version,
            updated_rules,
            'automatic_adjustment'
        )
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«é€šçŸ¥
        await self.notify_rule_update(user_id, {
            'type': 'automatic_adjustment',
            'version': new_version,
            'changes_summary': self.summarize_changes(
                current_profile['rule_config'],
                updated_rules
            )
        })
        
        logger.info(f"Applied automatic rule adjustments for user {user_id}")

# è‡ªå‹•æ›´æ–°ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼
class AutoUpdateScheduler:
    """è‡ªå‹•æ›´æ–°ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼"""
    
    def __init__(self, update_interval_hours: int = 24):
        self.update_interval = timedelta(hours=update_interval_hours)
        self.updater = AutomaticRuleUpdater()
        self.running = False
    
    async def start(self):
        """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã®é–‹å§‹"""
        
        self.running = True
        logger.info("Auto update scheduler started")
        
        while self.running:
            try:
                # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒªã‚¹ãƒˆã‚’å–å¾—
                active_users = await self.get_active_users()
                
                # å„ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ«ãƒ¼ãƒ«æ›´æ–°ã‚’è©•ä¾¡
                for user_id in active_users:
                    await self.updater.evaluate_rule_updates(user_id)
                
                # æ¬¡ã®å®Ÿè¡Œã¾ã§å¾…æ©Ÿ
                await asyncio.sleep(self.update_interval.total_seconds())
                
            except Exception as e:
                logger.error(f"Error in auto update scheduler: {e}")
                await asyncio.sleep(3600)  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯1æ™‚é–“å¾…æ©Ÿ
    
    def stop(self):
        """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã®åœæ­¢"""
        self.running = False
        logger.info("Auto update scheduler stopped")
    
    async def get_active_users(self) -> List[int]:
        """ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒªã‚¹ãƒˆå–å¾—"""
        # éå»24æ™‚é–“ä»¥å†…ã«ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ãŒã‚ã£ãŸãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’å–å¾—
        # å®Ÿè£…ã¯Djangoã®ãƒ¢ãƒ‡ãƒ«ã«ä¾å­˜
        return []  # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼

# ä½¿ç”¨ä¾‹
async def start_auto_update_system():
    """è‡ªå‹•æ›´æ–°ã‚·ã‚¹ãƒ†ãƒ ã®é–‹å§‹"""
    
    scheduler = AutoUpdateScheduler(update_interval_hours=24)
    await scheduler.start()
```

#### Week 10: è‡ªå‹•æ”¹å–„ã‚·ã‚¹ãƒ†ãƒ 

**Step 1: äºˆæ¸¬çš„å¼±ç‚¹æ¤œå‡º**
```python
# mcp_server/predictive_analysis.py
import numpy as np
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional

class PredictiveWeaknessDetector:
    """äºˆæ¸¬çš„å¼±ç‚¹æ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.anomaly_detector = IsolationForest(contamination=0.1, random_state=42)
        self.scaler = StandardScaler()
        self.is_trained = False
        self.feature_names = []
    
    def prepare_time_series_features(self, user_data: List[Dict]) -> np.ndarray:
        """æ™‚ç³»åˆ—ç‰¹å¾´é‡ã®æº–å‚™"""
        
        features_list = []
        
        for data_point in user_data:
            features = []
            
            # åŸºæœ¬ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç‰¹å¾´é‡
            features.extend([
                data_point.get('score', 0),
                data_point.get('completion_time', 0),
                data_point.get('error_count', 0),
                data_point.get('help_usage', 0)
            ])
            
            # å­¦ç¿’ãƒ‘ã‚¿ãƒ¼ãƒ³ç‰¹å¾´é‡
            learning_pattern = data_point.get('learning_pattern', {})
            features.extend([
                learning_pattern.get('session_duration', 0),
                learning_pattern.get('focus_level', 0),
                learning_pattern.get('difficulty_preference', 0)
            ])
            
            # æ™‚é–“çš„ç‰¹å¾´é‡
            timestamp = datetime.fromisoformat(data_point.get('timestamp'))
            features.extend([
                timestamp.hour,  # æ™‚é–“å¸¯
                timestamp.weekday(),  # æ›œæ—¥
                (datetime.now() - timestamp).days  # çµŒéæ—¥æ•°
            ])
            
            # ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ç‰¹å¾´é‡
            error_patterns = data_point.get('error_patterns', {})
            features.extend([
                error_patterns.get('syntax_errors', 0),
                error_patterns.get('logic_errors', 0),
                error_patterns.get('runtime_errors', 0)
            ])
            
            features_list.append(features)
        
        return np.array(features_list)
    
    def train_anomaly_detector(self, historical_data: List[Dict]):
        """ç•°å¸¸æ¤œå‡ºãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´"""
        
        if len(historical_data) < 50:  # æœ€å°ãƒ‡ãƒ¼ã‚¿æ•°
            return False
        
        features = self.prepare_time_series_features(historical_data)
        features_scaled = self.scaler.fit_transform(features)
        
        self.anomaly_detector.fit(features_scaled)
        self.is_trained = True
        
        return True
    
    def predict_potential_weaknesses(self, recent_data: List[Dict]) -> Dict:
        """æ½œåœ¨çš„å¼±ç‚¹ã®äºˆæ¸¬"""
        
        if not self.is_trained or len(recent_data) < 5:
            return self.fallback_prediction(recent_data)
        
        features = self.prepare_time_series_features(recent_data)
        features_scaled = self.scaler.transform(features)
        
        # ç•°å¸¸ã‚¹ã‚³ã‚¢ã®è¨ˆç®—
        anomaly_scores = self.anomaly_detector.decision_function(features_scaled)
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
        trend_analysis = self.analyze_trends(recent_data)
        
        # äºˆæ¸¬çµæœã®çµ±åˆ
        predictions = self.integrate_predictions(
            recent_data, anomaly_scores, trend_analysis
        )
        
        return predictions
    
    def analyze_trends(self, data: List[Dict]) -> Dict:
        """ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ"""
        
        if len(data) < 3:
            return {}
        
        # ã‚¹ã‚³ã‚¢ãƒˆãƒ¬ãƒ³ãƒ‰
        scores = [d.get('score', 0) for d in data]
        score_trend = self.calculate_trend(scores)
        
        # ã‚¨ãƒ©ãƒ¼ãƒˆãƒ¬ãƒ³ãƒ‰
        errors = [d.get('error_count', 0) for d in data]
        error_trend = self.calculate_trend(errors)
        
        # å®Œäº†æ™‚é–“ãƒˆãƒ¬ãƒ³ãƒ‰
        completion_times = [d.get('completion_time', 0) for d in data]
        time_trend = self.calculate_trend(completion_times)
        
        return {
            'score_trend': score_trend,
            'error_trend': error_trend,
            'time_trend': time_trend,
            'overall_trend': self.calculate_overall_trend([score_trend, -error_trend, -time_trend])
        }
    
    def calculate_trend(self, values: List[float]) -> float:
        """ç·šå½¢ãƒˆãƒ¬ãƒ³ãƒ‰ã®è¨ˆç®—"""
        
        if len(values) < 2:
            return 0.0
        
        x = np.arange(len(values))
        y = np.array(values)
        
        # ç·šå½¢å›å¸°ã®å‚¾ã
        slope = np.polyfit(x, y, 1)[0]
        return float(slope)
    
    def integrate_predictions(self, data: List[Dict], 
                            anomaly_scores: np.ndarray, 
                            trend_analysis: Dict) -> Dict:
        """äºˆæ¸¬çµæœã®çµ±åˆ"""
        
        predictions = {
            'risk_level': 'low',
            'potential_weaknesses': [],
            'early_warning_indicators': [],
            'recommended_interventions': [],
            'confidence_score': 0.0
        }
        
        # ç•°å¸¸ã‚¹ã‚³ã‚¢ã‹ã‚‰å±é™ºåº¦ã‚’åˆ¤å®š
        avg_anomaly_score = np.mean(anomaly_scores)
        if avg_anomaly_score < -0.3:
            predictions['risk_level'] = 'high'
        elif avg_anomaly_score < -0.1:
            predictions['risk_level'] = 'medium'
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æã‹ã‚‰æ½œåœ¨çš„å¼±ç‚¹ã‚’ç‰¹å®š
        overall_trend = trend_analysis.get('overall_trend', 0)
        if overall_trend < -0.1:  # æ‚ªåŒ–å‚¾å‘
            predictions['potential_weaknesses'].append({
                'type': 'performance_decline',
                'severity': 'medium' if overall_trend > -0.3 else 'high',
                'evidence': 'ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®æ‚ªåŒ–å‚¾å‘ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ',
                'timeline': '1-2é€±é–“ä»¥å†…ã«å½±éŸ¿ãŒé¡•åœ¨åŒ–ã™ã‚‹å¯èƒ½æ€§'
            })
        
        # ã‚¨ãƒ©ãƒ¼å¢—åŠ å‚¾å‘ã®æ¤œå‡º
        error_trend = trend_analysis.get('error_trend', 0)
        if error_trend > 0.2:
            predictions['potential_weaknesses'].append({
                'type': 'error_pattern_emergence',
                'severity': 'medium',
                'evidence': 'ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿç‡ã®å¢—åŠ å‚¾å‘',
                'timeline': 'æ•°æ—¥ä»¥å†…ã«å•é¡ŒãŒè¡¨é¢åŒ–ã™ã‚‹å¯èƒ½æ€§'
            })
        
        # æ—©æœŸè­¦å‘ŠæŒ‡æ¨™ã®è¨­å®š
        if predictions['risk_level'] != 'low':
            predictions['early_warning_indicators'] = self.generate_warning_indicators(
                data, trend_analysis
            )
        
        # æ¨å¥¨ä»‹å…¥ã®ç”Ÿæˆ
        predictions['recommended_interventions'] = self.generate_interventions(
            predictions['potential_weaknesses'],
            trend_analysis
        )
        
        # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã®è¨ˆç®—
        predictions['confidence_score'] = self.calculate_confidence_score(
            len(data), avg_anomaly_score, trend_analysis
        )
        
        return predictions
    
    def generate_warning_indicators(self, data: List[Dict], 
                                  trend_analysis: Dict) -> List[Dict]:
        """æ—©æœŸè­¦å‘ŠæŒ‡æ¨™ã®ç”Ÿæˆ"""
        
        indicators = []
        
        # å­¦ç¿’æ™‚é–“ã®çŸ­ç¸®å‚¾å‘
        if len(data) >= 3:
            recent_durations = [d.get('learning_pattern', {}).get('session_duration', 0) 
                              for d in data[-3:]]
            if all(recent_durations[i] > recent_durations[i+1] for i in range(len(recent_durations)-1)):
                indicators.append({
                    'type': 'decreasing_study_time',
                    'description': 'å­¦ç¿’æ™‚é–“ã®ç¶™ç¶šçš„ãªçŸ­ç¸®',
                    'risk_level': 'medium'
                })
        
        # é›£æ˜“åº¦å›é¿å‚¾å‘
        difficulty_preferences = [d.get('learning_pattern', {}).get('difficulty_preference', 0) 
                                for d in data]
        if len(difficulty_preferences) >= 3:
            recent_prefs = difficulty_preferences[-3:]
            if all(recent_prefs[i] > recent_prefs[i+1] for i in range(len(recent_prefs)-1)):
                indicators.append({
                    'type': 'difficulty_avoidance',
                    'description': 'å›°é›£ãªå•é¡Œã‚’é¿ã‘ã‚‹å‚¾å‘ã®å¢—åŠ ',
                    'risk_level': 'medium'
                })
        
        return indicators
    
    def generate_interventions(self, potential_weaknesses: List[Dict], 
                             trend_analysis: Dict) -> List[Dict]:
        """æ¨å¥¨ä»‹å…¥ã®ç”Ÿæˆ"""
        
        interventions = []
        
        for weakness in potential_weaknesses:
            weakness_type = weakness['type']
            
            if weakness_type == 'performance_decline':
                interventions.append({
                    'type': 'review_fundamentals',
                    'priority': 'high',
                    'description': 'åŸºç¤æ¦‚å¿µã®å¾©ç¿’ã‚’å¼·ãæ¨å¥¨ã—ã¾ã™',
                    'specific_actions': [
                        'éå»ã®å­¦ç¿’å†…å®¹ã®æŒ¯ã‚Šè¿”ã‚Š',
                        'ç†è§£ãŒæ›–æ˜§ãªæ¦‚å¿µã®å†å­¦ç¿’',
                        'ç°¡å˜ãªå•é¡Œã‹ã‚‰ã®æ®µéšçš„ãªå†é–‹'
                    ],
                    'timeline': 'ä»Šã™ãé–‹å§‹'
                })
            
            elif weakness_type == 'error_pattern_emergence':
                interventions.append({
                    'type': 'error_pattern_analysis',
                    'priority': 'medium',
                    'description': 'ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã®è©³ç´°åˆ†æã¨å¯¾ç­–',
                    'specific_actions': [
                        'ã‚ˆãã‚ã‚‹ã‚¨ãƒ©ãƒ¼ã®åŸå› åˆ†æ',
                        'ãƒ‡ãƒãƒƒã‚°ã‚¹ã‚­ãƒ«ã®å‘ä¸Š',
                        'ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®æ´»ç”¨'
                    ],
                    'timeline': '1é€±é–“ä»¥å†…'
                })
        
        # å…¨èˆ¬çš„ãªä»‹å…¥
        overall_trend = trend_analysis.get('overall_trend', 0)
        if overall_trend < -0.2:
            interventions.append({
                'type': 'comprehensive_support',
                'priority': 'high',
                'description': 'åŒ…æ‹¬çš„ãªå­¦ç¿’æ”¯æ´ã®æä¾›',
                'specific_actions': [
                    'ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸå­¦ç¿’è¨ˆç”»ã®å†ä½œæˆ',
                    'ãƒ¡ãƒ³ã‚¿ãƒ¼ã¨ã®1å¯¾1ã‚»ãƒƒã‚·ãƒ§ãƒ³',
                    'å­¦ç¿’æ–¹æ³•ã®è¦‹ç›´ã—ã¨æœ€é©åŒ–'
                ],
                'timeline': 'ç·Šæ€¥å¯¾å¿œ'
            })
        
        return interventions

class ProactiveContentGenerator:
    """ãƒ—ãƒ­ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.weakness_detector = PredictiveWeaknessDetector()
    
    async def generate_proactive_content(self, user_id: int, 
                                       prediction_results: Dict) -> Dict:
        """ãƒ—ãƒ­ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªå­¦ç¿’ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ç”Ÿæˆ"""
        
        content_plan = {
            'immediate_actions': [],
            'weekly_plan': [],
            'monthly_goals': [],
            'personalized_resources': []
        }
        
        potential_weaknesses = prediction_results.get('potential_weaknesses', [])
        
        for weakness in potential_weaknesses:
            # å³åº§ã®å¯¾å¿œã‚³ãƒ³ãƒ†ãƒ³ãƒ„
            immediate_content = await self.generate_immediate_content(weakness)
            content_plan['immediate_actions'].extend(immediate_content)
            
            # é€±é–“å­¦ç¿’è¨ˆç”»
            weekly_content = await self.generate_weekly_content(weakness)
            content_plan['weekly_plan'].extend(weekly_content)
            
            # æœˆé–“ç›®æ¨™
            monthly_goals = await self.generate_monthly_goals(weakness)
            content_plan['monthly_goals'].extend(monthly_goals)
        
        # ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸãƒªã‚½ãƒ¼ã‚¹
        content_plan['personalized_resources'] = await self.generate_personalized_resources(
            user_id, potential_weaknesses
        )
        
        return content_plan
    
    async def generate_immediate_content(self, weakness: Dict) -> List[Dict]:
        """å³åº§ã®å¯¾å¿œã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆ"""
        
        weakness_type = weakness['type']
        
        if weakness_type == 'performance_decline':
            return [
                {
                    'type': 'diagnostic_quiz',
                    'title': 'ç†è§£åº¦ç¢ºèªã‚¯ã‚¤ã‚º',
                    'description': 'ç¾åœ¨ã®ç†è§£ãƒ¬ãƒ™ãƒ«ã‚’æ­£ç¢ºã«æŠŠæ¡ã—ã¾ã™',
                    'estimated_time': 15,
                    'priority': 'high'
                },
                {
                    'type': 'concept_review',
                    'title': 'é‡è¦æ¦‚å¿µã®æŒ¯ã‚Šè¿”ã‚Š',
                    'description': 'åŸºç¤æ¦‚å¿µã®ç¢ºèªã¨è£œå¼·',
                    'estimated_time': 30,
                    'priority': 'high'
                }
            ]
        
        elif weakness_type == 'error_pattern_emergence':
            return [
                {
                    'type': 'error_analysis_exercise',
                    'title': 'ã‚¨ãƒ©ãƒ¼åˆ†ææ¼”ç¿’',
                    'description': 'ã‚ˆãã‚ã‚‹ã‚¨ãƒ©ãƒ¼ã®ç‰¹å®šã¨ä¿®æ­£æ–¹æ³•',
                    'estimated_time': 20,
                    'priority': 'medium'
                }
            ]
        
        return []
    
    async def generate_weekly_content(self, weakness: Dict) -> List[Dict]:
        """é€±é–“å­¦ç¿’ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆ"""
        
        return [
            {
                'day': 1,
                'content': 'åŸºç¤æ¦‚å¿µã®å¾©ç¿’',
                'activities': ['èª­ã‚€', 'ä¾‹é¡Œã‚’è§£ã', 'ç¢ºèªãƒ†ã‚¹ãƒˆ']
            },
            {
                'day': 3,
                'content': 'å¿œç”¨å•é¡Œã¸ã®æŒ‘æˆ¦',
                'activities': ['æ®µéšçš„ãªå•é¡Œè§£æ±º', 'ãƒ”ã‚¢ãƒ¬ãƒ“ãƒ¥ãƒ¼']
            },
            {
                'day': 5,
                'content': 'ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå®Ÿè·µ',
                'activities': ['ãƒŸãƒ‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ', 'æˆæœç‰©ã®ç™ºè¡¨']
            },
            {
                'day': 7,
                'content': 'é€±é–“æŒ¯ã‚Šè¿”ã‚Š',
                'activities': ['é€²æ—ç¢ºèª', 'æ¬¡é€±ã®è¨ˆç”»ç«‹æ¡ˆ']
            }
        ]

# ä½¿ç”¨ä¾‹
async def run_predictive_analysis(user_id: int):
    """äºˆæ¸¬åˆ†æã®å®Ÿè¡Œ"""
    
    # éå»ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    historical_data = await get_user_historical_data(user_id, days=90)
    recent_data = await get_user_recent_data(user_id, days=7)
    
    # äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´
    detector = PredictiveWeaknessDetector()
    if detector.train_anomaly_detector(historical_data):
        
        # å¼±ç‚¹äºˆæ¸¬ã®å®Ÿè¡Œ
        predictions = detector.predict_potential_weaknesses(recent_data)
        
        # ãƒ—ãƒ­ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ç”Ÿæˆ
        content_generator = ProactiveContentGenerator()
        content_plan = await content_generator.generate_proactive_content(
            user_id, predictions
        )
        
        # çµæœã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
        await save_prediction_results(user_id, predictions, content_plan)
        
        # å¿…è¦ã«å¿œã˜ã¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«é€šçŸ¥
        if predictions['risk_level'] in ['medium', 'high']:
            await notify_user_of_predictions(user_id, predictions, content_plan)
        
        return {
            'predictions': predictions,
            'content_plan': content_plan,
            'status': 'success'
        }
    
    else:
        return {
            'status': 'insufficient_data',
            'message': 'äºˆæ¸¬ã«å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™'
        }
```

#### Week 8-9: å®Ÿè·µãƒ­ã‚°çµ±åˆ
- [ ] ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ­ã‚°åé›†ã‚·ã‚¹ãƒ†ãƒ 
- [ ] ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³è‡ªå‹•åˆ†é¡
- [ ] ãƒ«ãƒ¼ãƒ«åŠ¹æœæ¸¬å®šã‚·ã‚¹ãƒ†ãƒ 
- [ ] çµ±è¨ˆãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰

#### Week 10: è‡ªå‹•æ”¹å–„ã‚·ã‚¹ãƒ†ãƒ   
- [ ] è‡ªå‹•çš„ãªå­¦ç¿’ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ›´æ–°ææ¡ˆ
- [ ] ãƒ«ãƒ¼ãƒ«ã®è‡ªå‹•èª¿æ•´æ©Ÿèƒ½
- [ ] äºˆæ¸¬çš„ãªå¼±ç‚¹æ¤œå‡º
- [ ] çµ±åˆãƒ†ã‚¹ãƒˆã¨ãƒ¦ãƒ¼ã‚¶ãƒ“ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆ

### Phase 4: ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºæ©Ÿèƒ½ (4é€±é–“)
**ç›®æ¨™**: çµ„ç¹”ãƒ¬ãƒ™ãƒ«ã®æ©Ÿèƒ½ã¨ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£

#### Week 11-12: ãƒãƒ¼ãƒ æ©Ÿèƒ½
- [ ] éƒ¨é–€åˆ¥çµ±è¨ˆãƒ»åˆ†æ
- [ ] ãƒãƒ¼ãƒ æ¨™æº–ãƒ«ãƒ¼ãƒ«ç®¡ç†
- [ ] ãƒ”ã‚¢ãƒ¬ãƒ“ãƒ¥ãƒ¼é€£æº
- [ ] ã‚¹ã‚­ãƒ«ãƒãƒˆãƒªãƒƒã‚¯ã‚¹è‡ªå‹•ç”Ÿæˆ

#### Week 13-14: æ‹¡å¼µæ€§ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
- [ ] æ°´å¹³ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¯¾å¿œ
- [ ] ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°æˆ¦ç•¥å®Ÿè£…
- [ ] ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ãƒ»ã‚¢ãƒ©ãƒ¼ãƒˆ
- [ ] ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç›£æŸ»ã¨ãƒšãƒãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆ

## ğŸ“Š æˆåŠŸæŒ‡æ¨™ã¨KPI

### æŠ€è¡“çš„æŒ‡æ¨™
- **APIå¿œç­”æ™‚é–“**: 95%ile < 500ms
- **MCPå‡¦ç†æ™‚é–“**: å­¦ç¿’åˆ†æ < 3ç§’ã€ãƒ«ãƒ¼ãƒ«ç”Ÿæˆ < 2ç§’
- **ã‚·ã‚¹ãƒ†ãƒ ç¨¼åƒç‡**: > 99.9%
- **ãƒ‡ãƒ¼ã‚¿å‡¦ç†ç²¾åº¦**: å¼±ç‚¹æ¤œå‡ºç²¾åº¦ > 85%

### ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“æŒ‡æ¨™
- **å­¦ç¿’åŠ¹ç‡å‘ä¸Šç‡**: +30% (å¾“æ¥ã®å­¦ç¿’æ–¹æ³•ã¨æ¯”è¼ƒ)
- **é–‹ç™ºã‚¨ãƒ©ãƒ¼å‰Šæ¸›ç‡**: -50% (Cursor Rulesé©ç”¨å‰å¾Œ)
- **ãƒ¦ãƒ¼ã‚¶ãƒ¼æº€è¶³åº¦**: > 4.5/5.0
- **ç¶™ç¶šåˆ©ç”¨ç‡**: > 80% (3ãƒ¶æœˆå¾Œ)

### ãƒ“ã‚¸ãƒã‚¹æŒ‡æ¨™
- **ã‚¹ã‚­ãƒ«å‘ä¸Šé€Ÿåº¦**: å¾“æ¥æ¯” +40%
- **ãƒãƒ¼ãƒ ç”Ÿç”£æ€§**: ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ“ãƒ¥ãƒ¼æ™‚é–“ -30%
- **å“è³ªæŒ‡æ¨™**: ãƒã‚°æ¤œå‡ºç‡å‘ä¸Š +60%
- **ROI**: å°å…¥ã‚³ã‚¹ãƒˆå›åæœŸé–“ < 6ãƒ¶æœˆ

## ğŸ”’ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã¨ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼

### ãƒ‡ãƒ¼ã‚¿ä¿è­·
- **å€‹äººå­¦ç¿’ãƒ‡ãƒ¼ã‚¿æš—å·åŒ–**: AES-256ã«ã‚ˆã‚‹æš—å·åŒ–
- **APIãƒˆãƒ¼ã‚¯ãƒ³ç®¡ç†**: JWT + OAuth 2.0
- **ãƒ‡ãƒ¼ã‚¿ã‚¢ã‚¯ã‚»ã‚¹åˆ¶å¾¡**: ãƒ­ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã‚¢ã‚¯ã‚»ã‚¹åˆ¶å¾¡ï¼ˆRBACï¼‰
- **ç›£æŸ»ãƒ­ã‚°**: å…¨ãƒ‡ãƒ¼ã‚¿ã‚¢ã‚¯ã‚»ã‚¹ã®è¨˜éŒ²

### ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ä¿è­·
- **ãƒ‡ãƒ¼ã‚¿åŒ¿ååŒ–**: çµ±è¨ˆå‡¦ç†æ™‚ã®å€‹äººæƒ…å ±é™¤å»
- **åŒæ„ç®¡ç†**: æ˜ç¤ºçš„ãªãƒ‡ãƒ¼ã‚¿ä½¿ç”¨åŒæ„
- **GDPRæº–æ‹ **: EUåœãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ¨©åˆ©ä¿è­·
- **ãƒ‡ãƒ¼ã‚¿ä¿æŒæœŸé–“**: æ˜ç¢ºãªä¿å­˜æœŸé–“ã¨è‡ªå‹•å‰Šé™¤

### ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¯¾ç­–
```python
# ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒŸãƒ‰ãƒ«ã‚¦ã‚§ã‚¢ã®ä¾‹
class CursorIntegrationSecurityMiddleware:
    def __init__(self, get_response):
        self.get_response = get_response
    
    def __call__(self, request):
        # Rate limiting
        if not self.check_rate_limit(request):
            return HttpResponse("Rate limit exceeded", status=429)
        
        # API token validation
        if not self.validate_api_token(request):
            return HttpResponse("Invalid token", status=401)
        
        # Data access authorization
        if not self.authorize_data_access(request):
            return HttpResponse("Unauthorized", status=403)
        
        response = self.get_response(request)
        
        # Response sanitization
        response = self.sanitize_response(response)
        
        return response
```

## ğŸ“ˆ ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã¨é‹ç”¨

### ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–
```python
# ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ã®ä¾‹
from prometheus_client import Counter, Histogram, Gauge

# APIä½¿ç”¨é‡ãƒ¡ãƒˆãƒªã‚¯ã‚¹
api_requests_total = Counter(
    'cursor_integration_api_requests_total',
    'Total API requests',
    ['endpoint', 'method', 'status']
)

# å‡¦ç†æ™‚é–“ãƒ¡ãƒˆãƒªã‚¯ã‚¹
processing_duration = Histogram(
    'cursor_integration_processing_seconds',
    'Processing duration',
    ['operation_type']
)

# ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°
active_users = Gauge(
    'cursor_integration_active_users',
    'Currently active users'
)
```

### ãƒ­ã‚°ç®¡ç†
```python
# æ§‹é€ åŒ–ãƒ­ã‚°ã®ä¾‹
import structlog

logger = structlog.get_logger()

# å­¦ç¿’åˆ†æãƒ­ã‚°
logger.info("learning_analysis_completed", 
    user_id=user_id,
    analysis_duration=duration,
    weak_points_count=len(weak_points),
    confidence_score=confidence
)

# ãƒ«ãƒ¼ãƒ«ç”Ÿæˆãƒ­ã‚°  
logger.info("cursor_rules_generated",
    user_id=user_id,
    rule_count=len(rules),
    customization_level=customization_level,
    generation_duration=duration
)
```

### ç•°å¸¸æ¤œçŸ¥
```python
# ç•°å¸¸æ¤œçŸ¥ã‚¢ãƒ©ãƒ¼ãƒˆã®ä¾‹
class AnomalyDetector:
    def check_analysis_accuracy(self, results):
        """åˆ†æç²¾åº¦ã®ç•°å¸¸ã‚’æ¤œçŸ¥"""
        if results.confidence_score < 0.7:
            self.send_alert("Low analysis confidence detected")
    
    def check_rule_effectiveness(self, effectiveness_metrics):
        """ãƒ«ãƒ¼ãƒ«åŠ¹æœã®ç•°å¸¸ã‚’æ¤œçŸ¥"""
        if effectiveness_metrics.error_reduction < 0.2:
            self.send_alert("Rule effectiveness below threshold")
    
    def check_system_performance(self, metrics):
        """ã‚·ã‚¹ãƒ†ãƒ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ç•°å¸¸ã‚’æ¤œçŸ¥"""
        if metrics.avg_response_time > 1000:  # 1ç§’
            self.send_alert("High response time detected")
```

## ğŸ”„ ç¶™ç¶šçš„æ”¹å–„ãƒ—ãƒ­ã‚»ã‚¹

### è‡ªå‹•å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ 
```python
class ContinuousLearningEngine:
    def analyze_user_feedback(self, feedback_data):
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‹ã‚‰æ”¹å–„ç‚¹ã‚’ç‰¹å®š"""
        # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åˆ†æ
        satisfaction_trends = self.analyze_satisfaction_trends(feedback_data)
        feature_requests = self.extract_feature_requests(feedback_data)
        
        # æ”¹å–„ææ¡ˆã®ç”Ÿæˆ
        improvement_proposals = self.generate_improvement_proposals(
            satisfaction_trends, feature_requests
        )
        
        return improvement_proposals
    
    def update_ai_models(self, practice_logs):
        """å®Ÿè·µãƒ­ã‚°ã‹ã‚‰AI ãƒ¢ãƒ‡ãƒ«ã‚’æ›´æ–°"""
        # æ–°ã—ã„å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
        training_data = self.prepare_training_data(practice_logs)
        
        # ãƒ¢ãƒ‡ãƒ«ã®å†è¨“ç·´
        updated_model = self.retrain_model(training_data)
        
        # A/Bãƒ†ã‚¹ãƒˆã§ã®æ¤œè¨¼
        self.deploy_model_for_testing(updated_model)
```

### ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†æˆ¦ç•¥
```python
class RuleVersionManager:
    def create_rule_version(self, user_id, rules):
        """æ–°ã—ã„ãƒ«ãƒ¼ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ä½œæˆ"""
        version = {
            "version_id": self.generate_version_id(),
            "user_id": user_id,
            "rules": rules,
            "created_at": datetime.utcnow(),
            "parent_version": self.get_current_version(user_id),
            "effectiveness_baseline": None  # å¾Œã§æ¸¬å®š
        }
        return self.save_version(version)
    
    def rollback_rules(self, user_id, target_version):
        """åŠ¹æœãŒä½ã„å ´åˆã®ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
        target = self.get_version(user_id, target_version)
        self.set_active_version(user_id, target)
        
        self.log_rollback(user_id, target_version, "low_effectiveness")
```

---

## ğŸ“‹ çµè«–

ã“ã®çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã‚Šã€asagami AIã¨Cursor Rulesã®é€£æºã«ã‚ˆã‚‹ã€Œé©å¿œå‹é–‹ç™ºç’°å¢ƒã€ãŒå®Ÿç¾ã•ã‚Œã¾ã™ã€‚å€‹äººã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç”Ÿæˆã•ã‚ŒãŸãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸCursor Rulesã«ã‚ˆã‚Šã€é–‹ç™ºè€…ã¯è‡ªåˆ†ã®å¼±ç‚¹ã«ç‰¹åŒ–ã—ãŸæ”¯æ´ã‚’å—ã‘ãªãŒã‚‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è¡Œã„ã€ãã®å®Ÿè·µçµæœãŒå†ã³å­¦ç¿’å†…å®¹ã®æ”¹å–„ã«æ´»ç”¨ã•ã‚Œã‚‹ç¶™ç¶šçš„ãªæˆé•·ã‚µã‚¤ã‚¯ãƒ«ãŒç¢ºç«‹ã•ã‚Œã¾ã™ã€‚

### æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ
1. **å€‹äººãƒ¬ãƒ™ãƒ«**: ã‚¹ã‚­ãƒ«å‘ä¸Šé€Ÿåº¦ +40%ã€é–‹ç™ºã‚¨ãƒ©ãƒ¼ -50%
2. **ãƒãƒ¼ãƒ ãƒ¬ãƒ™ãƒ«**: ã‚³ãƒ¼ãƒ‰å“è³ªå‡ä¸€åŒ–ã€ãƒŠãƒ¬ãƒƒã‚¸å…±æœ‰ä¿ƒé€²  
3. **çµ„ç¹”ãƒ¬ãƒ™ãƒ«**: é–‹ç™ºç”Ÿç”£æ€§å‘ä¸Šã€æŠ€è¡“è² å‚µå‰Šæ¸›

ã“ã®åŒ…æ‹¬çš„ãªè¨­è¨ˆä»•æ§˜æ›¸ã«åŸºã¥ãã€æ®µéšçš„ãªå®Ÿè£…ã‚’é€²ã‚ã‚‹ã“ã¨ã§ã€é©æ–°çš„ãªå­¦ç¿’æ”¯æ´ã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰ãŒå¯èƒ½ã¨ãªã‚Šã¾ã™ã€‚

---

**ä½œæˆæ—¥**: 2025-07-13  
**ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 1.0  
**ä½œæˆè€…**: asagami AIé–‹ç™ºãƒãƒ¼ãƒ   
**æ‰¿èª**: é–‹ç™ºåŠ¹ç‡åŒ–æ¨é€²ãƒãƒ¼ãƒ 

**æŠ€è¡“ãƒ¬ãƒ“ãƒ¥ãƒ¼**: ã‚·ã‚¹ãƒ†ãƒ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒˆã€ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢  
**ãƒ“ã‚¸ãƒã‚¹ãƒ¬ãƒ“ãƒ¥ãƒ¼**: ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã€å“è³ªä¿è¨¼ãƒãƒ¼ãƒ 

---

# INTEGRATION IMPLEMENTATION GUIDE

# asagami AI Ã— Cursor Rules çµ±åˆå®Ÿè£…ã‚¬ã‚¤ãƒ‰

## æ¦‚è¦
ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ã€asagami AIã¨Cursor Rulesã®é€£æºã«ã‚ˆã‚‹ã€Œé©å¿œå‹é–‹ç™ºç’°å¢ƒã€ã‚’å®Ÿç¾ã™ã‚‹ãŸã‚ã®åŒ…æ‹¬çš„ãªå®Ÿè£…ã‚¬ã‚¤ãƒ‰ã§ã™ã€‚

## 1. ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¦‚è¦

### 1.1 ã‚·ã‚¹ãƒ†ãƒ æ§‹æˆå›³
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   asagami AI    â”‚â—„â”€â”€â–ºâ”‚  MCP Server      â”‚â—„â”€â”€â–ºâ”‚   Cursor IDE     â”‚
â”‚   (Django)      â”‚    â”‚  (Python)        â”‚    â”‚   (Claude Code)  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚â€¢ å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ç®¡ç† â”‚    â”‚â€¢ ãƒ‡ãƒ¼ã‚¿åˆ†æ      â”‚    â”‚â€¢ é–‹ç™ºæ”¯æ´       â”‚
â”‚â€¢ å•é¡Œç”Ÿæˆ       â”‚    â”‚â€¢ ãƒ«ãƒ¼ãƒ«ç”Ÿæˆ      â”‚    â”‚â€¢ ãƒ­ã‚°åé›†       â”‚
â”‚â€¢ é€²æ—è¿½è·¡       â”‚    â”‚â€¢ AIå‡¦ç†          â”‚    â”‚â€¢ é©å¿œå‹ã‚¢ã‚·ã‚¹ãƒˆ â”‚
â”‚â€¢ ãƒ¦ãƒ¼ã‚¶ãƒ¼ç®¡ç†   â”‚    â”‚â€¢ çµ±åˆå‡¦ç†        â”‚    â”‚â€¢ ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼
```
å­¦ç¿’ â†’ åˆ†æ â†’ ãƒ«ãƒ¼ãƒ«ç”Ÿæˆ â†’ é–‹ç™ºæ”¯æ´ â†’ ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ â†’ æ”¹å–„
 â†‘                                                    â†“
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ç¶™ç¶šçš„æ”¹å–„ã‚µã‚¤ã‚¯ãƒ« â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 2. å®Ÿè£…ãƒ•ã‚§ãƒ¼ã‚º

### Phase 1: åŸºç›¤æ§‹ç¯‰ (4é€±é–“)
**ç›®æ¨™**: MVPã®å®Ÿè£…ã¨åŸºæœ¬çš„ãªé€£æºæ©Ÿèƒ½ã®ç¢ºç«‹

#### 2.1.1 asagami AIå´ã®æ‹¡å¼µ
```python
# æ–°è¦APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®è¿½åŠ 
# app/urls.py ã«è¿½åŠ 
path('api/cursor-integration/', include('cursor_integration.urls')),

# cursor_integration/urls.py
urlpatterns = [
    path('learning-data/<int:user_id>/', views.get_learning_data, name='get_learning_data'),
    path('generate-rules/', views.generate_cursor_rules, name='generate_cursor_rules'),
    path('practice-logs/', views.collect_practice_logs, name='collect_practice_logs'),
]
```

#### 2.1.2 æ–°è¦Djangoã‚¢ãƒ—ãƒªã®ä½œæˆ
```bash
python manage.py startapp cursor_integration
```

#### 2.1.3 å¿…è¦ãªãƒ¢ãƒ‡ãƒ«è¿½åŠ 
```python
# cursor_integration/models.py
class CursorRuleProfile(models.Model):
    user = models.ForeignKey(CustomUser, on_delete=models.CASCADE)
    rule_version = models.CharField(max_length=50)
    generated_at = models.DateTimeField(auto_now_add=True)
    rule_config = models.JSONField()
    is_active = models.BooleanField(default=True)

class PracticeLog(models.Model):
    user = models.ForeignKey(CustomUser, on_delete=models.CASCADE)
    session_id = models.CharField(max_length=100)
    start_time = models.DateTimeField()
    end_time = models.DateTimeField()
    error_data = models.JSONField()
    completion_data = models.JSONField()
    productivity_metrics = models.JSONField()

class LearningAnalysis(models.Model):
    user = models.ForeignKey(CustomUser, on_delete=models.CASCADE)
    analysis_date = models.DateField(auto_now_add=True)
    weak_points = models.JSONField()
    strong_points = models.JSONField()
    improvement_suggestions = models.JSONField()
    confidence_score = models.FloatField()
```

### Phase 2: MCP ã‚µãƒ¼ãƒãƒ¼æ§‹ç¯‰ (3é€±é–“)

#### 2.2.1 MCPã‚µãƒ¼ãƒãƒ¼ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
```python
# mcp_server/server.py
from mcp import McpServer
from mcp.types import Tool, TextContent

class AsagamiMcpServer(McpServer):
    def __init__(self):
        super().__init__("asagami-mcp-server", "1.0.0")
        self.setup_tools()
    
    def setup_tools(self):
        @self.tool()
        async def analyze_learning_data(user_id: int, period: int = 30):
            """å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã¦å¼±ç‚¹ã¨å¼·ã¿ã‚’ç‰¹å®š"""
            # Django APIã‚’å‘¼ã³å‡ºã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            analysis_result = await self.fetch_learning_data(user_id, period)
            return self.process_analysis(analysis_result)
        
        @self.tool()
        async def generate_cursor_rules(analysis_data: dict):
            """åˆ†æçµæœã‹ã‚‰Cursor Rulesã‚’ç”Ÿæˆ"""
            rules = await self.create_adaptive_rules(analysis_data)
            return self.format_cursor_rules(rules)
```

#### 2.2.2 AIåˆ†æã‚¨ãƒ³ã‚¸ãƒ³ã®å®Ÿè£…
```python
# mcp_server/ai_engine.py
import openai
from typing import Dict, List

class LearningAnalysisEngine:
    def __init__(self, openai_api_key: str):
        self.client = openai.OpenAI(api_key=openai_api_key)
    
    async def analyze_weak_points(self, user_data: Dict) -> List[Dict]:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å¼±ç‚¹ã‚’åˆ†æ"""
        prompt = f"""
        å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¼±ç‚¹ã‚’ç‰¹å®šã—ã¦ãã ã•ã„ï¼š
        - å•é¡Œè§£ç­”ãƒ‡ãƒ¼ã‚¿: {user_data['question_results']}
        - å­¦ç¿’æ™‚é–“: {user_data['study_time']}
        - ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³: {user_data['error_patterns']}
        
        ä»¥ä¸‹ã®å½¢å¼ã§JSONãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¿”ã—ã¦ãã ã•ã„ï¼š
        {{
            "weak_points": [
                {{
                    "topic": "ãƒˆãƒ”ãƒƒã‚¯å",
                    "score": æ•°å€¤ã‚¹ã‚³ã‚¢,
                    "priority": "high/medium/low",
                    "improvement_suggestions": ["ææ¡ˆ1", "ææ¡ˆ2"]
                }}
            ]
        }}
        """
        
        response = await self.client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"}
        )
        
        return json.loads(response.choices[0].message.content)
```

### Phase 3: Cursor Rulesç”Ÿæˆã‚¨ãƒ³ã‚¸ãƒ³ (2é€±é–“)

#### 2.3.1 ãƒ«ãƒ¼ãƒ«ç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯
```python
# mcp_server/rule_generator.py
class CursorRuleGenerator:
    def __init__(self):
        self.rule_templates = self.load_rule_templates()
    
    def generate_personalized_rules(self, weak_points: List[Dict], user_profile: Dict) -> Dict:
        """å€‹äººã«ç‰¹åŒ–ã—ãŸCursor Rulesã‚’ç”Ÿæˆ"""
        rules = {
            "version": "1.0",
            "user_profile": user_profile,
            "rules": {},
            "templates": [],
            "suggestions": []
        }
        
        for weak_point in weak_points:
            topic = weak_point['topic']
            if topic in self.rule_templates:
                personalized_rule = self.customize_rule(
                    self.rule_templates[topic], 
                    weak_point,
                    user_profile
                )
                rules['rules'][topic] = personalized_rule
        
        return rules
    
    def customize_rule(self, template: Dict, weak_point: Dict, profile: Dict) -> Dict:
        """ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«åˆã‚ã›ã¦ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º"""
        rule = template.copy()
        
        # ã‚¹ã‚­ãƒ«ãƒ¬ãƒ™ãƒ«ã«å¿œã˜ãŸèª¿æ•´
        skill_level = profile.get('skill_level', 'intermediate')
        if skill_level == 'beginner':
            rule['severity'] = 'error'
            rule['auto_fix'] = True
        elif skill_level == 'advanced':
            rule['severity'] = 'info'
            rule['auto_fix'] = False
        
        # å¼±ç‚¹ã«å¿œã˜ãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º
        rule['message'] = rule['message'].format(
            topic=weak_point['topic'],
            suggestions=', '.join(weak_point['improvement_suggestions'])
        )
        
        return rule
```

### Phase 4: ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—å®Ÿè£… (3é€±é–“)

#### 2.4.1 å®Ÿè·µãƒ­ã‚°åé›†ã‚·ã‚¹ãƒ†ãƒ 
```python
# cursor_integration/views.py
from django.http import JsonResponse
from django.views.decorators.csrf import csrf_exempt
import json

@csrf_exempt
def collect_practice_logs(request):
    """Cursorã‹ã‚‰ã®å®Ÿè·µãƒ­ã‚°ã‚’åé›†"""
    if request.method == 'POST':
        log_data = json.loads(request.body)
        
        # ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
        practice_log = PracticeLog.objects.create(
            user_id=log_data['user_id'],
            session_id=log_data['session_id'],
            start_time=log_data['session_start'],
            end_time=log_data['session_end'],
            error_data=log_data['development_data']['errors_encountered'],
            completion_data=log_data['development_data']['code_completions'],
            productivity_metrics=log_data['development_data']['productivity_metrics']
        )
        
        # éåŒæœŸã§ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åˆ†æã‚’å®Ÿè¡Œ
        analyze_feedback.delay(practice_log.id)
        
        return JsonResponse({'status': 'success', 'log_id': practice_log.id})
```

#### 2.4.2 ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åˆ†æã‚¿ã‚¹ã‚¯
```python
# cursor_integration/tasks.py
from celery import shared_task
from .ai_engine import LearningAnalysisEngine

@shared_task
def analyze_feedback(practice_log_id):
    """å®Ÿè·µãƒ­ã‚°ã‚’åˆ†æã—ã¦ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’ç”Ÿæˆ"""
    practice_log = PracticeLog.objects.get(id=practice_log_id)
    
    # ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ
    error_analysis = analyze_error_patterns(practice_log.error_data)
    
    # æ–°ã—ã„å¼±ç‚¹ã®æ¤œå‡º
    new_weak_points = detect_new_weak_points(error_analysis)
    
    # å­¦ç¿’ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æ›´æ–°ææ¡ˆ
    if new_weak_points:
        generate_new_learning_content.delay(
            practice_log.user_id, 
            new_weak_points
        )
    
    # ãƒ«ãƒ¼ãƒ«ã®åŠ¹æœæ¸¬å®š
    rule_effectiveness = measure_rule_effectiveness(practice_log)
    
    return {
        'new_weak_points': new_weak_points,
        'rule_effectiveness': rule_effectiveness
    }
```

## 3. æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯

### 3.1 ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰
- **Django**: 4.2+ (æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ )
- **Python**: 3.9+
- **PostgreSQL**: 14+ (æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹)
- **Redis**: 7+ (ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ»ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†)
- **Celery**: 5+ (éåŒæœŸã‚¿ã‚¹ã‚¯å‡¦ç†)

### 3.2 MCP ã‚µãƒ¼ãƒãƒ¼
- **mcp**: æœ€æ–°ç‰ˆ
- **FastAPI**: 0.104+ (APIéƒ¨åˆ†)
- **OpenAI API**: GPT-4 (AIåˆ†æ)
- **asyncio**: éåŒæœŸå‡¦ç†

### 3.3 ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ï¼ˆæ‹¡å¼µï¼‰
- **React**: 18.2+ (æ—¢å­˜)
- **Axios**: APIé€šä¿¡
- **WebSocket**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€šä¿¡

## 4. è¨­å®šã¨ãƒ‡ãƒ—ãƒ­ã‚¤

### 4.1 ç’°å¢ƒå¤‰æ•°
```bash
# .env
OPENAI_API_KEY=your_openai_api_key
MCP_SERVER_URL=http://localhost:8001
CURSOR_WEBHOOK_SECRET=your_webhook_secret
REDIS_URL=redis://localhost:6379
```

### 4.2 Dockerã‚³ãƒ³ãƒ†ãƒŠæ§‹æˆ
```yaml
# docker-compose.yml
version: '3.8'
services:
  django:
    build: .
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=postgresql://user:pass@db:5432/asagami
    depends_on:
      - db
      - redis
  
  mcp-server:
    build: ./mcp_server
    ports:
      - "8001:8001"
    environment:
      - DJANGO_API_URL=http://django:8000
      - OPENAI_API_KEY=${OPENAI_API_KEY}
  
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
  
  celery:
    build: .
    command: celery -A mysite worker -l info
    depends_on:
      - redis
      - db
```

## 5. ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è€ƒæ…®äº‹é …

### 5.1 ãƒ‡ãƒ¼ã‚¿ä¿è­·
- å€‹äººå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®æš—å·åŒ–
- APIã‚¢ã‚¯ã‚»ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³ã®é©åˆ‡ãªç®¡ç†
- CORSè¨­å®šã®æœ€é©åŒ–

### 5.2 èªè¨¼ãƒ»èªå¯
```python
# cursor_integration/authentication.py
from rest_framework.authentication import TokenAuthentication
from rest_framework.permissions import IsAuthenticated

class CursorIntegrationView(APIView):
    authentication_classes = [TokenAuthentication]
    permission_classes = [IsAuthenticated]
    
    def get(self, request, *args, **kwargs):
        # æ¨©é™ãƒã‚§ãƒƒã‚¯
        if not request.user.has_perm('cursor_integration.view_data'):
            return Response({'error': 'Permission denied'}, status=403)
```

## 6. ãƒ†ã‚¹ãƒˆæˆ¦ç•¥

### 6.1 å˜ä½“ãƒ†ã‚¹ãƒˆ
```python
# tests/test_rule_generator.py
class TestCursorRuleGenerator(TestCase):
    def setUp(self):
        self.generator = CursorRuleGenerator()
        self.sample_weak_points = [
            {
                'topic': 'SQL Injection',
                'score': 65,
                'priority': 'high'
            }
        ]
    
    def test_generate_personalized_rules(self):
        rules = self.generator.generate_personalized_rules(
            self.sample_weak_points,
            {'skill_level': 'intermediate'}
        )
        self.assertIn('SQL Injection', rules['rules'])
```

### 6.2 çµ±åˆãƒ†ã‚¹ãƒˆ
```python
# tests/test_integration.py
class TestAsagamiCursorIntegration(TestCase):
    def test_end_to_end_workflow(self):
        # 1. å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ
        user = create_test_user()
        create_learning_data(user)
        
        # 2. MCPåˆ†æã®å®Ÿè¡Œ
        analysis = call_mcp_analysis(user.id)
        
        # 3. Cursor Rulesç”Ÿæˆ
        rules = generate_cursor_rules(analysis)
        
        # 4. ãƒ«ãƒ¼ãƒ«é…ä¿¡
        response = deploy_rules_to_cursor(rules)
        
        self.assertEqual(response.status_code, 200)
```

## 7. ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹

### 7.1 KPIè¨­å®š
- å­¦ç¿’æ”¹å–„ç‡: å‰æœˆæ¯”ã§ã®å•é¡Œæ­£ç­”ç‡å‘ä¸Š
- é–‹ç™ºã‚¨ãƒ©ãƒ¼æ¸›å°‘ç‡: Cursorä½¿ç”¨å‰å¾Œã®ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿç‡æ¯”è¼ƒ
- ãƒ¦ãƒ¼ã‚¶ãƒ¼æº€è¶³åº¦: ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚¹ã‚³ã‚¢
- ã‚·ã‚¹ãƒ†ãƒ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹: APIå¿œç­”æ™‚é–“ã€MCPå‡¦ç†æ™‚é–“

### 7.2 ãƒ­ã‚°è¨­å®š
```python
# settings/logging.py
LOGGING = {
    'version': 1,
    'disable_existing_loggers': False,
    'formatters': {
        'verbose': {
            'format': '{levelname} {asctime} {module} {process:d} {thread:d} {message}',
            'style': '{',
        },
    },
    'handlers': {
        'cursor_integration': {
            'level': 'INFO',
            'class': 'logging.FileHandler',
            'filename': 'logs/cursor_integration.log',
            'formatter': 'verbose',
        },
    },
    'loggers': {
        'cursor_integration': {
            'handlers': ['cursor_integration'],
            'level': 'INFO',
            'propagate': False,
        },
    },
}
```

## 8. ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—

### 8.1 çŸ­æœŸç›®æ¨™ (3ãƒ¶æœˆ)
- [ ] MVPã®å®Œæˆã¨Î²ãƒ†ã‚¹ãƒˆé–‹å§‹
- [ ] åŸºæœ¬çš„ãªå­¦ç¿’ãƒ‡ãƒ¼ã‚¿åˆ†ææ©Ÿèƒ½
- [ ] ã‚·ãƒ³ãƒ—ãƒ«ãªCursor Rulesç”Ÿæˆ
- [ ] ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åé›†ã‚·ã‚¹ãƒ†ãƒ 

### 8.2 ä¸­æœŸç›®æ¨™ (6ãƒ¶æœˆ)
- [ ] AIåˆ†æç²¾åº¦ã®å‘ä¸Š
- [ ] ãƒãƒ¼ãƒ æ©Ÿèƒ½ã®å®Ÿè£…
- [ ] ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°ã‚·ã‚¹ãƒ†ãƒ 
- [ ] å¤–éƒ¨ãƒ„ãƒ¼ãƒ«é€£æºï¼ˆGitHubã€Slackç­‰ï¼‰

### 8.3 é•·æœŸç›®æ¨™ (12ãƒ¶æœˆ)
- [ ] æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹äºˆæ¸¬åˆ†æ
- [ ] è‡ªå‹•çš„ãªå­¦ç¿’ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆ
- [ ] ä¼æ¥­å‘ã‘ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºæ©Ÿèƒ½
- [ ] ä»–ã®IDEå¯¾å¿œï¼ˆVS Codeã€IntelliJç­‰ï¼‰

## 9. æˆåŠŸæŒ‡æ¨™

### 9.1 æŠ€è¡“çš„æŒ‡æ¨™
- APIå¿œç­”æ™‚é–“: < 500ms (95%ile)
- MCPå‡¦ç†æ™‚é–“: < 2ç§’
- ã‚·ã‚¹ãƒ†ãƒ ç¨¼åƒç‡: > 99.9%
- ã‚¨ãƒ©ãƒ¼ç‡: < 0.1%

### 9.2 ãƒ“ã‚¸ãƒã‚¹æŒ‡æ¨™
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å­¦ç¿’åŠ¹ç‡å‘ä¸Š: +30%
- é–‹ç™ºã‚¨ãƒ©ãƒ¼æ¸›å°‘: -50%
- ãƒ¦ãƒ¼ã‚¶ãƒ¼æº€è¶³åº¦: > 4.5/5.0
- ç¶™ç¶šåˆ©ç”¨ç‡: > 80%

---

**ä½œæˆæ—¥**: 2025-07-13  
**ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 1.0  
**ä½œæˆè€…**: asagami AIé–‹ç™ºãƒãƒ¼ãƒ   
**ãƒ¬ãƒ“ãƒ¥ãƒ¼**: é–‹ç™ºåŠ¹ç‡åŒ–æ¨é€²ãƒãƒ¼ãƒ 

---

# MCP DESIGN SPECIFICATION

# asagami AI Ã— Cursor Rules MCPè¨­è¨ˆä»•æ§˜æ›¸

## æ¦‚è¦
asagami AIã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã€å€‹äººãƒ»ãƒãƒ¼ãƒ ã®å¼±ç‚¹ã«åŸºã¥ã„ã¦Cursor Rulesã‚’è‡ªå‹•ç”Ÿæˆã™ã‚‹Model Context Protocolï¼ˆMCPï¼‰ã‚µãƒ¼ãƒãƒ¼ã®è¨­è¨ˆä»•æ§˜æ›¸ã§ã™ã€‚

## 1. ã‚·ã‚¹ãƒ†ãƒ æ¦‚è¦

### 1.1 ç›®çš„
- å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é©å¿œå‹é–‹ç™ºç’°å¢ƒã‚’æ§‹ç¯‰
- å€‹äººã®è‹¦æ‰‹åˆ†é‡ã«ç‰¹åŒ–ã—ãŸCursor Rulesã®è‡ªå‹•ç”Ÿæˆ
- å®Ÿè·µãƒ­ã‚°ã¨ã®ç¶™ç¶šçš„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—ã®å®Ÿç¾

### 1.2 åŸºæœ¬æ§‹æˆ
```
asagami AI (Django) â†â†’ MCP Server â†â†’ Cursor (Claude Code)
                         â†•
                   AI Analysis Engine
```

## 2. MCP ã‚µãƒ¼ãƒãƒ¼ä»•æ§˜

### 2.1 ã‚µãƒ¼ãƒãƒ¼æƒ…å ±
- **åå‰**: `asagami-mcp-server`
- **ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: `1.0.0`
- **ãƒ—ãƒ­ãƒˆã‚³ãƒ«**: MCP v1.0
- **ãƒãƒ¼ãƒˆ**: 8001

### 2.2 æä¾›ãƒ„ãƒ¼ãƒ«

#### 2.2.1 å­¦ç¿’ãƒ‡ãƒ¼ã‚¿åˆ†æãƒ„ãƒ¼ãƒ«
```json
{
  "name": "analyze_learning_data",
  "description": "å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã¦å¼±ç‚¹ã¨å¼·ã¿ã‚’ç‰¹å®š",
  "inputSchema": {
    "type": "object",
    "properties": {
      "user_id": {"type": "integer", "description": "ãƒ¦ãƒ¼ã‚¶ãƒ¼ID"},
      "department_id": {"type": "integer", "description": "éƒ¨é–€IDï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰"},
      "analysis_period": {"type": "string", "description": "åˆ†ææœŸé–“ï¼ˆæ—¥æ•°ï¼‰", "default": "30"},
      "analysis_type": {"type": "string", "enum": ["individual", "team", "department"], "default": "individual"}
    },
    "required": ["user_id"]
  }
}
```

#### 2.2.2 Cursor Rulesç”Ÿæˆãƒ„ãƒ¼ãƒ«
```json
{
  "name": "generate_cursor_rules",
  "description": "åˆ†æçµæœã‹ã‚‰Cursor Rulesã‚’ç”Ÿæˆ",
  "inputSchema": {
    "type": "object",
    "properties": {
      "analysis_data": {"type": "object", "description": "åˆ†æçµæœãƒ‡ãƒ¼ã‚¿"},
      "rule_template": {"type": "string", "description": "ãƒ«ãƒ¼ãƒ«ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç¨®åˆ¥"},
      "target_skills": {"type": "array", "items": {"type": "string"}, "description": "å¯¾è±¡ã‚¹ã‚­ãƒ«é ˜åŸŸ"},
      "severity_level": {"type": "string", "enum": ["basic", "intermediate", "advanced"], "default": "intermediate"}
    },
    "required": ["analysis_data"]
  }
}
```

#### 2.2.3 å®Ÿè·µãƒ­ã‚°åé›†ãƒ„ãƒ¼ãƒ«
```json
{
  "name": "collect_practice_logs",
  "description": "Cursorã§ã®å®Ÿè·µãƒ­ã‚°ã‚’åé›†ãƒ»åˆ†æ",
  "inputSchema": {
    "type": "object",
    "properties": {
      "user_id": {"type": "integer", "description": "ãƒ¦ãƒ¼ã‚¶ãƒ¼ID"},
      "session_id": {"type": "string", "description": "ã‚»ãƒƒã‚·ãƒ§ãƒ³ID"},
      "error_logs": {"type": "array", "description": "ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°"},
      "completion_data": {"type": "object", "description": "ã‚³ãƒ¼ãƒ‰è£œå®Œãƒ‡ãƒ¼ã‚¿"},
      "time_metrics": {"type": "object", "description": "æ™‚é–“æŒ‡æ¨™"}
    },
    "required": ["user_id", "session_id"]
  }
}
```

#### 2.2.4 ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å‡¦ç†ãƒ„ãƒ¼ãƒ«
```json
{
  "name": "process_feedback",
  "description": "å®Ÿè·µãƒ­ã‚°ã‹ã‚‰ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’å‡¦ç†ã—ã€å­¦ç¿’ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ›´æ–°",
  "inputSchema": {
    "type": "object",
    "properties": {
      "practice_data": {"type": "object", "description": "å®Ÿè·µãƒ‡ãƒ¼ã‚¿"},
      "feedback_type": {"type": "string", "enum": ["error_pattern", "skill_gap", "improvement"]},
      "auto_generate_content": {"type": "boolean", "default": true}
    },
    "required": ["practice_data", "feedback_type"]
  }
}
```

## 3. ãƒ‡ãƒ¼ã‚¿æ§‹é€ 

### 3.1 å­¦ç¿’åˆ†æãƒ‡ãƒ¼ã‚¿
```json
{
  "user_id": 123,
  "analysis_date": "2025-07-13",
  "weak_points": [
    {
      "topic": "ãƒ‡ãƒ¼ã‚¿æš—å·åŒ–",
      "score": 65,
      "problem_areas": ["éµç®¡ç†", "æš—å·åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ é¸æŠ"],
      "frequency": 8,
      "improvement_suggestions": ["åŸºç¤ç†è«–ã®å¾©ç¿’", "å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç·´ç¿’"]
    }
  ],
  "strong_points": [
    {
      "topic": "APIè¨­è¨ˆ",
      "score": 92,
      "mastery_level": "advanced"
    }
  ],
  "learning_patterns": {
    "preferred_time": "morning",
    "avg_session_duration": 45,
    "retention_rate": 0.78
  }
}
```

### 3.2 Cursor Rulesæ§‹é€ 
```json
{
  "version": "1.0",
  "generated_at": "2025-07-13T10:30:00Z",
  "user_profile": {
    "user_id": 123,
    "skill_level": "intermediate",
    "focus_areas": ["security", "database"]
  },
  "rules": {
    "security": {
      "encryption": {
        "reminder": "æš—å·åŒ–å®Ÿè£…æ™‚ã¯éµç®¡ç†ã®è¦ä»¶ã‚’å¿…ãšç¢ºèªã—ã¦ãã ã•ã„",
        "auto_suggestions": true,
        "code_templates": ["aes_encryption.py", "key_management.py"]
      }
    },
    "code_quality": {
      "warnings": ["hardcoded_secrets", "sql_injection_risk"],
      "auto_fix": true
    }
  }
}
```

## 4. APIé€£æº

### 4.1 asagami AIé€£æºã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
- `GET /api/mcp/learning-data/{user_id}` - å­¦ç¿’ãƒ‡ãƒ¼ã‚¿å–å¾—
- `GET /api/mcp/question-results/{user_id}` - å•é¡Œè§£ç­”çµæœå–å¾—
- `GET /api/mcp/team-analytics/{department_id}` - ãƒãƒ¼ãƒ åˆ†æãƒ‡ãƒ¼ã‚¿å–å¾—
- `POST /api/mcp/feedback` - ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿é€ä¿¡

### 4.2 Cursoré€£æºã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
- `POST /mcp/cursor-rules` - Cursor Rulesç”Ÿæˆ
- `POST /mcp/practice-logs` - å®Ÿè·µãƒ­ã‚°é€ä¿¡
- `GET /mcp/personalized-rules/{user_id}` - å€‹äººç”¨ãƒ«ãƒ¼ãƒ«å–å¾—

## 5. AIåˆ†æã‚¨ãƒ³ã‚¸ãƒ³

### 5.1 å¼±ç‚¹æ¤œå‡ºã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
```python
def detect_weak_points(user_data):
    """
    å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å¼±ç‚¹ã‚’æ¤œå‡º
    - å•é¡Œè§£ç­”ã®æ­£ç­”ç‡åˆ†æ
    - å›ç­”æ™‚é–“ã®çµ±è¨ˆåˆ†æ
    - é–“é•ã„ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†é¡
    - ãƒˆãƒ”ãƒƒã‚¯é–“ã®é–¢é€£æ€§åˆ†æ
    """
    pass
```

### 5.2 ãƒ«ãƒ¼ãƒ«ç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯
```python
def generate_adaptive_rules(weak_points, user_profile):
    """
    å€‹äººã«ç‰¹åŒ–ã—ãŸCursor Rulesã‚’ç”Ÿæˆ
    - å¼±ç‚¹ã«å¯¾ã™ã‚‹å…·ä½“çš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹
    - ã‚³ãƒ¼ãƒ‰ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ææ¡ˆ
    - ã‚¨ãƒ©ãƒ¼é˜²æ­¢ã®ãŸã‚ã®ãƒã‚§ãƒƒã‚¯
    - å­¦ç¿’ãƒªã‚½ãƒ¼ã‚¹ã¸ã®ãƒªãƒ³ã‚¯
    """
    pass
```

## 6. ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£

### 6.1 èªè¨¼ãƒ»èªå¯
- JWT ãƒˆãƒ¼ã‚¯ãƒ³ãƒ™ãƒ¼ã‚¹èªè¨¼
- ãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ¥ãƒ‡ãƒ¼ã‚¿ã‚¢ã‚¯ã‚»ã‚¹åˆ¶å¾¡
- çµ„ç¹”ãƒ¬ãƒ™ãƒ«ã®æ¨©é™ç®¡ç†

### 6.2 ãƒ‡ãƒ¼ã‚¿ä¿è­·
- å€‹äººå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®æš—å·åŒ–
- GDPRæº–æ‹ ã®ãƒ‡ãƒ¼ã‚¿å‡¦ç†
- ç›£æŸ»ãƒ­ã‚°ã®è¨˜éŒ²

## 7. æ‹¡å¼µæ€§

### 7.1 ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
- ã‚«ã‚¹ã‚¿ãƒ åˆ†æã‚¨ãƒ³ã‚¸ãƒ³ã®è¿½åŠ 
- å¤–éƒ¨å­¦ç¿’ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ é€£æº
- ã‚µãƒ¼ãƒ‰ãƒ‘ãƒ¼ãƒ†ã‚£ãƒ¼IDEå¯¾å¿œ

### 7.2 ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£
- æ°´å¹³ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¯¾å¿œ
- ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥
- éåŒæœŸå‡¦ç†

## 8. ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°

### 8.1 ãƒ¡ãƒˆãƒªã‚¯ã‚¹
- MCP ã‚µãƒ¼ãƒãƒ¼ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
- ãƒ«ãƒ¼ãƒ«ç”Ÿæˆã®æˆåŠŸç‡
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å­¦ç¿’æ”¹å–„ç‡

### 8.2 ãƒ­ã‚°
- APIå‘¼ã³å‡ºã—ãƒ­ã‚°
- ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°
- ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡Œå‹•ãƒ­ã‚°

## 9. å®Ÿè£…ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—

### Phase 1: åŸºæœ¬MCP ã‚µãƒ¼ãƒãƒ¼
- å­¦ç¿’ãƒ‡ãƒ¼ã‚¿åˆ†ææ©Ÿèƒ½
- åŸºæœ¬çš„ãªCursor Rulesç”Ÿæˆ

### Phase 2: é«˜åº¦ãªåˆ†æ
- AI ã«ã‚ˆã‚‹è©³ç´°åˆ†æ
- ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸãƒ«ãƒ¼ãƒ«ç”Ÿæˆ

### Phase 3: ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—
- å®Ÿè·µãƒ­ã‚°åé›†
- ç¶™ç¶šçš„æ”¹å–„ã‚·ã‚¹ãƒ†ãƒ 

---

**ä½œæˆæ—¥**: 2025-07-13  
**ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 1.0  
**ä½œæˆè€…**: asagami AIé–‹ç™ºãƒãƒ¼ãƒ 

---

# REST API DESIGN SPECIFICATION

# asagami AI Ã— Cursor Rules REST APIè¨­è¨ˆä»•æ§˜æ›¸

## æ¦‚è¦
asagami AIã¨Cursor Rulesã®é€£æºã‚’å®Ÿç¾ã™ã‚‹REST APIè¨­è¨ˆä»•æ§˜æ›¸ã§ã™ã€‚å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®åˆ†æã‹ã‚‰Cursor Rulesã®ç”Ÿæˆã€å®Ÿè·µãƒ­ã‚°ã®åé›†ã¾ã§ã€ç¶™ç¶šçš„ãªå­¦ç¿’æ”¹å–„ã‚µã‚¤ã‚¯ãƒ«ã‚’æ”¯æ´ã—ã¾ã™ã€‚

## 1. APIæ¦‚è¦

### 1.1 ç›®çš„
- å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®åˆ†æã¨é›†è¨ˆ
- å€‹äººãƒ»ãƒãƒ¼ãƒ åˆ¥ã®å¼±ç‚¹æ¤œå‡º
- Cursor Rulesã®è‡ªå‹•ç”Ÿæˆã¨é…ä¿¡
- å®Ÿè·µãƒ­ã‚°ã®åé›†ã¨ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å‡¦ç†

### 1.2 åŸºæœ¬æƒ…å ±
- **ãƒ™ãƒ¼ã‚¹URL**: `https://api.asagami.ai/v1`
- **èªè¨¼**: JWT Bearer Token
- **ãƒ‡ãƒ¼ã‚¿å½¢å¼**: JSON
- **æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°**: UTF-8

## 2. èªè¨¼ãƒ»èªå¯

### 2.1 èªè¨¼æ–¹å¼
```http
Authorization: Bearer <JWT_TOKEN>
```

### 2.2 ã‚¹ã‚³ãƒ¼ãƒ—
- `read:learning_data` - å­¦ç¿’ãƒ‡ãƒ¼ã‚¿èª­ã¿å–ã‚Š
- `write:feedback` - ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯æ›¸ãè¾¼ã¿
- `generate:rules` - Cursor Rulesç”Ÿæˆ
- `admin:analytics` - ç®¡ç†è€…åˆ†ææ©Ÿèƒ½

## 3. å­¦ç¿’ãƒ‡ãƒ¼ã‚¿åˆ†æAPI

### 3.1 å€‹äººå­¦ç¿’ãƒ‡ãƒ¼ã‚¿å–å¾—
```
GET /api/cursor-integration/learning-data/{user_id}
```

**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:**
- `user_id` (required): ãƒ¦ãƒ¼ã‚¶ãƒ¼ID
- `period` (optional): åˆ†ææœŸé–“ï¼ˆæ—¥æ•°ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ30ï¼‰
- `subject_filter` (optional): ç§‘ç›®ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼

**ãƒ¬ã‚¹ãƒãƒ³ã‚¹ä¾‹:**
```json
{
  "user_id": 123,
  "analysis_period": 30,
  "summary": {
    "total_notes": 45,
    "total_questions": 180,
    "average_score": 78.5,
    "study_hours": 67.2
  },
  "weak_points": [
    {
      "topic": "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­è¨ˆ",
      "subject_id": 5,
      "score": 62,
      "question_count": 15,
      "error_patterns": ["æ­£è¦åŒ–ã®ç†è§£ä¸è¶³", "ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¨­è¨ˆã®èª¤ã‚Š"],
      "recommended_actions": ["æ­£è¦åŒ–ç†è«–ã®å¾©ç¿’", "ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã®å®Ÿè·µ"]
    }
  ],
  "strong_points": [
    {
      "topic": "APIè¨­è¨ˆ",
      "subject_id": 3,
      "score": 94,
      "mastery_level": "advanced"
    }
  ],
  "learning_patterns": {
    "preferred_study_time": "09:00-11:00",
    "avg_session_duration": 45,
    "retention_rate": 0.82,
    "difficulty_preference": "intermediate"
  }
}
```

### 3.2 ãƒãƒ¼ãƒ å­¦ç¿’åˆ†æ
```
GET /api/cursor-integration/team-analytics/{department_id}
```

**ãƒ¬ã‚¹ãƒãƒ³ã‚¹ä¾‹:**
```json
{
  "department_id": 10,
  "team_size": 12,
  "analysis_date": "2025-07-13",
  "common_weak_points": [
    {
      "topic": "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å®Ÿè£…",
      "affected_members": 8,
      "average_score": 65,
      "priority": "high"
    }
  ],
  "skill_distribution": {
    "beginner": 3,
    "intermediate": 7,
    "advanced": 2
  },
  "improvement_suggestions": [
    "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£åŸºç¤ã®ãƒãƒ¼ãƒ å‹‰å¼·ä¼š",
    "ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ“ãƒ¥ãƒ¼ã§ã®ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒã‚§ãƒƒã‚¯å¼·åŒ–"
  ]
}
```

## 4. Cursor Rulesç”ŸæˆAPI

### 4.1 å€‹äººç”¨ãƒ«ãƒ¼ãƒ«ç”Ÿæˆ
```
POST /api/cursor-integration/generate-rules
```

**ãƒªã‚¯ã‚¨ã‚¹ãƒˆä¾‹:**
```json
{
  "user_id": 123,
  "analysis_data": {
    "weak_points": [...],
    "strong_points": [...],
    "learning_patterns": {...}
  },
  "rule_config": {
    "strictness_level": "intermediate",
    "focus_areas": ["security", "performance"],
    "include_templates": true,
    "auto_suggestions": true
  }
}
```

**ãƒ¬ã‚¹ãƒãƒ³ã‚¹ä¾‹:**
```json
{
  "rule_id": "rule_123_20250713",
  "generated_at": "2025-07-13T10:30:00Z",
  "user_profile": {
    "user_id": 123,
    "skill_level": "intermediate",
    "specializations": ["web_development", "database"]
  },
  "cursor_rules": {
    "security": {
      "sql_injection": {
        "enabled": true,
        "severity": "error",
        "message": "SQLã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³å¯¾ç­–ï¼šãƒ—ãƒªãƒšã‚¢ãƒ‰ã‚¹ãƒ†ãƒ¼ãƒˆãƒ¡ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„",
        "examples": ["examples/prepared_statement.py"],
        "auto_fix": true
      },
      "password_handling": {
        "enabled": true,
        "severity": "warning",
        "message": "ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã®å¹³æ–‡ä¿å­˜ã¯ç¦æ­¢ã§ã™ã€‚ãƒãƒƒã‚·ãƒ¥åŒ–ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„"
      }
    },
    "performance": {
      "database_queries": {
        "enabled": true,
        "message": "N+1ã‚¯ã‚¨ãƒªå•é¡Œã«æ³¨æ„ï¼šãƒãƒƒãƒãƒ­ãƒ¼ãƒ‰å‡¦ç†ã‚’æ¤œè¨ã—ã¦ãã ã•ã„"
      }
    },
    "code_templates": [
      {
        "name": "secure_database_connection",
        "file_path": "templates/db_connection.py",
        "description": "ã‚»ã‚­ãƒ¥ã‚¢ãªãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ"
      }
    ]
  },
  "personalized_suggestions": [
    "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­è¨ˆã®å¾©ç¿’ã‚’ãŠå‹§ã‚ã—ã¾ã™",
    "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å®Ÿè£…ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„"
  ]
}
```

### 4.2 ãƒãƒ¼ãƒ ç”¨ãƒ«ãƒ¼ãƒ«ç”Ÿæˆ
```
POST /api/cursor-integration/generate-team-rules
```

**ãƒªã‚¯ã‚¨ã‚¹ãƒˆä¾‹:**
```json
{
  "department_id": 10,
  "team_analytics": {...},
  "rule_config": {
    "shared_standards": true,
    "compliance_rules": ["GDPR", "PCI_DSS"],
    "team_best_practices": true
  }
}
```

## 5. å®Ÿè·µãƒ­ã‚°åé›†API

### 5.1 é–‹ç™ºã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ­ã‚°é€ä¿¡
```
POST /api/cursor-integration/practice-logs
```

**ãƒªã‚¯ã‚¨ã‚¹ãƒˆä¾‹:**
```json
{
  "user_id": 123,
  "session_id": "sess_20250713_001",
  "session_start": "2025-07-13T09:00:00Z",
  "session_end": "2025-07-13T10:30:00Z",
  "development_data": {
    "errors_encountered": [
      {
        "error_type": "syntax_error",
        "error_message": "Uncaught TypeError: Cannot read property",
        "file_path": "src/auth.js",
        "line_number": 45,
        "resolution_time": 180,
        "resolution_method": "cursor_suggestion"
      }
    ],
    "code_completions": [
      {
        "trigger": "database connection",
        "suggestion_used": true,
        "completion_time": 5,
        "satisfaction_rating": 4
      }
    ],
    "rule_triggers": [
      {
        "rule_id": "security.sql_injection",
        "triggered_at": "2025-07-13T09:45:00Z",
        "user_action": "accepted",
        "effectiveness": "high"
      }
    ],
    "productivity_metrics": {
      "lines_of_code": 156,
      "files_modified": 3,
      "commits_made": 2,
      "test_coverage": 0.78
    }
  }
}
```

### 5.2 ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
```
GET /api/cursor-integration/error-patterns/{user_id}
```

**ãƒ¬ã‚¹ãƒãƒ³ã‚¹ä¾‹:**
```json
{
  "user_id": 123,
  "analysis_period": 7,
  "common_errors": [
    {
      "error_category": "database_queries",
      "frequency": 12,
      "avg_resolution_time": 240,
      "improvement_trend": "stable",
      "recommendations": [
        "ORMä½¿ç”¨æ–¹æ³•ã®å¾©ç¿’",
        "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³ã®å­¦ç¿’"
      ]
    }
  ],
  "skill_improvement": [
    {
      "skill": "error_handling",
      "before_score": 65,
      "current_score": 78,
      "improvement_rate": 0.2
    }
  ]
}
```

## 6. ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å‡¦ç†API

### 6.1 å­¦ç¿’ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ›´æ–°ææ¡ˆ
```
POST /api/cursor-integration/feedback-analysis
```

**ãƒªã‚¯ã‚¨ã‚¹ãƒˆä¾‹:**
```json
{
  "user_id": 123,
  "feedback_data": {
    "error_patterns": [...],
    "skill_gaps": [...],
    "improvement_areas": [...]
  },
  "auto_generate": true
}
```

**ãƒ¬ã‚¹ãƒãƒ³ã‚¹ä¾‹:**
```json
{
  "feedback_id": "fb_123_20250713",
  "analysis_results": {
    "new_weak_points_detected": [
      {
        "topic": "éåŒæœŸå‡¦ç†",
        "confidence": 0.85,
        "evidence": ["Promiseæœªå‡¦ç†ã‚¨ãƒ©ãƒ¼3å›", "async/awaitä½¿ç”¨ãƒŸã‚¹2å›"]
      }
    ],
    "improvement_confirmed": [
      {
        "topic": "SQLæœ€é©åŒ–",
        "improvement_score": 15,
        "evidence": ["N+1ã‚¯ã‚¨ãƒªã‚¨ãƒ©ãƒ¼0å›ï¼ˆå‰é€±3å›ï¼‰"]
      }
    ]
  },
  "content_suggestions": [
    {
      "type": "new_question",
      "topic": "éåŒæœŸå‡¦ç†",
      "difficulty": "intermediate",
      "description": "Promise ã¨async/awaitã®é©åˆ‡ãªä½¿ã„åˆ†ã‘"
    },
    {
      "type": "update_rule",
      "rule_id": "performance.database_queries",
      "change": "strictness_increase",
      "reason": "æ”¹å–„ãŒç¢ºèªã•ã‚ŒãŸãŸã‚ã€ã‚ˆã‚Šé«˜åº¦ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¿½åŠ "
    }
  ]
}
```

### 6.2 ç¶™ç¶šçš„æ”¹å–„ãƒ¬ãƒãƒ¼ãƒˆ
```
GET /api/cursor-integration/improvement-report/{user_id}
```

**ãƒ¬ã‚¹ãƒãƒ³ã‚¹ä¾‹:**
```json
{
  "user_id": 123,
  "report_period": "2025-06-13 to 2025-07-13",
  "overall_improvement": {
    "score_change": "+12.5",
    "skill_level_change": "intermediate â†’ upper-intermediate",
    "confidence_index": 0.78
  },
  "learning_effectiveness": {
    "asagami_study_impact": 0.65,
    "cursor_practice_impact": 0.72,
    "combined_effectiveness": 0.89
  },
  "next_focus_areas": [
    "ã‚¯ãƒ©ã‚¦ãƒ‰ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆ",
    "ãƒã‚¤ã‚¯ãƒ­ã‚µãƒ¼ãƒ“ã‚¹å®Ÿè£…"
  ],
  "recommended_actions": [
    "AWSã‚³ãƒ¼ã‚¹ã®å—è¬›",
    "Dockerå®Ÿè·µãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®é–‹å§‹"
  ]
}
```

## 7. çµ±è¨ˆãƒ»åˆ†æAPI

### 7.1 ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“çµ±è¨ˆ
```
GET /api/cursor-integration/system-analytics
```

### 7.2 ROIåˆ†æ
```
GET /api/cursor-integration/roi-analysis/{organization_id}
```

## 8. Webhookçµ±åˆ

### 8.1 ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€šçŸ¥
```
POST /webhooks/cursor-integration/notifications
```

### 8.2 è‡ªå‹•ãƒ«ãƒ¼ãƒ«æ›´æ–°
```
POST /webhooks/cursor-integration/rule-updates
```

## 9. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

### 9.1 ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼
```json
{
  "error": {
    "code": "ANALYSIS_FAILED",
    "message": "å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®åˆ†æã«å¤±æ•—ã—ã¾ã—ãŸ",
    "details": "insufficient_data",
    "timestamp": "2025-07-13T10:30:00Z"
  }
}
```

### 9.2 ã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ‰ä¸€è¦§
- `INVALID_USER_ID` - ç„¡åŠ¹ãªãƒ¦ãƒ¼ã‚¶ãƒ¼ID
- `INSUFFICIENT_DATA` - åˆ†æã«å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³
- `RULE_GENERATION_FAILED` - ãƒ«ãƒ¼ãƒ«ç”Ÿæˆã«å¤±æ•—
- `UNAUTHORIZED_ACCESS` - èªè¨¼ã‚¨ãƒ©ãƒ¼

## 10. ãƒ¬ãƒ¼ãƒˆåˆ¶é™

### 10.1 åˆ¶é™å€¤
- å­¦ç¿’ãƒ‡ãƒ¼ã‚¿å–å¾—: 100 requests/hour/user
- ãƒ«ãƒ¼ãƒ«ç”Ÿæˆ: 10 requests/hour/user
- ãƒ­ã‚°é€ä¿¡: 1000 requests/hour/user

## 11. å®Ÿè£…å„ªå…ˆåº¦

### Phase 1 (MVP)
- åŸºæœ¬çš„ãªå­¦ç¿’ãƒ‡ãƒ¼ã‚¿åˆ†æAPI
- ã‚·ãƒ³ãƒ—ãƒ«ãªCursor Rulesç”Ÿæˆ
- åŸºæœ¬çš„ãªå®Ÿè·µãƒ­ã‚°åé›†

### Phase 2 (æ‹¡å¼µæ©Ÿèƒ½)
- é«˜åº¦ãªåˆ†ææ©Ÿèƒ½
- ãƒãƒ¼ãƒ æ©Ÿèƒ½
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°

### Phase 3 (AIå¼·åŒ–)
- æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹äºˆæ¸¬åˆ†æ
- è‡ªå‹•çš„ãªå­¦ç¿’ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆ
- ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã®é«˜åº¦åŒ–

---

**ä½œæˆæ—¥**: 2025-07-13  
**ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 1.0  
**ä½œæˆè€…**: asagami AIé–‹ç™ºãƒãƒ¼ãƒ 